{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","type":"user","message":{"role":"user","content":"Investigate why inline comments are not properly attributed to files in extract-coderabbit-feedback.py:\n\n**Context:**\nThe comment \"Critical: Skipped tests document unresolved SQL injection vulnerabilities\" appears under \"unknown\" but should be attributed to:\n- File: apps/platform-frontend/src/app/api/organisation/[organisationId]/connections/[connectionId]/value/route.integration.test.ts\n- Lines: 829-897\n\n**Investigation tasks:**\n\n1. **Examine fetch_pr_inline_comments() function (lines 114-142)**:\n   - Uses `gh api repos/{owner}/{repo}/pulls/{pr_number}/comments`\n   - What fields does this API endpoint return?\n   - Does it include `path`, `line`, `original_line`, `position`, `original_position`?\n\n2. **Examine fetch_review_threads_graphql() function (lines 145-223)**:\n   - Uses GitHub GraphQL API for review threads\n   - Currently only queries: id, isResolved, resolvedBy, comments (body, createdAt, author)\n   - Check if the GraphQL schema supports querying for: path, line, position, or diffHunk\n\n3. **Check how inline_comments are processed (line 798, 822, 837)**:\n   - Line 798: `inline_comments = fetch_pr_inline_comments(pr_url)`\n   - Line 822: `coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)`\n   - Line 837: GraphQL threads are converted and appended to coderabbit_inline_comments\n   - Does the GraphQL conversion (lines 825-837) overwrite data from inline_comments?\n\n4. **Verify extract_coderabbit_inline_comments() function (lines 293-298)**:\n   - Filters for 'coderabbitai[bot]' user\n   - Does it preserve the path and line fields from the API response?\n\n5. **Check if there's duplicate processing**:\n   - Are the same comments being fetched twice (once via REST API inline comments, once via GraphQL threads)?\n   - Is the GraphQL conversion overwriting good data with 'unknown' and 0?\n\n**Expected findings:**\n- Identify whether path/line data exists in API responses but is being discarded\n- Determine if GraphQL threads are duplicating REST API inline comments and overwriting metadata\n- Propose whether we should: (a) use REST API data preferentially, (b) enhance GraphQL query, or (c) merge data from both sources\n\nReturn:\n- What fields are available in each API response\n- Where the data loss happens\n- Recommended solution approach"},"uuid":"ef827578-2de5-438d-bde3-7a4a97c756f8","timestamp":"2025-11-13T12:14:33.266Z"}
{"parentUuid":"ef827578-2de5-438d-bde3-7a4a97c756f8","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01XCvRmvahy9KWUtDmLMPQn1","type":"message","role":"assistant","content":[{"type":"text","text":"I'll investigate why inline comments are not properly attributed to files. Let me start by examining the relevant file and functions."}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8618,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":8618,"ephemeral_1h_input_tokens":0},"output_tokens":104,"service_tier":"standard"}},"requestId":"req_011CV5sS7ab4e3ACRZaiJope","type":"assistant","uuid":"1033fc5d-879a-40b5-abf8-5a389199087c","timestamp":"2025-11-13T12:14:35.356Z"}
{"parentUuid":"1033fc5d-879a-40b5-abf8-5a389199087c","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01XCvRmvahy9KWUtDmLMPQn1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FA5pKQtAkXFbP4N6Fv1hCa","name":"Read","input":{"file_path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8618,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":8618,"ephemeral_1h_input_tokens":0},"output_tokens":104,"service_tier":"standard"}},"requestId":"req_011CV5sS7ab4e3ACRZaiJope","type":"assistant","uuid":"5e6e8d50-1a3a-4721-9e42-7e03309d6d12","timestamp":"2025-11-13T12:14:36.112Z"}
{"parentUuid":"5e6e8d50-1a3a-4721-9e42-7e03309d6d12","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FA5pKQtAkXFbP4N6Fv1hCa","type":"tool_result","content":"     1â†’#!/usr/bin/env python3\n     2â†’\"\"\"\n     3â†’ABOUTME: Extract CodeRabbit review feedback from GitHub PRs for LLM consumption\n     4â†’ABOUTME: Uses gh CLI to fetch PR comments and formats them as plain text\n     5â†’\"\"\"\n     6â†’\n     7â†’__version__ = \"1.0.0\"\n     8â†’__author__ = \"Jesse Vincent\"\n     9â†’__license__ = \"MIT\"\n    10â†’import argparse\n    11â†’import json\n    12â†’import os\n    13â†’import re\n    14â†’import subprocess\n    15â†’import sys\n    16â†’from pathlib import Path\n    17â†’from urllib.parse import urlparse\n    18â†’from typing import List, Dict, Any, Tuple, Optional\n    19â†’from bs4 import BeautifulSoup\n    20â†’import html\n    21â†’\n    22â†’\n    23â†’def load_preamble_config() -> Optional[str]:\n    24â†’    \"\"\"Load custom preamble from dotfile.\"\"\"\n    25â†’    config_path = Path.home() / '.coderabbit-extractor'\n    26â†’    \n    27â†’    if config_path.exists():\n    28â†’        try:\n    29â†’            return config_path.read_text(encoding='utf-8').strip()\n    30â†’        except Exception as e:\n    31â†’            print(f\"Warning: Could not read config file {config_path}: {e}\", file=sys.stderr)\n    32â†’            return None\n    33â†’    \n    34â†’    return None\n    35â†’\n    36â†’\n    37â†’def parse_pr_input(input_str: str) -> str:\n    38â†’    \"\"\"Parse PR URL or owner/repo/number format into a full GitHub PR URL.\"\"\"\n    39â†’    input_str = input_str.strip()\n    40â†’    \n    41â†’    # If it's already a full URL, return as-is\n    42â†’    if input_str.startswith('https://github.com/'):\n    43â†’        return input_str\n    44â†’    \n    45â†’    # Parse owner/repo/number format (e.g., \"obra/lace/278\")\n    46â†’    parts = input_str.split('/')\n    47â†’    if len(parts) == 3:\n    48â†’        owner, repo, pr_num = parts\n    49â†’        return f\"https://github.com/{owner}/{repo}/pull/{pr_num}\"\n    50â†’    \n    51â†’    raise ValueError(f\"Invalid PR format: {input_str}. Use 'owner/repo/number' or full URL\")\n    52â†’\n    53â†’\n    54â†’def extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\n    55â†’    \"\"\"Extract owner, repo, and PR number from GitHub PR URL.\"\"\"\n    56â†’    # Parse URL like https://github.com/owner/repo/pull/123\n    57â†’    if 'github.com' in pr_url:\n    58â†’        parts = pr_url.replace('https://github.com/', '').split('/')\n    59â†’        if len(parts) >= 4 and parts[2] == 'pull':\n    60â†’            owner, repo, _, pr_number = parts[:4]\n    61â†’            try:\n    62â†’                return owner, repo, int(pr_number)\n    63â†’            except ValueError:\n    64â†’                raise ValueError(f\"Invalid PR number: {pr_number}\")\n    65â†’    \n    66â†’    raise ValueError(f\"Could not parse PR URL: {pr_url}\")\n    67â†’\n    68â†’\n    69â†’def fetch_pr_reviews(pr_url: str) -> List[Dict[str, Any]]:\n    70â†’    \"\"\"Fetch PR reviews using gh CLI.\"\"\"\n    71â†’    try:\n    72â†’        result = subprocess.run(\n    73â†’            ['gh', 'pr', 'view', pr_url, '--json', 'reviews'],\n    74â†’            capture_output=True,\n    75â†’            text=True,\n    76â†’            check=True\n    77â†’        )\n    78â†’        data = json.loads(result.stdout)\n    79â†’        return data.get('reviews', [])\n    80â†’    except subprocess.CalledProcessError as e:\n    81â†’        print(f\"Error fetching PR: {e.stderr}\", file=sys.stderr)\n    82â†’        sys.exit(1)\n    83â†’    except json.JSONDecodeError as e:\n    84â†’        print(f\"Error parsing JSON response: {e}\", file=sys.stderr)\n    85â†’        sys.exit(1)\n    86â†’\n    87â†’\n    88â†’def fetch_latest_commit_time(pr_url: str) -> Optional[str]:\n    89â†’    \"\"\"Fetch the timestamp of the most recent commit in the PR.\"\"\"\n    90â†’    try:\n    91â†’        # Extract owner/repo/number from URL\n    92â†’        owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n    93â†’        \n    94â†’        # Get commits from the PR\n    95â†’        result = subprocess.run(\n    96â†’            ['gh', 'api', f'repos/{owner}/{repo}/pulls/{pr_number}/commits'],\n    97â†’            capture_output=True,\n    98â†’            text=True,\n    99â†’            check=True\n   100â†’        )\n   101â†’        commits = json.loads(result.stdout)\n   102â†’        \n   103â†’        if commits:\n   104â†’            # Get the most recent commit (last in the list)\n   105â†’            latest_commit = commits[-1]\n   106â†’            return latest_commit['commit']['committer']['date']\n   107â†’        \n   108â†’        return None\n   109â†’    except Exception as e:\n   110â†’        print(f\"Warning: Could not fetch latest commit time: {e}\", file=sys.stderr)\n   111â†’        return None\n   112â†’\n   113â†’\n   114â†’def fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n   115â†’    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n   116â†’    try:\n   117â†’        # Extract owner/repo/number from URL\n   118â†’        if 'github.com' in pr_url:\n   119â†’            parts = pr_url.replace('https://github.com/', '').split('/')\n   120â†’            if len(parts) >= 4 and parts[2] == 'pull':\n   121â†’                owner, repo, _, pr_number = parts[:4]\n   122â†’                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n   123â†’            else:\n   124â†’                raise ValueError(\"Invalid GitHub PR URL format\")\n   125â†’        else:\n   126â†’            raise ValueError(\"URL must be a GitHub PR URL\")\n   127â†’        \n   128â†’        result = subprocess.run(\n   129â†’            ['gh', 'api', api_path],\n   130â†’            capture_output=True,\n   131â†’            text=True,\n   132â†’            check=True\n   133â†’        )\n   134â†’        data = json.loads(result.stdout)\n   135â†’        return data if isinstance(data, list) else []\n   136â†’    except subprocess.CalledProcessError as e:\n   137â†’        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n   138â†’        return []\n   139â†’    except json.JSONDecodeError as e:\n   140â†’        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n   141â†’        return []\n   142â†’        return []\n   143â†’\n   144â†’\n   145â†’def fetch_review_threads_graphql(owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:\n   146â†’    \"\"\"Fetch review threads with resolution status using GitHub GraphQL API.\"\"\"\n   147â†’    query = '''\n   148â†’    query($owner: String!, $repo: String!, $prNumber: Int!) {\n   149â†’      repository(owner: $owner, name: $repo) {\n   150â†’        pullRequest(number: $prNumber) {\n   151â†’          reviewThreads(first: 100) {\n   152â†’            nodes {\n   153â†’              id\n   154â†’              isResolved\n   155â†’              resolvedBy {\n   156â†’                login\n   157â†’              }\n   158â†’              comments(first: 1) {\n   159â†’                nodes {\n   160â†’                  body\n   161â†’                  createdAt\n   162â†’                  author {\n   163â†’                    login\n   164â†’                  }\n   165â†’                }\n   166â†’              }\n   167â†’            }\n   168â†’          }\n   169â†’        }\n   170â†’      }\n   171â†’    }\n   172â†’    '''\n   173â†’    \n   174â†’    variables = {\n   175â†’        \"owner\": owner,\n   176â†’        \"repo\": repo, \n   177â†’        \"prNumber\": pr_number\n   178â†’    }\n   179â†’    \n   180â†’    try:\n   181â†’        # Use gh api to make GraphQL request\n   182â†’        import json\n   183â†’        result = subprocess.run([\n   184â†’            'gh', 'api', 'graphql', \n   185â†’            '-F', f'owner={owner}',\n   186â†’            '-F', f'repo={repo}', \n   187â†’            '-F', f'prNumber={pr_number}',\n   188â†’            '-f', f'query={query}'\n   189â†’        ], capture_output=True, text=True, check=True)\n   190â†’        \n   191â†’        data = json.loads(result.stdout)\n   192â†’        \n   193â†’        # Extract review threads\n   194â†’        threads = data.get('data', {}).get('repository', {}).get('pullRequest', {}).get('reviewThreads', {}).get('nodes', [])\n   195â†’        \n   196â†’        # Filter to only CodeRabbit threads and return as list of dicts\n   197â†’        coderabbit_threads = []\n   198â†’        for thread in threads:\n   199â†’            if thread and thread.get('comments', {}).get('nodes', []):\n   200â†’                comment = thread['comments']['nodes'][0]\n   201â†’                author_login = comment.get('author', {}).get('login', '')\n   202â†’                # Check for both coderabbitai and coderabbitai[bot] formats\n   203â†’                if author_login in ['coderabbitai', 'coderabbitai[bot]']:\n   204â†’                    coderabbit_threads.append({\n   205â†’                        'id': thread['id'],\n   206â†’                        'isResolved': thread.get('isResolved', False),\n   207â†’                        'resolvedBy': thread.get('resolvedBy', {}).get('login') if thread.get('resolvedBy') else None,\n   208â†’                        'body': comment.get('body', ''),\n   209â†’                        'createdAt': comment.get('createdAt', ''),\n   210â†’                        'author': author_login\n   211â†’                    })\n   212â†’        \n   213â†’        return coderabbit_threads\n   214â†’        \n   215â†’    except subprocess.CalledProcessError as e:\n   216â†’        print(f\"Warning: Could not fetch review threads via GraphQL: {e.stderr}\", file=sys.stderr)\n   217â†’        return []\n   218â†’    except json.JSONDecodeError as e:\n   219â†’        print(f\"Warning: Could not parse GraphQL response: {e}\", file=sys.stderr)\n   220â†’        return []\n   221â†’    except Exception as e:\n   222â†’        print(f\"Warning: Error processing GraphQL data: {e}\", file=sys.stderr)\n   223â†’        return []\n   224â†’\n   225â†’\n   226â†’def extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   227â†’    \"\"\"Filter reviews to only CodeRabbit ones.\"\"\"\n   228â†’    return [\n   229â†’        review for review in reviews\n   230â†’        if review.get('author', {}).get('login') == 'coderabbitai'\n   231â†’    ]\n   232â†’\n   233â†’\n   234â†’def filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   235â†’    \"\"\"Filter out comments that have been marked as resolved.\"\"\"\n   236â†’    return [\n   237â†’        comment for comment in comments\n   238â†’        if not is_comment_resolved(comment.get('body', ''))\n   239â†’    ]\n   240â†’\n   241â†’\n   242â†’def filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   243â†’    \"\"\"Filter out review threads that have been marked as resolved via GitHub's native resolution.\"\"\"\n   244â†’    return [\n   245â†’        thread for thread in threads\n   246â†’        if not thread.get('isResolved', False)\n   247â†’    ]\n   248â†’\n   249â†’\n   250â†’def filter_resolved_review_content(review_body: str) -> str:\n   251â†’    \"\"\"Filter out resolved suggestions from review body content.\"\"\"\n   252â†’    # Look for resolved comment patterns and remove them\n   253â†’    # This handles CodeRabbit's text-based resolution markers\n   254â†’    \n   255â†’    # Pattern to match resolved comments within the review body\n   256â†’    # Look for sections that contain resolution markers\n   257â†’    resolved_patterns = [\n   258â†’        # Match individual comment sections that are resolved\n   259â†’        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n   260â†’        # Match line-specific resolved comments\n   261â†’        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n   262â†’    ]\n   263â†’    \n   264â†’    filtered_body = review_body\n   265â†’    for pattern in resolved_patterns:\n   266â†’        filtered_body = re.sub(pattern, '', filtered_body, flags=re.DOTALL | re.IGNORECASE)\n   267â†’    \n   268â†’    # Also remove any standalone resolution lines\n   269â†’    filtered_body = re.sub(r'^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?', '', filtered_body, flags=re.MULTILINE | re.IGNORECASE)\n   270â†’    \n   271â†’    return filtered_body.strip()\n   272â†’\n   273â†’\n   274â†’def is_comment_resolved(comment_body: str) -> bool:\n   275â†’    \"\"\"Check if a comment has been marked as resolved/addressed.\"\"\"\n   276â†’    # Look for resolution indicators in the comment body\n   277â†’    resolution_patterns = [\n   278â†’        r'âœ…\\s*Addressed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   279â†’        r'âœ…\\s*Resolved\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   280â†’        r'âœ…\\s*Fixed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   281â†’        r'âœ…\\s*Completed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   282â†’        r'\\[x\\]',  # Checkbox checked\n   283â†’        r'\\[X\\]',  # Checkbox checked\n   284â†’    ]\n   285â†’    \n   286â†’    for pattern in resolution_patterns:\n   287â†’        if re.search(pattern, comment_body, re.IGNORECASE):\n   288â†’            return True\n   289â†’    \n   290â†’    return False\n   291â†’\n   292â†’\n   293â†’def extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   294â†’    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n   295â†’    return [\n   296â†’        comment for comment in comments\n   297â†’        if comment.get('user', {}).get('login') == 'coderabbitai[bot]'\n   298â†’    ]\n   299â†’\n   300â†’\n   301â†’def parse_review_type(body: str) -> str:\n   302â†’    \"\"\"Determine the type of CodeRabbit review.\"\"\"\n   303â†’    if 'Actionable comments posted:' in body:\n   304â†’        return 'ACTIONABLE_REVIEW'\n   305â†’    elif 'walkthrough_start' in body and 'walkthrough_end' in body:\n   306â†’        return 'WALKTHROUGH'\n   307â†’    elif '<!-- This is an auto-generated comment: summarize by coderabbit.ai -->' in body:\n   308â†’        return 'SUMMARY'\n   309â†’    else:\n   310â†’        return 'OTHER'\n   311â†’\n   312â†’\n   313â†’def clean_html_artifacts(text: str) -> str:\n   314â†’    \"\"\"Remove HTML comments and artifacts from text.\"\"\"\n   315â†’    # Remove HTML comments\n   316â†’    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n   317â†’    \n   318â†’    # Remove CodeRabbit warning blocks\n   319â†’    text = re.sub(r'>\\s*â€¼ï¸\\s*\\*\\*IMPORTANT\\*\\*.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n   320â†’    \n   321â†’    # Remove other quote block artifacts  \n   322â†’    text = re.sub(r'>\\s*Carefully review.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n   323â†’    \n   324â†’    # Remove empty lines that result from artifact removal\n   325â†’    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n   326â†’    \n   327â†’    return text.strip()\n   328â†’\n   329â†’\n   330â†’def clean_diff_artifacts(diff: str) -> str:\n   331â†’    \"\"\"Clean up diff artifacts and markers.\"\"\"\n   332â†’    if not diff:\n   333â†’        return diff\n   334â†’\n   335â†’    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n   336â†’    diff = re.sub(r'^@@.*?@@.*?\\n', '', diff, flags=re.MULTILINE)  # Full hunk headers\n   337â†’    diff = re.sub(r'^\\s*@@\\s*\\n', '', diff, flags=re.MULTILINE)    # Lone @@ lines\n   338â†’\n   339â†’    # Remove empty lines at start/end\n   340â†’    diff = diff.strip()\n   341â†’\n   342â†’    return diff\n   343â†’\n   344â†’\n   345â†’def remove_details_sections(text: str) -> str:\n   346â†’    \"\"\"Remove verbose <details> sections containing analysis chains and script outputs.\n   347â†’\n   348â†’    Removes complete <details>...</details> blocks to filter out:\n   349â†’    - Analysis/reasoning chains\n   350â†’    - Script execution outputs\n   351â†’    - Internal processing logs\n   352â†’\n   353â†’    This helps keep the output focused on actionable feedback.\n   354â†’    \"\"\"\n   355â†’    # Remove all <details>...</details> blocks including nested content\n   356â†’    text = re.sub(r'<details>.*?</details>', '', text, flags=re.DOTALL | re.IGNORECASE)\n   357â†’\n   358â†’    # Clean up any resulting excessive whitespace\n   359â†’    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n   360â†’\n   361â†’    return text.strip()\n   362â†’\n   363â†’\n   364â†’def extract_file_and_lines_from_body(body: str) -> Tuple[Optional[str], Optional[str]]:\n   365â†’    \"\"\"Extract file path and line range from CodeRabbit comment body.\n   366â†’\n   367â†’    CodeRabbit comments typically include file references like:\n   368â†’    - \"In <filepath> around line X\"\n   369â†’    - \"In <filepath> around lines X to Y\"\n   370â†’    - \"In <filepath> around lines X-Y\"\n   371â†’\n   372â†’    Returns:\n   373â†’        Tuple of (file_path, line_range) or (None, None) if not found\n   374â†’    \"\"\"\n   375â†’    # Pattern to match file path references in CodeRabbit comments\n   376â†’    # Matches: \"In <filepath> around line(s) X\" or \"around lines X to Y\" or \"around lines X-Y\"\n   377â†’    file_pattern = r'In\\s+([^\\s]+(?:\\.ts|\\.tsx|\\.js|\\.jsx|\\.py|\\.java|\\.go|\\.rs|\\.md|\\.json|\\.yaml|\\.yml))\\s+around\\s+lines?\\s+([\\d\\-to\\s]+)'\n   378â†’\n   379â†’    match = re.search(file_pattern, body, re.IGNORECASE)\n   380â†’    if match:\n   381â†’        file_path = match.group(1).strip()\n   382â†’        line_range = match.group(2).strip()\n   383â†’\n   384â†’        # Normalize line range format (convert \"X to Y\" to \"X-Y\")\n   385â†’        line_range = re.sub(r'(\\d+)\\s+to\\s+(\\d+)', r'\\1-\\2', line_range)\n   386â†’\n   387â†’        return (file_path, line_range)\n   388â†’\n   389â†’    return (None, None)\n   390â†’\n   391â†’\n   392â†’def extract_prompt_for_ai_agents(body: str) -> List[str]:\n   393â†’    \"\"\"Extract 'Prompt for AI Agents' sections from review body.\"\"\"\n   394â†’    prompts = []\n   395â†’    \n   396â†’    # Pattern to find ğŸ¤– Prompt for AI Agents sections\n   397â†’    # These appear to be in <summary> tags followed by clipboard content\n   398â†’    pattern = r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*<div[^>]*data-snippet-clipboard-copy-content=\"([^\"]*)\"'\n   399â†’    matches = re.findall(pattern, body, re.DOTALL)\n   400â†’    \n   401â†’    for match in matches:\n   402â†’        # Decode HTML entities\n   403â†’        clean_text = (match\n   404â†’                     .replace('&lt;', '<')\n   405â†’                     .replace('&gt;', '>')\n   406â†’                     .replace('&amp;', '&')\n   407â†’                     .replace('&quot;', '\"')\n   408â†’                     .replace('\\\\n', '\\n')\n   409â†’                     .strip())\n   410â†’        if clean_text:\n   411â†’            prompts.append(clean_text)\n   412â†’    \n   413â†’    return prompts\n   414â†’\n   415â†’\n   416â†’# Removed unused functions - now using parse_review_sections instead\n   417â†’\n   418â†’\n   419â†’def parse_review_sections(body: str) -> Dict[str, Any]:\n   420â†’    \"\"\"Parse different sections from CodeRabbit review body using HTML parsing.\"\"\"\n   421â†’    sections = {}\n   422â†’    \n   423â†’    # Extract actionable comments count from text\n   424â†’    actionable_match = re.search(r'Actionable comments posted:\\s*(\\d+)', body)\n   425â†’    if actionable_match:\n   426â†’        sections['actionable_count'] = int(actionable_match.group(1))\n   427â†’    \n   428â†’    # Parse HTML structure - but be careful not to mangle code content\n   429â†’    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n   430â†’    soup = BeautifulSoup(body, 'html.parser')\n   431â†’    \n   432â†’    # Find sections by looking for <details> elements with specific summary text\n   433â†’    details_elements = soup.find_all('details')\n   434â†’    \n   435â†’    for details in details_elements:\n   436â†’        summary = details.find('summary')\n   437â†’        if not summary:\n   438â†’            continue\n   439â†’            \n   440â†’        summary_text = summary.get_text(strip=True)\n   441â†’        \n   442â†’        # Check for outside diff range comments (avoid emoji dependency)\n   443â†’        if 'Outside diff range comments' in summary_text:\n   444â†’            # Extract count from summary text\n   445â†’            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n   446â†’            if count_match:\n   447â†’                count = int(count_match.group(1))\n   448â†’                blockquote = details.find('blockquote')\n   449â†’                if blockquote:\n   450â†’                    # Get all content inside the blockquote\n   451â†’                    content = str(blockquote)\n   452â†’                    sections['outside_diff_comments'] = {'count': count, 'content': content}\n   453â†’        \n   454â†’        # Check for nitpick comments or duplicate comments  \n   455â†’        elif 'Nitpick comments' in summary_text or 'Duplicate comments' in summary_text:\n   456â†’            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n   457â†’            if count_match:\n   458â†’                count = int(count_match.group(1))\n   459â†’                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n   460â†’                # extract the raw content from the original body text\n   461â†’                \n   462â†’                # Find the position of this details section in the original body\n   463â†’                details_start = body.find('Nitpick comments')\n   464â†’                if details_start != -1:\n   465â†’                    # Find where this section ends (next major section or end)\n   466â†’                    section_markers = ['ğŸ“œ Review details', 'ğŸ”‡ Additional comments', '</details>\\n\\n</details>']\n   467â†’                    section_end = len(body)\n   468â†’                    \n   469â†’                    for marker in section_markers:\n   470â†’                        marker_pos = body.find(marker, details_start)\n   471â†’                        if marker_pos != -1:\n   472â†’                            section_end = min(section_end, marker_pos)\n   473â†’                    \n   474â†’                    # Extract the raw text content\n   475â†’                    raw_content = body[details_start:section_end]\n   476â†’                    sections['nitpick_comments'] = {'count': count, 'content': raw_content}\n   477â†’        \n   478â†’        # Check for review details (usually at the end)\n   479â†’        elif 'Review details' in summary_text:\n   480â†’            # This section contains metadata, we might want it for context\n   481â†’            blockquote = details.find('blockquote')\n   482â†’            if blockquote:\n   483â†’                content = str(blockquote)\n   484â†’                sections['review_details'] = {'content': content}\n   485â†’    \n   486â†’    # Extract AI prompts (look for specific patterns)\n   487â†’    sections['ai_prompts'] = extract_prompt_for_ai_agents(body)\n   488â†’    \n   489â†’    return sections\n   490â†’\n   491â†’\n   492â†’def parse_file_level_comments(content: str, debug: bool = False) -> List[Dict[str, Any]]:\n   493â†’    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n   494â†’    comments = []\n   495â†’    \n   496â†’    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n   497â†’    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n   498â†’    file_pattern = r'<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>'\n   499â†’    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n   500â†’    \n   501â†’    if debug:\n   502â†’        print(f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\", file=sys.stderr)\n   503â†’    \n   504â†’    for file_match in file_matches:\n   505â†’        file_path = file_match.group(1).strip()\n   506â†’        comment_count = int(file_match.group(2))\n   507â†’        file_content = file_match.group(3)  # Raw blockquote content, not parsed by BeautifulSoup\n   508â†’        \n   509â†’        # Now parse line comments within this file content\n   510â†’        # Find line comment patterns: `16-24`: **Title**\n   511â†’        line_pattern = r'`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)'\n   512â†’        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n   513â†’        \n   514â†’        if debug:\n   515â†’            print(f\"DEBUG: File {file_path} has {len(line_matches)} line comments\", file=sys.stderr)\n   516â†’        \n   517â†’        for line_match in line_matches:\n   518â†’            line_range = line_match.group(1).strip()\n   519â†’            title = line_match.group(2).strip()\n   520â†’            content_block = line_match.group(3).strip()\n   521â†’\n   522â†’            # Remove verbose <details> sections before processing\n   523â†’            content_block = remove_details_sections(content_block)\n   524â†’\n   525â†’            # Check if this looks like a line reference\n   526â†’            if re.match(r'^[\\d\\-,\\s]+$', line_range):\n   527â†’                # Extract description (everything before first code block)\n   528â†’                description = content_block\n   529â†’                code_diff = \"\"\n   530â†’                \n   531â†’                code_start = content_block.find('```')\n   532â†’                if code_start != -1:\n   533â†’                    description = content_block[:code_start].strip()\n   534â†’                    \n   535â†’                    # Extract code diff from raw content (no BeautifulSoup mangling)\n   536â†’                    diff_match = re.search(r'```diff\\n(.*?)\\n```', content_block, re.DOTALL)\n   537â†’                    if diff_match:\n   538â†’                        code_diff = diff_match.group(1).strip()\n   539â†’                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n   540â†’                        code_diff = html.unescape(code_diff)\n   541â†’                        code_diff = clean_html_artifacts(code_diff)  # Remove any HTML comments in diffs\n   542â†’                        code_diff = clean_diff_artifacts(code_diff)  # Clean up @@ markers and artifacts\n   543â†’                \n   544â†’                # Clean up description\n   545â†’                description = clean_html_artifacts(description)  # Remove HTML comments first\n   546â†’                description_lines = []\n   547â†’                for line in description.split('\\n'):\n   548â†’                    line = line.strip()\n   549â†’                    if line and not line.startswith('Also applies to:') and line != '---':\n   550â†’                        description_lines.append(line)\n   551â†’                description = ' '.join(description_lines)\n   552â†’                \n   553â†’                comments.append({\n   554â†’                    'file': file_path,\n   555â†’                    'lines': line_range,\n   556â†’                    'title': title,\n   557â†’                    'description': description,\n   558â†’                    'code_diff': code_diff\n   559â†’                })\n   560â†’    \n   561â†’    return comments\n   562â†’\n   563â†’\n   564â†’def group_comments_by_file(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, include_resolved: bool = True) -> Dict[str, List[Dict[str, Any]]]:\n   565â†’    \"\"\"Group all comments by file for better organization.\"\"\"\n   566â†’    file_groups = {}\n   567â†’    \n   568â†’    # Process review body comments\n   569â†’    for review in coderabbit_reviews:\n   570â†’        review_body = review['body']\n   571â†’        \n   572â†’        # Filter out resolved content from review body if requested\n   573â†’        if not include_resolved:\n   574â†’            original_body = review_body\n   575â†’            review_body = filter_resolved_review_content(review_body)\n   576â†’            if debug and len(original_body) != len(review_body):\n   577â†’                print(f\"DEBUG: Filtered resolved content from review body (length: {len(original_body)} -> {len(review_body)})\", file=sys.stderr)\n   578â†’        \n   579â†’        sections = parse_review_sections(review_body)\n   580â†’        \n   581â†’        # Add outside diff comments\n   582â†’        if 'outside_diff_comments' in sections:\n   583â†’            outside_comments = parse_file_level_comments(sections['outside_diff_comments']['content'], debug)\n   584â†’            for comment in outside_comments:\n   585â†’                file_path = comment['file']\n   586â†’                if file_path not in file_groups:\n   587â†’                    file_groups[file_path] = []\n   588â†’                comment['source'] = 'outside_diff'\n   589â†’                file_groups[file_path].append(comment)\n   590â†’        \n   591â†’        # Add nitpick comments\n   592â†’        if 'nitpick_comments' in sections:\n   593â†’            nitpick_comments = parse_file_level_comments(sections['nitpick_comments']['content'], debug)\n   594â†’            for comment in nitpick_comments:\n   595â†’                file_path = comment['file']\n   596â†’                if file_path not in file_groups:\n   597â†’                    file_groups[file_path] = []\n   598â†’                comment['source'] = 'nitpick'\n   599â†’                file_groups[file_path].append(comment)\n   600â†’    \n   601â†’    # Process inline comments\n   602â†’    if inline_comments:\n   603â†’        for comment in inline_comments:\n   604â†’            body = comment.get('body', '')\n   605â†’\n   606â†’            # Try to extract file path and line range from comment body\n   607â†’            extracted_file, extracted_lines = extract_file_and_lines_from_body(body)\n   608â†’\n   609â†’            # Use extracted values if available, otherwise fall back to comment metadata\n   610â†’            file_path = extracted_file if extracted_file else comment.get('path', 'unknown')\n   611â†’            line_num = comment.get('original_line', 'unknown')\n   612â†’\n   613â†’            # Use extracted line range if available\n   614â†’            if extracted_lines:\n   615â†’                line_range = extracted_lines\n   616â†’            else:\n   617â†’                line_range = str(line_num) if line_num != 'unknown' and line_num != 0 else ''\n   618â†’\n   619â†’            if file_path not in file_groups:\n   620â†’                file_groups[file_path] = []\n   621â†’\n   622â†’            # Convert inline comment to standard format\n   623â†’            comment_type = 'ğŸ› ï¸ Refactor Suggestion' if 'ğŸ› ï¸ Refactor suggestion' in body else 'Comment'\n   624â†’\n   625â†’            # Extract title from body\n   626â†’            title_match = re.search(r'\\*\\*(.*?)\\*\\*', body)\n   627â†’            title = title_match.group(1).strip() if title_match else \"Suggestion\"\n   628â†’            \n   629â†’            # Extract description and diff\n   630â†’            if '<details>' in body and 'Prompt for AI Agents' in body:\n   631â†’                main_content = body.split('<details>')[0].strip()\n   632â†’            else:\n   633â†’                main_content = body\n   634â†’\n   635â†’            # Remove verbose <details> sections\n   636â†’            main_content = remove_details_sections(main_content)\n   637â†’            main_content = main_content.replace('_ğŸ› ï¸ Refactor suggestion_', '').strip()\n   638â†’            main_content = clean_html_artifacts(main_content)\n   639â†’            \n   640â†’            # Extract AI prompt\n   641â†’            ai_prompt = \"\"\n   642â†’            ai_prompt_match = re.search(r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*\\n\\n```\\n(.*?)\\n```', body, re.DOTALL)\n   643â†’            if ai_prompt_match:\n   644â†’                ai_prompt = ai_prompt_match.group(1).strip()\n   645â†’            \n   646â†’            file_groups[file_path].append({\n   647â†’                'lines': line_range,\n   648â†’                'title': title,\n   649â†’                'description': main_content,\n   650â†’                'code_diff': '',  # Inline comments don't have separate diffs\n   651â†’                'ai_prompt': ai_prompt,\n   652â†’                'source': 'inline',\n   653â†’                'comment_type': comment_type\n   654â†’            })\n   655â†’    \n   656â†’    return file_groups\n   657â†’\n   658â†’\n   659â†’def format_for_llm(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, preamble: Optional[str] = None, include_resolved: bool = True) -> str:\n   660â†’    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n   661â†’    output = []\n   662â†’    \n   663â†’    # Add custom preamble if provided\n   664â†’    if preamble:\n   665â†’        output.append(preamble)\n   666â†’        output.append(\"\")\n   667â†’        output.append(\"=\" * 60)\n   668â†’        output.append(\"\")\n   669â†’    \n   670â†’    output.append(\"# CodeRabbit Review Feedback\")\n   671â†’    output.append(\"=\" * 40)\n   672â†’    output.append(\"\")\n   673â†’    \n   674â†’    # Group all comments by file\n   675â†’    file_groups = group_comments_by_file(coderabbit_reviews, inline_comments, debug, include_resolved)\n   676â†’    \n   677â†’    if not file_groups:\n   678â†’        output.append(\"No actionable comments found.\")\n   679â†’        return \"\\n\".join(output)\n   680â†’    \n   681â†’    # Add summary\n   682â†’    total_comments = sum(len(comments) for comments in file_groups.values())\n   683â†’    output.append(f\"**Total files with feedback:** {len(file_groups)}\")\n   684â†’    output.append(f\"**Total comments:** {total_comments}\")\n   685â†’    output.append(\"\")\n   686â†’    \n   687â†’    # Process each file\n   688â†’    for file_path in sorted(file_groups.keys()):\n   689â†’        comments = file_groups[file_path]\n   690â†’        \n   691â†’        output.append(f\"## {file_path}\")\n   692â†’        output.append(f\"**{len(comments)} suggestion(s)**\")\n   693â†’        output.append(\"\")\n   694â†’        \n   695â†’        # Sort comments by priority: AI prompts first, then by line number\n   696â†’        def get_sort_key(comment):\n   697â†’            # Priority 1: Comments with AI prompts (highest priority)\n   698â†’            has_ai_prompt = bool(comment.get('ai_prompt'))\n   699â†’\n   700â†’            # Priority 2: Line number (for logical code flow)\n   701â†’            line_num = 999\n   702â†’            lines = comment.get('lines', '')\n   703â†’            if lines and lines.replace('-', '').isdigit():\n   704â†’                line_num = int(lines.split('-')[0])\n   705â†’\n   706â†’            # Return tuple: (ai_prompt_priority, line_number)\n   707â†’            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n   708â†’            return (0 if has_ai_prompt else 1, line_num)\n   709â†’\n   710â†’        comments_sorted = sorted(comments, key=get_sort_key)\n   711â†’\n   712â†’        for i, comment in enumerate(comments_sorted, 1):\n   713â†’            # Add AI prompt indicator to title for high-priority items\n   714â†’            title = comment['title']\n   715â†’            if comment.get('ai_prompt'):\n   716â†’                title = f\"ğŸ¤– {title}\"\n   717â†’\n   718â†’            # Only include line numbers if they're meaningful (not empty or \"0\")\n   719â†’            lines = comment.get('lines', '')\n   720â†’            if lines and lines != '0':\n   721â†’                output.append(f\"### {i}. Lines {lines}: {title}\")\n   722â†’            else:\n   723â†’                output.append(f\"### {i}. {title}\")\n   724â†’            output.append(\"\")\n   725â†’            \n   726â†’            # Show AI prompt, or fallback to description + diff\n   727â†’            has_prompt = bool(comment.get('ai_prompt'))\n   728â†’            has_diff = bool(comment.get('code_diff'))\n   729â†’            \n   730â†’            if debug:\n   731â†’                output.append(f\"<!-- DEBUG: has_prompt={has_prompt}, has_diff={has_diff}, source={comment.get('source')} -->\")\n   732â†’            \n   733â†’            if has_prompt:\n   734â†’                if debug:\n   735â†’                    output.append(\"<!-- DEBUG: Showing prompt only, skipping description and diff -->\")\n   736â†’                # AI prompt contains better explanation than description + diff\n   737â†’                output.append(\"**Proposed prompt:**\")\n   738â†’                output.append(\"```\")\n   739â†’                output.append(comment['ai_prompt'])\n   740â†’                output.append(\"```\")\n   741â†’                output.append(\"\")\n   742â†’            else:\n   743â†’                if debug:\n   744â†’                    output.append(\"<!-- DEBUG: No prompt, showing description and diff -->\")\n   745â†’                \n   746â†’                # Show description only if no AI prompt available\n   747â†’                if comment['description'] and len(comment['description']) > 10:\n   748â†’                    desc = comment['description']\n   749â†’                    desc = re.sub(r'^_.*?_\\s*', '', desc)  # Remove _âš ï¸ Potential issue_ etc\n   750â†’                    desc = re.sub(r'\\*\\*.*?\\*\\*\\s*', '', desc, count=1)  # Remove first **title** if duplicated\n   751â†’                    desc = re.sub(r'Apply this diff:\\s*', '', desc)\n   752â†’                    desc = desc.strip()\n   753â†’                    \n   754â†’                    if desc and desc != title:\n   755â†’                        output.append(desc)\n   756â†’                        output.append(\"\")\n   757â†’                \n   758â†’                # Show diff only if no AI prompt available\n   759â†’                if has_diff:\n   760â†’                    output.append(\"```diff\")\n   761â†’                    output.append(comment['code_diff'])\n   762â†’                    output.append(\"```\")\n   763â†’                    output.append(\"\")\n   764â†’            \n   765â†’            if i < len(comments_sorted):\n   766â†’                output.append(\"-\" * 30)\n   767â†’                output.append(\"\")\n   768â†’        \n   769â†’        output.append(\"=\" * 60)\n   770â†’        output.append(\"\")\n   771â†’    \n   772â†’    return \"\\n\".join(output)\n   773â†’\n   774â†’\n   775â†’def main():\n   776â†’    \"\"\"Main entry point.\"\"\"\n   777â†’    parser = argparse.ArgumentParser(\n   778â†’        description=\"\"\"\n   779â†’CodeRabbit Review Extractor - Convert GitHub PR reviews to LLM-friendly text\n   780â†’\n   781â†’This tool extracts CodeRabbit automated code review feedback from GitHub pull \n   782â†’requests and formats it as clean, organized text suitable for AI coding agents.\n   783â†’\n   784â†’Perfect for feeding CodeRabbit suggestions to Claude, ChatGPT, or other LLMs\n   785â†’to automatically apply code improvements, refactoring suggestions, and fixes.\n   786â†’        \"\"\".strip(),\n   787â†’        epilog=\"\"\"\n   788â†’EXAMPLES:\n   789â†’  %(prog)s https://github.com/owner/repo/pull/123\n   790â†’  %(prog)s owner/repo/123  \n   791â†’  %(prog)s obra/lace/278 --all-reviews\n   792â†’\n   793â†’WHAT IT EXTRACTS:\n   794â†’  â€¢ Refactor suggestions with specific code diffs\n   795â†’  â€¢ AI implementation instructions (ğŸ¤– prompts) with detailed steps  \n   796â†’  â€¢ Nitpick comments and code quality improvements\n   797â†’  â€¢ File-organized feedback sorted by priority\n   798â†’  â€¢ Clean format without HTML artifacts or noise\n   799â†’\n   800â†’HOW IT WORKS:\n   801â†’  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n   802â†’  2. Extracts both main review summaries and inline code comments\n   803â†’  3. Organizes feedback by file with line-number targeting\n   804â†’  4. Prioritizes actionable items with AI implementation guides\n   805â†’  5. Outputs clean text ready for LLM consumption\n   806â†’\n   807â†’DEFAULT BEHAVIOR:\n   808â†’  Processes only the LATEST CodeRabbit review to avoid overwhelming output.\n   809â†’  Use --all-reviews for PRs with multiple review iterations if needed.\n   810â†’\n   811â†’REQUIREMENTS:\n   812â†’  â€¢ GitHub CLI (gh) installed and authenticated\n   813â†’  â€¢ Python 3.6+ with beautifulsoup4 package\n   814â†’\n   815â†’CONFIGURATION:\n   816â†’  Create ~/.coderabbit-extractor with custom preamble text to prepend to output.\n   817â†’  Useful for framing feedback (e.g., critical evaluation prompts for AI agents).\n   818â†’        \"\"\",\n   819â†’        formatter_class=argparse.RawDescriptionHelpFormatter\n   820â†’    )\n   821â†’    \n   822â†’    parser.add_argument('pr_input', \n   823â†’                       help='GitHub PR URL or owner/repo/number format (e.g., \"owner/repo/123\")')\n   824â†’    parser.add_argument('--include-resolved', \n   825â†’                       action='store_true',\n   826â†’                       help='Include review suggestions that have been marked as resolved (default: filter them out)')\n   827â†’    parser.add_argument('--all-reviews', \n   828â†’                       action='store_true',\n   829â†’                       help='Extract all CodeRabbit reviews (default: latest only)')\n   830â†’    parser.add_argument('--since-commit',\n   831â†’                       metavar='SHA', \n   832â†’                       help='Extract reviews submitted after this commit SHA')\n   833â†’    parser.add_argument('--debug',\n   834â†’                       action='store_true',\n   835â†’                       help='Show debug annotations in output to trace processing')\n   836â†’    parser.add_argument('--version',\n   837â†’                       action='version',\n   838â†’                       version=f'%(prog)s {__version__}')\n   839â†’    \n   840â†’    # Custom handling for no arguments to show helpful info\n   841â†’    if len(sys.argv) == 1:\n   842â†’        print(\"ğŸ¤– CodeRabbit Review Extractor\")\n   843â†’        print(\"=\" * 40)\n   844â†’        print()\n   845â†’        print(\"WHAT THIS DOES:\")\n   846â†’        print(\"  Converts CodeRabbit GitHub PR reviews into clean text for AI coding agents\")\n   847â†’        print(\"  (Claude, ChatGPT, etc.) to automatically apply code suggestions.\")\n   848â†’        print()\n   849â†’        print(\"QUICK START:\")\n   850â†’        print(\"  python3 extract-coderabbit-feedback.py https://github.com/owner/repo/pull/123\")\n   851â†’        print(\"  python3 extract-coderabbit-feedback.py owner/repo/123\")\n   852â†’        print()\n   853â†’        print(\"For full help and options, run:\")\n   854â†’        print(\"  python3 extract-coderabbit-feedback.py --help\")\n   855â†’        sys.exit(1)\n   856â†’    \n   857â†’    args = parser.parse_args()\n   858â†’    \n   859â†’    try:\n   860â†’        pr_url = parse_pr_input(args.pr_input)\n   861â†’        \n   862â†’        # Load custom preamble from dotfile\n   863â†’        preamble = load_preamble_config()\n   864â†’        \n   865â†’        # Fetch both reviews and inline comments\n   866â†’        reviews = fetch_pr_reviews(pr_url)\n   867â†’        inline_comments = fetch_pr_inline_comments(pr_url)\n   868â†’        \n   869â†’        # Extract PR info for GraphQL query\n   870â†’        try:\n   871â†’            owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n   872â†’            \n   873â†’            # Fetch review threads with resolution status via GraphQL\n   874â†’            review_threads = fetch_review_threads_graphql(owner, repo, pr_number)\n   875â†’            \n   876â†’            if review_threads and not args.include_resolved:\n   877â†’                # Filter out resolved threads (GitHub native conversation resolution)\n   878â†’                original_thread_count = len(review_threads)\n   879â†’                review_threads = filter_resolved_threads(review_threads)\n   880â†’                filtered_thread_count = original_thread_count - len(review_threads)\n   881â†’                if filtered_thread_count > 0:\n   882â†’                    print(f\"Filtered resolved review threads: {filtered_thread_count} resolved threads hidden\", file=sys.stderr)\n   883â†’            elif review_threads:\n   884â†’                print(f\"Found {len(review_threads)} review threads (including resolved)\", file=sys.stderr)\n   885â†’                \n   886â†’        except Exception as e:\n   887â†’            print(f\"Warning: Could not fetch review thread resolution data: {e}\", file=sys.stderr)\n   888â†’            review_threads = []\n   889â†’        \n   890â†’        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n   891â†’        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n   892â†’        \n   893â†’        # Convert review threads to inline comment format and merge\n   894â†’        if review_threads:\n   895â†’            for thread in review_threads:\n   896â†’                # Convert thread to inline comment format\n   897â†’                inline_comment = {\n   898â†’                    'id': thread['id'],\n   899â†’                    'body': thread['body'],\n   900â†’                    'user': {'login': 'coderabbitai[bot]'},\n   901â†’                    'created_at': thread.get('createdAt', ''),  # Now includes timestamp from GraphQL\n   902â†’                    'path': 'unknown',  # GraphQL doesn't provide file path in this query\n   903â†’                    'original_line': 0,  # GraphQL doesn't provide line number in this query\n   904â†’                    'html_url': f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\"\n   905â†’                }\n   906â†’                coderabbit_inline_comments.append(inline_comment)\n   907â†’        \n   908â†’        # Filter out resolved comments if not including them\n   909â†’        if not args.include_resolved:\n   910â†’            original_count = len(coderabbit_inline_comments)\n   911â†’            coderabbit_inline_comments = filter_resolved_comments(coderabbit_inline_comments)\n   912â†’            filtered_count = original_count - len(coderabbit_inline_comments)\n   913â†’            if filtered_count > 0:\n   914â†’                print(f\"Filtered resolved comments: {filtered_count} resolved comments hidden\", file=sys.stderr)\n   915â†’        # Note: Review-level filtering is handled in group_comments_by_file()\n   916â†’        \n   917â†’        # Filter reviews based on options\n   918â†’        if not args.all_reviews and not args.since_commit:\n   919â†’            # Default: latest review with actual content\n   920â†’            if coderabbit_reviews:\n   921â†’                # Find the latest review with substantial content (not just continuation)\n   922â†’                latest_review = None\n   923â†’                for review in reversed(coderabbit_reviews):\n   924â†’                    body = review.get('body', '')\n   925â†’                    # Skip continuation reviews that have minimal content\n   926â†’                    if (len(body) > 100 and \n   927â†’                        'Actionable comments posted:' in body and\n   928â†’                        not body.strip().startswith('**Review continued from previous batch')):\n   929â†’                        latest_review = review\n   930â†’                        break\n   931â†’                \n   932â†’                # Fallback to chronologically latest if no substantial review found\n   933â†’                if not latest_review:\n   934â†’                    latest_review = coderabbit_reviews[-1]\n   935â†’                    \n   936â†’                coderabbit_reviews = [latest_review]\n   937â†’                \n   938â†’                # Filter inline comments to only those posted after the most recent commit\n   939â†’                latest_commit_time = fetch_latest_commit_time(pr_url)\n   940â†’                if latest_commit_time and coderabbit_inline_comments:\n   941â†’                    from datetime import datetime, timedelta\n   942â†’                    \n   943â†’                    try:\n   944â†’                        # Parse ISO format timestamps (GitHub API standard)\n   945â†’                        commit_dt = datetime.fromisoformat(latest_commit_time.replace('Z', '+00:00'))\n   946â†’                        \n   947â†’                        filtered_inline = []\n   948â†’                        for comment in coderabbit_inline_comments:\n   949â†’                            comment_time = comment.get('created_at', '')\n   950â†’                            if comment_time:\n   951â†’                                comment_dt = datetime.fromisoformat(comment_time.replace('Z', '+00:00'))\n   952â†’                                # Include comments posted after the latest commit\n   953â†’                                if comment_dt > commit_dt:\n   954â†’                                    filtered_inline.append(comment)\n   955â†’                        \n   956â†’                        original_count = len(coderabbit_inline_comments)\n   957â†’                        coderabbit_inline_comments = filtered_inline\n   958â†’                        if original_count != len(filtered_inline):\n   959â†’                            print(f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\", file=sys.stderr)\n   960â†’                            \n   961â†’                    except Exception as e:\n   962â†’                        print(f\"Warning: Could not filter inline comments by commit time: {e}\", file=sys.stderr)\n   963â†’        elif args.since_commit:\n   964â†’            # Filter reviews submitted after the specified commit\n   965â†’            # This would require additional logic to compare timestamps with commit dates\n   966â†’            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n   967â†’            sys.exit(1)\n   968â†’        \n   969â†’        if not coderabbit_reviews and not coderabbit_inline_comments:\n   970â†’            print(\"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr)\n   971â†’            sys.exit(1)\n   972â†’        \n   973â†’        # Show processing info\n   974â†’        if len(coderabbit_reviews) > 1:\n   975â†’            print(f\"Processing {len(coderabbit_reviews)} CodeRabbit reviews...\", file=sys.stderr)\n   976â†’        elif len(coderabbit_reviews) == 1:\n   977â†’            print(f\"Processing latest CodeRabbit review...\", file=sys.stderr)\n   978â†’            \n   979â†’        if coderabbit_inline_comments:\n   980â†’            print(f\"Found {len(coderabbit_inline_comments)} inline comments\", file=sys.stderr)\n   981â†’        \n   982â†’        formatted_output = format_for_llm(coderabbit_reviews, coderabbit_inline_comments, args.debug, preamble, args.include_resolved)\n   983â†’        print(formatted_output)\n   984â†’        \n   985â†’    except Exception as e:\n   986â†’        print(f\"Error: {e}\", file=sys.stderr)\n   987â†’        sys.exit(1)\n   988â†’\n   989â†’\n   990â†’if __name__ == \"__main__\":\n   991â†’    main()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"95bd6b09-194a-4194-9728-f97a0907c882","timestamp":"2025-11-13T12:14:36.362Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","content":"#!/usr/bin/env python3\n\"\"\"\nABOUTME: Extract CodeRabbit review feedback from GitHub PRs for LLM consumption\nABOUTME: Uses gh CLI to fetch PR comments and formats them as plain text\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Jesse Vincent\"\n__license__ = \"MIT\"\nimport argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom urllib.parse import urlparse\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom bs4 import BeautifulSoup\nimport html\n\n\ndef load_preamble_config() -> Optional[str]:\n    \"\"\"Load custom preamble from dotfile.\"\"\"\n    config_path = Path.home() / '.coderabbit-extractor'\n    \n    if config_path.exists():\n        try:\n            return config_path.read_text(encoding='utf-8').strip()\n        except Exception as e:\n            print(f\"Warning: Could not read config file {config_path}: {e}\", file=sys.stderr)\n            return None\n    \n    return None\n\n\ndef parse_pr_input(input_str: str) -> str:\n    \"\"\"Parse PR URL or owner/repo/number format into a full GitHub PR URL.\"\"\"\n    input_str = input_str.strip()\n    \n    # If it's already a full URL, return as-is\n    if input_str.startswith('https://github.com/'):\n        return input_str\n    \n    # Parse owner/repo/number format (e.g., \"obra/lace/278\")\n    parts = input_str.split('/')\n    if len(parts) == 3:\n        owner, repo, pr_num = parts\n        return f\"https://github.com/{owner}/{repo}/pull/{pr_num}\"\n    \n    raise ValueError(f\"Invalid PR format: {input_str}. Use 'owner/repo/number' or full URL\")\n\n\ndef extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\n    \"\"\"Extract owner, repo, and PR number from GitHub PR URL.\"\"\"\n    # Parse URL like https://github.com/owner/repo/pull/123\n    if 'github.com' in pr_url:\n        parts = pr_url.replace('https://github.com/', '').split('/')\n        if len(parts) >= 4 and parts[2] == 'pull':\n            owner, repo, _, pr_number = parts[:4]\n            try:\n                return owner, repo, int(pr_number)\n            except ValueError:\n                raise ValueError(f\"Invalid PR number: {pr_number}\")\n    \n    raise ValueError(f\"Could not parse PR URL: {pr_url}\")\n\n\ndef fetch_pr_reviews(pr_url: str) -> List[Dict[str, Any]]:\n    \"\"\"Fetch PR reviews using gh CLI.\"\"\"\n    try:\n        result = subprocess.run(\n            ['gh', 'pr', 'view', pr_url, '--json', 'reviews'],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        data = json.loads(result.stdout)\n        return data.get('reviews', [])\n    except subprocess.CalledProcessError as e:\n        print(f\"Error fetching PR: {e.stderr}\", file=sys.stderr)\n        sys.exit(1)\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON response: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\ndef fetch_latest_commit_time(pr_url: str) -> Optional[str]:\n    \"\"\"Fetch the timestamp of the most recent commit in the PR.\"\"\"\n    try:\n        # Extract owner/repo/number from URL\n        owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n        \n        # Get commits from the PR\n        result = subprocess.run(\n            ['gh', 'api', f'repos/{owner}/{repo}/pulls/{pr_number}/commits'],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        commits = json.loads(result.stdout)\n        \n        if commits:\n            # Get the most recent commit (last in the list)\n            latest_commit = commits[-1]\n            return latest_commit['commit']['committer']['date']\n        \n        return None\n    except Exception as e:\n        print(f\"Warning: Could not fetch latest commit time: {e}\", file=sys.stderr)\n        return None\n\n\ndef fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n    try:\n        # Extract owner/repo/number from URL\n        if 'github.com' in pr_url:\n            parts = pr_url.replace('https://github.com/', '').split('/')\n            if len(parts) >= 4 and parts[2] == 'pull':\n                owner, repo, _, pr_number = parts[:4]\n                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n            else:\n                raise ValueError(\"Invalid GitHub PR URL format\")\n        else:\n            raise ValueError(\"URL must be a GitHub PR URL\")\n        \n        result = subprocess.run(\n            ['gh', 'api', api_path],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        data = json.loads(result.stdout)\n        return data if isinstance(data, list) else []\n    except subprocess.CalledProcessError as e:\n        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n        return []\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n        return []\n        return []\n\n\ndef fetch_review_threads_graphql(owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:\n    \"\"\"Fetch review threads with resolution status using GitHub GraphQL API.\"\"\"\n    query = '''\n    query($owner: String!, $repo: String!, $prNumber: Int!) {\n      repository(owner: $owner, name: $repo) {\n        pullRequest(number: $prNumber) {\n          reviewThreads(first: 100) {\n            nodes {\n              id\n              isResolved\n              resolvedBy {\n                login\n              }\n              comments(first: 1) {\n                nodes {\n                  body\n                  createdAt\n                  author {\n                    login\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    '''\n    \n    variables = {\n        \"owner\": owner,\n        \"repo\": repo, \n        \"prNumber\": pr_number\n    }\n    \n    try:\n        # Use gh api to make GraphQL request\n        import json\n        result = subprocess.run([\n            'gh', 'api', 'graphql', \n            '-F', f'owner={owner}',\n            '-F', f'repo={repo}', \n            '-F', f'prNumber={pr_number}',\n            '-f', f'query={query}'\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        \n        # Extract review threads\n        threads = data.get('data', {}).get('repository', {}).get('pullRequest', {}).get('reviewThreads', {}).get('nodes', [])\n        \n        # Filter to only CodeRabbit threads and return as list of dicts\n        coderabbit_threads = []\n        for thread in threads:\n            if thread and thread.get('comments', {}).get('nodes', []):\n                comment = thread['comments']['nodes'][0]\n                author_login = comment.get('author', {}).get('login', '')\n                # Check for both coderabbitai and coderabbitai[bot] formats\n                if author_login in ['coderabbitai', 'coderabbitai[bot]']:\n                    coderabbit_threads.append({\n                        'id': thread['id'],\n                        'isResolved': thread.get('isResolved', False),\n                        'resolvedBy': thread.get('resolvedBy', {}).get('login') if thread.get('resolvedBy') else None,\n                        'body': comment.get('body', ''),\n                        'createdAt': comment.get('createdAt', ''),\n                        'author': author_login\n                    })\n        \n        return coderabbit_threads\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Warning: Could not fetch review threads via GraphQL: {e.stderr}\", file=sys.stderr)\n        return []\n    except json.JSONDecodeError as e:\n        print(f\"Warning: Could not parse GraphQL response: {e}\", file=sys.stderr)\n        return []\n    except Exception as e:\n        print(f\"Warning: Error processing GraphQL data: {e}\", file=sys.stderr)\n        return []\n\n\ndef extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter reviews to only CodeRabbit ones.\"\"\"\n    return [\n        review for review in reviews\n        if review.get('author', {}).get('login') == 'coderabbitai'\n    ]\n\n\ndef filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter out comments that have been marked as resolved.\"\"\"\n    return [\n        comment for comment in comments\n        if not is_comment_resolved(comment.get('body', ''))\n    ]\n\n\ndef filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter out review threads that have been marked as resolved via GitHub's native resolution.\"\"\"\n    return [\n        thread for thread in threads\n        if not thread.get('isResolved', False)\n    ]\n\n\ndef filter_resolved_review_content(review_body: str) -> str:\n    \"\"\"Filter out resolved suggestions from review body content.\"\"\"\n    # Look for resolved comment patterns and remove them\n    # This handles CodeRabbit's text-based resolution markers\n    \n    # Pattern to match resolved comments within the review body\n    # Look for sections that contain resolution markers\n    resolved_patterns = [\n        # Match individual comment sections that are resolved\n        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n        # Match line-specific resolved comments\n        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n    ]\n    \n    filtered_body = review_body\n    for pattern in resolved_patterns:\n        filtered_body = re.sub(pattern, '', filtered_body, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Also remove any standalone resolution lines\n    filtered_body = re.sub(r'^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?', '', filtered_body, flags=re.MULTILINE | re.IGNORECASE)\n    \n    return filtered_body.strip()\n\n\ndef is_comment_resolved(comment_body: str) -> bool:\n    \"\"\"Check if a comment has been marked as resolved/addressed.\"\"\"\n    # Look for resolution indicators in the comment body\n    resolution_patterns = [\n        r'âœ…\\s*Addressed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'âœ…\\s*Resolved\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'âœ…\\s*Fixed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'âœ…\\s*Completed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'\\[x\\]',  # Checkbox checked\n        r'\\[X\\]',  # Checkbox checked\n    ]\n    \n    for pattern in resolution_patterns:\n        if re.search(pattern, comment_body, re.IGNORECASE):\n            return True\n    \n    return False\n\n\ndef extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n    return [\n        comment for comment in comments\n        if comment.get('user', {}).get('login') == 'coderabbitai[bot]'\n    ]\n\n\ndef parse_review_type(body: str) -> str:\n    \"\"\"Determine the type of CodeRabbit review.\"\"\"\n    if 'Actionable comments posted:' in body:\n        return 'ACTIONABLE_REVIEW'\n    elif 'walkthrough_start' in body and 'walkthrough_end' in body:\n        return 'WALKTHROUGH'\n    elif '<!-- This is an auto-generated comment: summarize by coderabbit.ai -->' in body:\n        return 'SUMMARY'\n    else:\n        return 'OTHER'\n\n\ndef clean_html_artifacts(text: str) -> str:\n    \"\"\"Remove HTML comments and artifacts from text.\"\"\"\n    # Remove HTML comments\n    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n    \n    # Remove CodeRabbit warning blocks\n    text = re.sub(r'>\\s*â€¼ï¸\\s*\\*\\*IMPORTANT\\*\\*.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n    \n    # Remove other quote block artifacts  \n    text = re.sub(r'>\\s*Carefully review.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n    \n    # Remove empty lines that result from artifact removal\n    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n    \n    return text.strip()\n\n\ndef clean_diff_artifacts(diff: str) -> str:\n    \"\"\"Clean up diff artifacts and markers.\"\"\"\n    if not diff:\n        return diff\n\n    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n    diff = re.sub(r'^@@.*?@@.*?\\n', '', diff, flags=re.MULTILINE)  # Full hunk headers\n    diff = re.sub(r'^\\s*@@\\s*\\n', '', diff, flags=re.MULTILINE)    # Lone @@ lines\n\n    # Remove empty lines at start/end\n    diff = diff.strip()\n\n    return diff\n\n\ndef remove_details_sections(text: str) -> str:\n    \"\"\"Remove verbose <details> sections containing analysis chains and script outputs.\n\n    Removes complete <details>...</details> blocks to filter out:\n    - Analysis/reasoning chains\n    - Script execution outputs\n    - Internal processing logs\n\n    This helps keep the output focused on actionable feedback.\n    \"\"\"\n    # Remove all <details>...</details> blocks including nested content\n    text = re.sub(r'<details>.*?</details>', '', text, flags=re.DOTALL | re.IGNORECASE)\n\n    # Clean up any resulting excessive whitespace\n    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n\n    return text.strip()\n\n\ndef extract_file_and_lines_from_body(body: str) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"Extract file path and line range from CodeRabbit comment body.\n\n    CodeRabbit comments typically include file references like:\n    - \"In <filepath> around line X\"\n    - \"In <filepath> around lines X to Y\"\n    - \"In <filepath> around lines X-Y\"\n\n    Returns:\n        Tuple of (file_path, line_range) or (None, None) if not found\n    \"\"\"\n    # Pattern to match file path references in CodeRabbit comments\n    # Matches: \"In <filepath> around line(s) X\" or \"around lines X to Y\" or \"around lines X-Y\"\n    file_pattern = r'In\\s+([^\\s]+(?:\\.ts|\\.tsx|\\.js|\\.jsx|\\.py|\\.java|\\.go|\\.rs|\\.md|\\.json|\\.yaml|\\.yml))\\s+around\\s+lines?\\s+([\\d\\-to\\s]+)'\n\n    match = re.search(file_pattern, body, re.IGNORECASE)\n    if match:\n        file_path = match.group(1).strip()\n        line_range = match.group(2).strip()\n\n        # Normalize line range format (convert \"X to Y\" to \"X-Y\")\n        line_range = re.sub(r'(\\d+)\\s+to\\s+(\\d+)', r'\\1-\\2', line_range)\n\n        return (file_path, line_range)\n\n    return (None, None)\n\n\ndef extract_prompt_for_ai_agents(body: str) -> List[str]:\n    \"\"\"Extract 'Prompt for AI Agents' sections from review body.\"\"\"\n    prompts = []\n    \n    # Pattern to find ğŸ¤– Prompt for AI Agents sections\n    # These appear to be in <summary> tags followed by clipboard content\n    pattern = r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*<div[^>]*data-snippet-clipboard-copy-content=\"([^\"]*)\"'\n    matches = re.findall(pattern, body, re.DOTALL)\n    \n    for match in matches:\n        # Decode HTML entities\n        clean_text = (match\n                     .replace('&lt;', '<')\n                     .replace('&gt;', '>')\n                     .replace('&amp;', '&')\n                     .replace('&quot;', '\"')\n                     .replace('\\\\n', '\\n')\n                     .strip())\n        if clean_text:\n            prompts.append(clean_text)\n    \n    return prompts\n\n\n# Removed unused functions - now using parse_review_sections instead\n\n\ndef parse_review_sections(body: str) -> Dict[str, Any]:\n    \"\"\"Parse different sections from CodeRabbit review body using HTML parsing.\"\"\"\n    sections = {}\n    \n    # Extract actionable comments count from text\n    actionable_match = re.search(r'Actionable comments posted:\\s*(\\d+)', body)\n    if actionable_match:\n        sections['actionable_count'] = int(actionable_match.group(1))\n    \n    # Parse HTML structure - but be careful not to mangle code content\n    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n    soup = BeautifulSoup(body, 'html.parser')\n    \n    # Find sections by looking for <details> elements with specific summary text\n    details_elements = soup.find_all('details')\n    \n    for details in details_elements:\n        summary = details.find('summary')\n        if not summary:\n            continue\n            \n        summary_text = summary.get_text(strip=True)\n        \n        # Check for outside diff range comments (avoid emoji dependency)\n        if 'Outside diff range comments' in summary_text:\n            # Extract count from summary text\n            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n            if count_match:\n                count = int(count_match.group(1))\n                blockquote = details.find('blockquote')\n                if blockquote:\n                    # Get all content inside the blockquote\n                    content = str(blockquote)\n                    sections['outside_diff_comments'] = {'count': count, 'content': content}\n        \n        # Check for nitpick comments or duplicate comments  \n        elif 'Nitpick comments' in summary_text or 'Duplicate comments' in summary_text:\n            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n            if count_match:\n                count = int(count_match.group(1))\n                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n                # extract the raw content from the original body text\n                \n                # Find the position of this details section in the original body\n                details_start = body.find('Nitpick comments')\n                if details_start != -1:\n                    # Find where this section ends (next major section or end)\n                    section_markers = ['ğŸ“œ Review details', 'ğŸ”‡ Additional comments', '</details>\\n\\n</details>']\n                    section_end = len(body)\n                    \n                    for marker in section_markers:\n                        marker_pos = body.find(marker, details_start)\n                        if marker_pos != -1:\n                            section_end = min(section_end, marker_pos)\n                    \n                    # Extract the raw text content\n                    raw_content = body[details_start:section_end]\n                    sections['nitpick_comments'] = {'count': count, 'content': raw_content}\n        \n        # Check for review details (usually at the end)\n        elif 'Review details' in summary_text:\n            # This section contains metadata, we might want it for context\n            blockquote = details.find('blockquote')\n            if blockquote:\n                content = str(blockquote)\n                sections['review_details'] = {'content': content}\n    \n    # Extract AI prompts (look for specific patterns)\n    sections['ai_prompts'] = extract_prompt_for_ai_agents(body)\n    \n    return sections\n\n\ndef parse_file_level_comments(content: str, debug: bool = False) -> List[Dict[str, Any]]:\n    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n    comments = []\n    \n    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n    file_pattern = r'<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>'\n    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n    \n    if debug:\n        print(f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\", file=sys.stderr)\n    \n    for file_match in file_matches:\n        file_path = file_match.group(1).strip()\n        comment_count = int(file_match.group(2))\n        file_content = file_match.group(3)  # Raw blockquote content, not parsed by BeautifulSoup\n        \n        # Now parse line comments within this file content\n        # Find line comment patterns: `16-24`: **Title**\n        line_pattern = r'`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)'\n        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n        \n        if debug:\n            print(f\"DEBUG: File {file_path} has {len(line_matches)} line comments\", file=sys.stderr)\n        \n        for line_match in line_matches:\n            line_range = line_match.group(1).strip()\n            title = line_match.group(2).strip()\n            content_block = line_match.group(3).strip()\n\n            # Remove verbose <details> sections before processing\n            content_block = remove_details_sections(content_block)\n\n            # Check if this looks like a line reference\n            if re.match(r'^[\\d\\-,\\s]+$', line_range):\n                # Extract description (everything before first code block)\n                description = content_block\n                code_diff = \"\"\n                \n                code_start = content_block.find('```')\n                if code_start != -1:\n                    description = content_block[:code_start].strip()\n                    \n                    # Extract code diff from raw content (no BeautifulSoup mangling)\n                    diff_match = re.search(r'```diff\\n(.*?)\\n```', content_block, re.DOTALL)\n                    if diff_match:\n                        code_diff = diff_match.group(1).strip()\n                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n                        code_diff = html.unescape(code_diff)\n                        code_diff = clean_html_artifacts(code_diff)  # Remove any HTML comments in diffs\n                        code_diff = clean_diff_artifacts(code_diff)  # Clean up @@ markers and artifacts\n                \n                # Clean up description\n                description = clean_html_artifacts(description)  # Remove HTML comments first\n                description_lines = []\n                for line in description.split('\\n'):\n                    line = line.strip()\n                    if line and not line.startswith('Also applies to:') and line != '---':\n                        description_lines.append(line)\n                description = ' '.join(description_lines)\n                \n                comments.append({\n                    'file': file_path,\n                    'lines': line_range,\n                    'title': title,\n                    'description': description,\n                    'code_diff': code_diff\n                })\n    \n    return comments\n\n\ndef group_comments_by_file(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, include_resolved: bool = True) -> Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Group all comments by file for better organization.\"\"\"\n    file_groups = {}\n    \n    # Process review body comments\n    for review in coderabbit_reviews:\n        review_body = review['body']\n        \n        # Filter out resolved content from review body if requested\n        if not include_resolved:\n            original_body = review_body\n            review_body = filter_resolved_review_content(review_body)\n            if debug and len(original_body) != len(review_body):\n                print(f\"DEBUG: Filtered resolved content from review body (length: {len(original_body)} -> {len(review_body)})\", file=sys.stderr)\n        \n        sections = parse_review_sections(review_body)\n        \n        # Add outside diff comments\n        if 'outside_diff_comments' in sections:\n            outside_comments = parse_file_level_comments(sections['outside_diff_comments']['content'], debug)\n            for comment in outside_comments:\n                file_path = comment['file']\n                if file_path not in file_groups:\n                    file_groups[file_path] = []\n                comment['source'] = 'outside_diff'\n                file_groups[file_path].append(comment)\n        \n        # Add nitpick comments\n        if 'nitpick_comments' in sections:\n            nitpick_comments = parse_file_level_comments(sections['nitpick_comments']['content'], debug)\n            for comment in nitpick_comments:\n                file_path = comment['file']\n                if file_path not in file_groups:\n                    file_groups[file_path] = []\n                comment['source'] = 'nitpick'\n                file_groups[file_path].append(comment)\n    \n    # Process inline comments\n    if inline_comments:\n        for comment in inline_comments:\n            body = comment.get('body', '')\n\n            # Try to extract file path and line range from comment body\n            extracted_file, extracted_lines = extract_file_and_lines_from_body(body)\n\n            # Use extracted values if available, otherwise fall back to comment metadata\n            file_path = extracted_file if extracted_file else comment.get('path', 'unknown')\n            line_num = comment.get('original_line', 'unknown')\n\n            # Use extracted line range if available\n            if extracted_lines:\n                line_range = extracted_lines\n            else:\n                line_range = str(line_num) if line_num != 'unknown' and line_num != 0 else ''\n\n            if file_path not in file_groups:\n                file_groups[file_path] = []\n\n            # Convert inline comment to standard format\n            comment_type = 'ğŸ› ï¸ Refactor Suggestion' if 'ğŸ› ï¸ Refactor suggestion' in body else 'Comment'\n\n            # Extract title from body\n            title_match = re.search(r'\\*\\*(.*?)\\*\\*', body)\n            title = title_match.group(1).strip() if title_match else \"Suggestion\"\n            \n            # Extract description and diff\n            if '<details>' in body and 'Prompt for AI Agents' in body:\n                main_content = body.split('<details>')[0].strip()\n            else:\n                main_content = body\n\n            # Remove verbose <details> sections\n            main_content = remove_details_sections(main_content)\n            main_content = main_content.replace('_ğŸ› ï¸ Refactor suggestion_', '').strip()\n            main_content = clean_html_artifacts(main_content)\n            \n            # Extract AI prompt\n            ai_prompt = \"\"\n            ai_prompt_match = re.search(r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*\\n\\n```\\n(.*?)\\n```', body, re.DOTALL)\n            if ai_prompt_match:\n                ai_prompt = ai_prompt_match.group(1).strip()\n            \n            file_groups[file_path].append({\n                'lines': line_range,\n                'title': title,\n                'description': main_content,\n                'code_diff': '',  # Inline comments don't have separate diffs\n                'ai_prompt': ai_prompt,\n                'source': 'inline',\n                'comment_type': comment_type\n            })\n    \n    return file_groups\n\n\ndef format_for_llm(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, preamble: Optional[str] = None, include_resolved: bool = True) -> str:\n    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n    output = []\n    \n    # Add custom preamble if provided\n    if preamble:\n        output.append(preamble)\n        output.append(\"\")\n        output.append(\"=\" * 60)\n        output.append(\"\")\n    \n    output.append(\"# CodeRabbit Review Feedback\")\n    output.append(\"=\" * 40)\n    output.append(\"\")\n    \n    # Group all comments by file\n    file_groups = group_comments_by_file(coderabbit_reviews, inline_comments, debug, include_resolved)\n    \n    if not file_groups:\n        output.append(\"No actionable comments found.\")\n        return \"\\n\".join(output)\n    \n    # Add summary\n    total_comments = sum(len(comments) for comments in file_groups.values())\n    output.append(f\"**Total files with feedback:** {len(file_groups)}\")\n    output.append(f\"**Total comments:** {total_comments}\")\n    output.append(\"\")\n    \n    # Process each file\n    for file_path in sorted(file_groups.keys()):\n        comments = file_groups[file_path]\n        \n        output.append(f\"## {file_path}\")\n        output.append(f\"**{len(comments)} suggestion(s)**\")\n        output.append(\"\")\n        \n        # Sort comments by priority: AI prompts first, then by line number\n        def get_sort_key(comment):\n            # Priority 1: Comments with AI prompts (highest priority)\n            has_ai_prompt = bool(comment.get('ai_prompt'))\n\n            # Priority 2: Line number (for logical code flow)\n            line_num = 999\n            lines = comment.get('lines', '')\n            if lines and lines.replace('-', '').isdigit():\n                line_num = int(lines.split('-')[0])\n\n            # Return tuple: (ai_prompt_priority, line_number)\n            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n            return (0 if has_ai_prompt else 1, line_num)\n\n        comments_sorted = sorted(comments, key=get_sort_key)\n\n        for i, comment in enumerate(comments_sorted, 1):\n            # Add AI prompt indicator to title for high-priority items\n            title = comment['title']\n            if comment.get('ai_prompt'):\n                title = f\"ğŸ¤– {title}\"\n\n            # Only include line numbers if they're meaningful (not empty or \"0\")\n            lines = comment.get('lines', '')\n            if lines and lines != '0':\n                output.append(f\"### {i}. Lines {lines}: {title}\")\n            else:\n                output.append(f\"### {i}. {title}\")\n            output.append(\"\")\n            \n            # Show AI prompt, or fallback to description + diff\n            has_prompt = bool(comment.get('ai_prompt'))\n            has_diff = bool(comment.get('code_diff'))\n            \n            if debug:\n                output.append(f\"<!-- DEBUG: has_prompt={has_prompt}, has_diff={has_diff}, source={comment.get('source')} -->\")\n            \n            if has_prompt:\n                if debug:\n                    output.append(\"<!-- DEBUG: Showing prompt only, skipping description and diff -->\")\n                # AI prompt contains better explanation than description + diff\n                output.append(\"**Proposed prompt:**\")\n                output.append(\"```\")\n                output.append(comment['ai_prompt'])\n                output.append(\"```\")\n                output.append(\"\")\n            else:\n                if debug:\n                    output.append(\"<!-- DEBUG: No prompt, showing description and diff -->\")\n                \n                # Show description only if no AI prompt available\n                if comment['description'] and len(comment['description']) > 10:\n                    desc = comment['description']\n                    desc = re.sub(r'^_.*?_\\s*', '', desc)  # Remove _âš ï¸ Potential issue_ etc\n                    desc = re.sub(r'\\*\\*.*?\\*\\*\\s*', '', desc, count=1)  # Remove first **title** if duplicated\n                    desc = re.sub(r'Apply this diff:\\s*', '', desc)\n                    desc = desc.strip()\n                    \n                    if desc and desc != title:\n                        output.append(desc)\n                        output.append(\"\")\n                \n                # Show diff only if no AI prompt available\n                if has_diff:\n                    output.append(\"```diff\")\n                    output.append(comment['code_diff'])\n                    output.append(\"```\")\n                    output.append(\"\")\n            \n            if i < len(comments_sorted):\n                output.append(\"-\" * 30)\n                output.append(\"\")\n        \n        output.append(\"=\" * 60)\n        output.append(\"\")\n    \n    return \"\\n\".join(output)\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"\"\"\nCodeRabbit Review Extractor - Convert GitHub PR reviews to LLM-friendly text\n\nThis tool extracts CodeRabbit automated code review feedback from GitHub pull \nrequests and formats it as clean, organized text suitable for AI coding agents.\n\nPerfect for feeding CodeRabbit suggestions to Claude, ChatGPT, or other LLMs\nto automatically apply code improvements, refactoring suggestions, and fixes.\n        \"\"\".strip(),\n        epilog=\"\"\"\nEXAMPLES:\n  %(prog)s https://github.com/owner/repo/pull/123\n  %(prog)s owner/repo/123  \n  %(prog)s obra/lace/278 --all-reviews\n\nWHAT IT EXTRACTS:\n  â€¢ Refactor suggestions with specific code diffs\n  â€¢ AI implementation instructions (ğŸ¤– prompts) with detailed steps  \n  â€¢ Nitpick comments and code quality improvements\n  â€¢ File-organized feedback sorted by priority\n  â€¢ Clean format without HTML artifacts or noise\n\nHOW IT WORKS:\n  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n  2. Extracts both main review summaries and inline code comments\n  3. Organizes feedback by file with line-number targeting\n  4. Prioritizes actionable items with AI implementation guides\n  5. Outputs clean text ready for LLM consumption\n\nDEFAULT BEHAVIOR:\n  Processes only the LATEST CodeRabbit review to avoid overwhelming output.\n  Use --all-reviews for PRs with multiple review iterations if needed.\n\nREQUIREMENTS:\n  â€¢ GitHub CLI (gh) installed and authenticated\n  â€¢ Python 3.6+ with beautifulsoup4 package\n\nCONFIGURATION:\n  Create ~/.coderabbit-extractor with custom preamble text to prepend to output.\n  Useful for framing feedback (e.g., critical evaluation prompts for AI agents).\n        \"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    parser.add_argument('pr_input', \n                       help='GitHub PR URL or owner/repo/number format (e.g., \"owner/repo/123\")')\n    parser.add_argument('--include-resolved', \n                       action='store_true',\n                       help='Include review suggestions that have been marked as resolved (default: filter them out)')\n    parser.add_argument('--all-reviews', \n                       action='store_true',\n                       help='Extract all CodeRabbit reviews (default: latest only)')\n    parser.add_argument('--since-commit',\n                       metavar='SHA', \n                       help='Extract reviews submitted after this commit SHA')\n    parser.add_argument('--debug',\n                       action='store_true',\n                       help='Show debug annotations in output to trace processing')\n    parser.add_argument('--version',\n                       action='version',\n                       version=f'%(prog)s {__version__}')\n    \n    # Custom handling for no arguments to show helpful info\n    if len(sys.argv) == 1:\n        print(\"ğŸ¤– CodeRabbit Review Extractor\")\n        print(\"=\" * 40)\n        print()\n        print(\"WHAT THIS DOES:\")\n        print(\"  Converts CodeRabbit GitHub PR reviews into clean text for AI coding agents\")\n        print(\"  (Claude, ChatGPT, etc.) to automatically apply code suggestions.\")\n        print()\n        print(\"QUICK START:\")\n        print(\"  python3 extract-coderabbit-feedback.py https://github.com/owner/repo/pull/123\")\n        print(\"  python3 extract-coderabbit-feedback.py owner/repo/123\")\n        print()\n        print(\"For full help and options, run:\")\n        print(\"  python3 extract-coderabbit-feedback.py --help\")\n        sys.exit(1)\n    \n    args = parser.parse_args()\n    \n    try:\n        pr_url = parse_pr_input(args.pr_input)\n        \n        # Load custom preamble from dotfile\n        preamble = load_preamble_config()\n        \n        # Fetch both reviews and inline comments\n        reviews = fetch_pr_reviews(pr_url)\n        inline_comments = fetch_pr_inline_comments(pr_url)\n        \n        # Extract PR info for GraphQL query\n        try:\n            owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n            \n            # Fetch review threads with resolution status via GraphQL\n            review_threads = fetch_review_threads_graphql(owner, repo, pr_number)\n            \n            if review_threads and not args.include_resolved:\n                # Filter out resolved threads (GitHub native conversation resolution)\n                original_thread_count = len(review_threads)\n                review_threads = filter_resolved_threads(review_threads)\n                filtered_thread_count = original_thread_count - len(review_threads)\n                if filtered_thread_count > 0:\n                    print(f\"Filtered resolved review threads: {filtered_thread_count} resolved threads hidden\", file=sys.stderr)\n            elif review_threads:\n                print(f\"Found {len(review_threads)} review threads (including resolved)\", file=sys.stderr)\n                \n        except Exception as e:\n            print(f\"Warning: Could not fetch review thread resolution data: {e}\", file=sys.stderr)\n            review_threads = []\n        \n        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n        \n        # Convert review threads to inline comment format and merge\n        if review_threads:\n            for thread in review_threads:\n                # Convert thread to inline comment format\n                inline_comment = {\n                    'id': thread['id'],\n                    'body': thread['body'],\n                    'user': {'login': 'coderabbitai[bot]'},\n                    'created_at': thread.get('createdAt', ''),  # Now includes timestamp from GraphQL\n                    'path': 'unknown',  # GraphQL doesn't provide file path in this query\n                    'original_line': 0,  # GraphQL doesn't provide line number in this query\n                    'html_url': f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\"\n                }\n                coderabbit_inline_comments.append(inline_comment)\n        \n        # Filter out resolved comments if not including them\n        if not args.include_resolved:\n            original_count = len(coderabbit_inline_comments)\n            coderabbit_inline_comments = filter_resolved_comments(coderabbit_inline_comments)\n            filtered_count = original_count - len(coderabbit_inline_comments)\n            if filtered_count > 0:\n                print(f\"Filtered resolved comments: {filtered_count} resolved comments hidden\", file=sys.stderr)\n        # Note: Review-level filtering is handled in group_comments_by_file()\n        \n        # Filter reviews based on options\n        if not args.all_reviews and not args.since_commit:\n            # Default: latest review with actual content\n            if coderabbit_reviews:\n                # Find the latest review with substantial content (not just continuation)\n                latest_review = None\n                for review in reversed(coderabbit_reviews):\n                    body = review.get('body', '')\n                    # Skip continuation reviews that have minimal content\n                    if (len(body) > 100 and \n                        'Actionable comments posted:' in body and\n                        not body.strip().startswith('**Review continued from previous batch')):\n                        latest_review = review\n                        break\n                \n                # Fallback to chronologically latest if no substantial review found\n                if not latest_review:\n                    latest_review = coderabbit_reviews[-1]\n                    \n                coderabbit_reviews = [latest_review]\n                \n                # Filter inline comments to only those posted after the most recent commit\n                latest_commit_time = fetch_latest_commit_time(pr_url)\n                if latest_commit_time and coderabbit_inline_comments:\n                    from datetime import datetime, timedelta\n                    \n                    try:\n                        # Parse ISO format timestamps (GitHub API standard)\n                        commit_dt = datetime.fromisoformat(latest_commit_time.replace('Z', '+00:00'))\n                        \n                        filtered_inline = []\n                        for comment in coderabbit_inline_comments:\n                            comment_time = comment.get('created_at', '')\n                            if comment_time:\n                                comment_dt = datetime.fromisoformat(comment_time.replace('Z', '+00:00'))\n                                # Include comments posted after the latest commit\n                                if comment_dt > commit_dt:\n                                    filtered_inline.append(comment)\n                        \n                        original_count = len(coderabbit_inline_comments)\n                        coderabbit_inline_comments = filtered_inline\n                        if original_count != len(filtered_inline):\n                            print(f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\", file=sys.stderr)\n                            \n                    except Exception as e:\n                        print(f\"Warning: Could not filter inline comments by commit time: {e}\", file=sys.stderr)\n        elif args.since_commit:\n            # Filter reviews submitted after the specified commit\n            # This would require additional logic to compare timestamps with commit dates\n            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n            sys.exit(1)\n        \n        if not coderabbit_reviews and not coderabbit_inline_comments:\n            print(\"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr)\n            sys.exit(1)\n        \n        # Show processing info\n        if len(coderabbit_reviews) > 1:\n            print(f\"Processing {len(coderabbit_reviews)} CodeRabbit reviews...\", file=sys.stderr)\n        elif len(coderabbit_reviews) == 1:\n            print(f\"Processing latest CodeRabbit review...\", file=sys.stderr)\n            \n        if coderabbit_inline_comments:\n            print(f\"Found {len(coderabbit_inline_comments)} inline comments\", file=sys.stderr)\n        \n        formatted_output = format_for_llm(coderabbit_reviews, coderabbit_inline_comments, args.debug, preamble, args.include_resolved)\n        print(formatted_output)\n        \n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()","numLines":991,"startLine":1,"totalLines":991}}}
{"parentUuid":"95bd6b09-194a-4194-9728-f97a0907c882","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018sPiYDNCm5ytiGsmMeBRDn","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me examine the specific functions and API calls to understand what data is available and where it's being lost."}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":14857,"cache_read_input_tokens":8618,"cache_creation":{"ephemeral_5m_input_tokens":14857,"ephemeral_1h_input_tokens":0},"output_tokens":203,"service_tier":"standard"}},"requestId":"req_011CV5sSLxkJoXyXESDNx2Mj","type":"assistant","uuid":"13891209-41aa-44ed-8bb4-e6308b909d43","timestamp":"2025-11-13T12:14:42.903Z"}
{"parentUuid":"13891209-41aa-44ed-8bb4-e6308b909d43","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018sPiYDNCm5ytiGsmMeBRDn","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AUjQGeiuGFau9zqDJDfsJ3","name":"Bash","input":{"command":"gh api --help | head -30","description":"Check gh api help for understanding API usage"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":14857,"cache_read_input_tokens":8618,"cache_creation":{"ephemeral_5m_input_tokens":14857,"ephemeral_1h_input_tokens":0},"output_tokens":203,"service_tier":"standard"}},"requestId":"req_011CV5sSLxkJoXyXESDNx2Mj","type":"assistant","uuid":"89c3edde-8686-48c5-871d-3770fc5dc61d","timestamp":"2025-11-13T12:14:43.850Z"}
{"parentUuid":"89c3edde-8686-48c5-871d-3770fc5dc61d","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018sPiYDNCm5ytiGsmMeBRDn","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Fa61Wkr6FFgFGSpZ2BSXED","name":"WebSearch","input":{"query":"GitHub API pulls comments endpoint response fields path line position 2025"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":14857,"cache_read_input_tokens":8618,"cache_creation":{"ephemeral_5m_input_tokens":14857,"ephemeral_1h_input_tokens":0},"output_tokens":203,"service_tier":"standard"}},"requestId":"req_011CV5sSLxkJoXyXESDNx2Mj","type":"assistant","uuid":"4d6d4310-d5b2-4c4d-b2b7-d70ed9caf921","timestamp":"2025-11-13T12:14:44.735Z"}
{"parentUuid":"4d6d4310-d5b2-4c4d-b2b7-d70ed9caf921","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018sPiYDNCm5ytiGsmMeBRDn","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016C97rjsgXmaTyVFwBwPHgR","name":"WebSearch","input":{"query":"GitHub GraphQL reviewThreads path line diffHunk schema 2025"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":14857,"cache_read_input_tokens":8618,"cache_creation":{"ephemeral_5m_input_tokens":14857,"ephemeral_1h_input_tokens":0},"output_tokens":203,"service_tier":"standard"}},"requestId":"req_011CV5sSLxkJoXyXESDNx2Mj","type":"assistant","uuid":"2936e7a4-15cb-4700-8ff0-9b8755ab5eb1","timestamp":"2025-11-13T12:14:45.478Z"}
{"parentUuid":"2936e7a4-15cb-4700-8ff0-9b8755ab5eb1","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AUjQGeiuGFau9zqDJDfsJ3","type":"tool_result","content":"Makes an authenticated HTTP request to the GitHub API and prints the response.\n\nThe endpoint argument should either be a path of a GitHub API v3 endpoint, or\n`graphql` to access the GitHub API v4.\n\nPlaceholder values `{owner}`, `{repo}`, and `{branch}` in the endpoint\nargument will get replaced with values from the repository of the current\ndirectory or the repository specified in the `GH_REPO` environment variable.\nNote that in some shells, for example PowerShell, you may need to enclose\nany value that contains `{...}` in quotes to prevent the shell from\napplying special meaning to curly braces.\n\nThe `-p/--preview` flag enables opting into previews, which are feature-flagged,\nexperimental API endpoints or behaviors. The API expects opt-in via the `Accept`\nheader with format `application/vnd.github.<preview-name>-preview+json` and this\ncommand facilitates that via `--preview <preview-name>`. To send a request for\nthe corsair and scarlet witch previews, you could use `-p corsair,scarlet-witch`\nor `--preview corsair --preview scarlet-witch`.\n\nThe default HTTP request method is `GET` normally and `POST` if any parameters\nwere added. Override the method with `--method`.\n\nPass one or more `-f/--raw-field` values in `key=value` format to add static string\nparameters to the request payload. To add non-string or placeholder-determined values, see\n`-F/--field` below. Note that adding request parameters will automatically switch the\nrequest method to `POST`. To send the parameters as a `GET` query string instead, use\n`--method GET`.\n\nThe `-F/--field` flag has magic type conversion based on the format of the value:","is_error":false}]},"uuid":"1eb97b54-2407-473e-8c4d-439ad378efbc","timestamp":"2025-11-13T12:14:45.545Z","toolUseResult":{"stdout":"Makes an authenticated HTTP request to the GitHub API and prints the response.\n\nThe endpoint argument should either be a path of a GitHub API v3 endpoint, or\n`graphql` to access the GitHub API v4.\n\nPlaceholder values `{owner}`, `{repo}`, and `{branch}` in the endpoint\nargument will get replaced with values from the repository of the current\ndirectory or the repository specified in the `GH_REPO` environment variable.\nNote that in some shells, for example PowerShell, you may need to enclose\nany value that contains `{...}` in quotes to prevent the shell from\napplying special meaning to curly braces.\n\nThe `-p/--preview` flag enables opting into previews, which are feature-flagged,\nexperimental API endpoints or behaviors. The API expects opt-in via the `Accept`\nheader with format `application/vnd.github.<preview-name>-preview+json` and this\ncommand facilitates that via `--preview <preview-name>`. To send a request for\nthe corsair and scarlet witch previews, you could use `-p corsair,scarlet-witch`\nor `--preview corsair --preview scarlet-witch`.\n\nThe default HTTP request method is `GET` normally and `POST` if any parameters\nwere added. Override the method with `--method`.\n\nPass one or more `-f/--raw-field` values in `key=value` format to add static string\nparameters to the request payload. To add non-string or placeholder-determined values, see\n`-F/--field` below. Note that adding request parameters will automatically switch the\nrequest method to `POST`. To send the parameters as a `GET` query string instead, use\n`--method GET`.\n\nThe `-F/--field` flag has magic type conversion based on the format of the value:","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"1eb97b54-2407-473e-8c4d-439ad378efbc","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Fa61Wkr6FFgFGSpZ2BSXED","type":"tool_result","content":"Web search results for query: \"GitHub API pulls comments endpoint response fields path line position 2025\"\n\nLinks: [{\"title\":\"REST API endpoints for pull request reviews - GitHub Docs\",\"url\":\"https://docs.github.com/en/rest/pulls/reviews\"},{\"title\":\"REST API endpoints for pull request review comments - GitHub Docs\",\"url\":\"https://docs.github.com/en/rest/pulls/comments\"},{\"title\":\"Pull request review comments - GitHub Enterprise Server 3.2 Docs\",\"url\":\"https://docs.github.com/en/enterprise-server@3.2/rest/pulls/comments\"},{\"title\":\"REST API endpoints for pull request reviews - GitHub Enterprise Server 3.12 Docs\",\"url\":\"https://docs.github.com/en/enterprise-server@3.12/rest/pulls/reviews\"},{\"title\":\"REST API endpoints for pull request reviews - GitHub Enterprise Server 3.11 Docs\",\"url\":\"https://docs.github.com/en/enterprise-server@3.11/rest/pulls/reviews\"},{\"title\":\"REST API endpoints for issue comments - GitHub Docs\",\"url\":\"https://docs.github.com/en/rest/issues/comments\"},{\"title\":\"REST API endpoints for pull request reviews - GitHub Enterprise Server 3.8 Docs\",\"url\":\"https://docs.github.com/en/enterprise-server@3.8/rest/pulls/reviews\"},{\"title\":\"REST API endpoints for pull requests - GitHub Docs\",\"url\":\"https://docs.github.com/en/rest/pulls/pulls\"},{\"title\":\"Working with comments - GitHub Docs\",\"url\":\"https://docs.github.com/en/rest/guides/working-with-comments\"},{\"title\":\"REST API endpoints for pull request review comments - GitHub Enterprise Server 3.15 Docs\",\"url\":\"https://docs.github.com/en/enterprise-server@3.15/rest/pulls/comments\"}]\n\nI found information about the GitHub API pull request comments endpoint response fields including `path`, `line`, and `position`. Here are the key details:\n\n## Response Fields\n\nThe GitHub API allows creating comments on pull requests with fields like `body`, `commit_id`, `path`, `start_line`, `start_side`, `line`, and `side`.\n\n## Key Field Explanations\n\n**Position Field:**\nThe position value equals the number of lines down from the first \"@@\" hunk header in the file you want to add a comment. The line just below the \"@@\" line is position 1, the next line is position 2, and so on.(no content)The position in the diff continues to increase through lines of whitespace and additional hunks until the beginning of a new file.\n\n**Multi-line Comment Fields:**\nFor multi-line comments, values for start_line, original_start_line, start_side, line, original_line, and side are shown. For single-line comments, values for line, original_line, and side are shown with a null value for start_line, original_start_line, and start_side.\n\n## Endpoints\n\nThe primary endpoint for pull request review comments is:\n- `GET /repos/OWNER/REPO/pulls/PULL_NUMBER/comments` - List comments on a pull request\n- `GET /repos/OWNER/REPO/pulls/comments/COMMENT_ID` - Get a specific comment\n- `POST /repos/OWNER/REPO/pulls/PULL_NUMBER/comments` - Create a comment\n\nYou can find the complete documentation at the official GitHub REST API documentation for pull request review comments."}]},"uuid":"1e5e4e35-3c24-475e-b0fc-d637fe46e645","timestamp":"2025-11-13T13:09:31.016Z","toolUseResult":{"query":"GitHub API pulls comments endpoint response fields path line position 2025","results":[{"tool_use_id":"srvtoolu_01PdNZE8CU47uoQo8Euw2JVb","content":[{"title":"REST API endpoints for pull request reviews - GitHub Docs","url":"https://docs.github.com/en/rest/pulls/reviews"},{"title":"REST API endpoints for pull request review comments - GitHub Docs","url":"https://docs.github.com/en/rest/pulls/comments"},{"title":"Pull request review comments - GitHub Enterprise Server 3.2 Docs","url":"https://docs.github.com/en/enterprise-server@3.2/rest/pulls/comments"},{"title":"REST API endpoints for pull request reviews - GitHub Enterprise Server 3.12 Docs","url":"https://docs.github.com/en/enterprise-server@3.12/rest/pulls/reviews"},{"title":"REST API endpoints for pull request reviews - GitHub Enterprise Server 3.11 Docs","url":"https://docs.github.com/en/enterprise-server@3.11/rest/pulls/reviews"},{"title":"REST API endpoints for issue comments - GitHub Docs","url":"https://docs.github.com/en/rest/issues/comments"},{"title":"REST API endpoints for pull request reviews - GitHub Enterprise Server 3.8 Docs","url":"https://docs.github.com/en/enterprise-server@3.8/rest/pulls/reviews"},{"title":"REST API endpoints for pull requests - GitHub Docs","url":"https://docs.github.com/en/rest/pulls/pulls"},{"title":"Working with comments - GitHub Docs","url":"https://docs.github.com/en/rest/guides/working-with-comments"},{"title":"REST API endpoints for pull request review comments - GitHub Enterprise Server 3.15 Docs","url":"https://docs.github.com/en/enterprise-server@3.15/rest/pulls/comments"}]},"I found information about the GitHub API pull request comments endpoint response fields including `path`, `line`, and `position`. Here are the key details:\n\n## Response Fields\n\nThe GitHub API allows creating comments on pull requests with fields like `body`, `commit_id`, `path`, `start_line`, `start_side`, `line`, and `side`.\n\n## Key Field Explanations\n\n**Position Field:**\nThe position value equals the number of lines down from the first \"@@\" hunk header in the file you want to add a comment. The line just below the \"@@\" line is position 1, the next line is position 2, and so on.(no content)The position in the diff continues to increase through lines of whitespace and additional hunks until the beginning of a new file.\n\n**Multi-line Comment Fields:**\nFor multi-line comments, values for start_line, original_start_line, start_side, line, original_line, and side are shown. For single-line comments, values for line, original_line, and side are shown with a null value for start_line, original_start_line, and start_side.\n\n## Endpoints\n\nThe primary endpoint for pull request review comments is:\n- `GET /repos/OWNER/REPO/pulls/PULL_NUMBER/comments` - List comments on a pull request\n- `GET /repos/OWNER/REPO/pulls/comments/COMMENT_ID` - Get a specific comment\n- `POST /repos/OWNER/REPO/pulls/PULL_NUMBER/comments` - Create a comment\n\nYou can find the complete documentation at the official GitHub REST API documentation for pull request review comments."],"durationSeconds":15.735503042000346}}
{"parentUuid":"1e5e4e35-3c24-475e-b0fc-d637fe46e645","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016C97rjsgXmaTyVFwBwPHgR","type":"tool_result","content":"Web search results for query: \"GitHub GraphQL reviewThreads path line diffHunk schema 2025\"\n\nLinks: [{\"title\":\"Graphql PullRequest Timeline review threads Â· community Â· Discussion #24850\",\"url\":\"https://github.com/orgs/community/discussions/24850\"},{\"title\":\"graphql-config-examples/github-api-v4/github-schema.graphql at master Â· jimkyndemeyer/graphql-config-examples\",\"url\":\"https://github.com/jimkyndemeyer/graphql-config-examples/blob/master/github-api-v4/github-schema.graphql\"},{\"title\":\"GitHub - fabsrc/graphql-schema-diff: :page_facing_up::arrows_counterclockwise::page_facing_up: Returns the diff of two GraphQL schemas. Detects dangerous and breaking changes.\",\"url\":\"https://github.com/fabsrc/graphql-schema-diff\"},{\"title\":\"GraphQL resolved conversations Â· community Â· Discussion #24854\",\"url\":\"https://github.com/orgs/community/discussions/24854\"},{\"title\":\"Changelog - GitHub Docs\",\"url\":\"https://docs.github.com/en/graphql/overview/changelog\"},{\"title\":\"[Bug] v4 (GraphQL) API: Trouble retrieving pull request review comments Â· community Â· Discussion #24666\",\"url\":\"https://github.com/orgs/community/discussions/24666\"},{\"title\":\"graphql-schema Â· GitHub Topics Â· GitHub\",\"url\":\"https://github.com/topics/graphql-schema?o=desc&s=updated\"},{\"title\":\"GitHub GraphQL API documentation - GitHub Docs\",\"url\":\"https://docs.github.com/en/graphql\"},{\"title\":\"GitHub - Ambro17/graphql-schema-diff: ğŸ” Compare GraphQL Schemas and detect Breaking Changes\",\"url\":\"https://github.com/Ambro17/graphql-schema-diff\"},{\"title\":\"Diff and Validate GraphQL Schemas (GraphQL-Inspector)\",\"url\":\"https://the-guild.dev/graphql/inspector/docs/commands/diff\"}]\n\nI found several search results related to GitHub's GraphQL API schema, specifically regarding `reviewThreads` and related fields like `path`, `line`, and `diffHunk`. Here's what I found:\n\n## GitHub GraphQL API - reviewThreads Schema\n\nThe GitHub GraphQL schema includes a `reviewThreads` field on pull requests that returns \"The list of all review threads for this pull request.\"\n\n### Available Fields on Pull Request Review Comments:\n\nReview comments include a `diffHunk` field (String!) which represents \"The diff hunk to which the comment applies.\"(no content)Comments also have a `path` field (String!) that represents \"The path to which the comment applies.\"(no content)There's an `originalPosition` field (Int!) that represents \"The original line index in the diff to which the comment applies.\"\n\n### Example Query Usage:\n\nThe schema allows querying review threads with nested comment information including fields like `diffHunk`, `body`, `originalPosition`, `path`, and `outdated`.(no content)The PullRequestReviewThread type contains information about resolved state, accessible through fields like `isResolved`.\n\nThe schema supports retrieving detailed review thread information including the file path, line positions, and diff context for each comment in a pull request review."}]},"uuid":"ac3a3ea5-8bea-4c64-b6dc-3efd5061ea67","timestamp":"2025-11-13T13:09:31.964Z","toolUseResult":{"query":"GitHub GraphQL reviewThreads path line diffHunk schema 2025","results":[{"tool_use_id":"srvtoolu_01P45GXY6t29X1YFoPZd6nD1","content":[{"title":"Graphql PullRequest Timeline review threads Â· community Â· Discussion #24850","url":"https://github.com/orgs/community/discussions/24850"},{"title":"graphql-config-examples/github-api-v4/github-schema.graphql at master Â· jimkyndemeyer/graphql-config-examples","url":"https://github.com/jimkyndemeyer/graphql-config-examples/blob/master/github-api-v4/github-schema.graphql"},{"title":"GitHub - fabsrc/graphql-schema-diff: :page_facing_up::arrows_counterclockwise::page_facing_up: Returns the diff of two GraphQL schemas. Detects dangerous and breaking changes.","url":"https://github.com/fabsrc/graphql-schema-diff"},{"title":"GraphQL resolved conversations Â· community Â· Discussion #24854","url":"https://github.com/orgs/community/discussions/24854"},{"title":"Changelog - GitHub Docs","url":"https://docs.github.com/en/graphql/overview/changelog"},{"title":"[Bug] v4 (GraphQL) API: Trouble retrieving pull request review comments Â· community Â· Discussion #24666","url":"https://github.com/orgs/community/discussions/24666"},{"title":"graphql-schema Â· GitHub Topics Â· GitHub","url":"https://github.com/topics/graphql-schema?o=desc&s=updated"},{"title":"GitHub GraphQL API documentation - GitHub Docs","url":"https://docs.github.com/en/graphql"},{"title":"GitHub - Ambro17/graphql-schema-diff: ğŸ” Compare GraphQL Schemas and detect Breaking Changes","url":"https://github.com/Ambro17/graphql-schema-diff"},{"title":"Diff and Validate GraphQL Schemas (GraphQL-Inspector)","url":"https://the-guild.dev/graphql/inspector/docs/commands/diff"}]},"I found several search results related to GitHub's GraphQL API schema, specifically regarding `reviewThreads` and related fields like `path`, `line`, and `diffHunk`. Here's what I found:\n\n## GitHub GraphQL API - reviewThreads Schema\n\nThe GitHub GraphQL schema includes a `reviewThreads` field on pull requests that returns \"The list of all review threads for this pull request.\"\n\n### Available Fields on Pull Request Review Comments:\n\nReview comments include a `diffHunk` field (String!) which represents \"The diff hunk to which the comment applies.\"(no content)Comments also have a `path` field (String!) that represents \"The path to which the comment applies.\"(no content)There's an `originalPosition` field (Int!) that represents \"The original line index in the diff to which the comment applies.\"\n\n### Example Query Usage:\n\nThe schema allows querying review threads with nested comment information including fields like `diffHunk`, `body`, `originalPosition`, `path`, and `outdated`.(no content)The PullRequestReviewThread type contains information about resolved state, accessible through fields like `isResolved`.\n\nThe schema supports retrieving detailed review thread information including the file path, line positions, and diff context for each comment in a pull request review."],"durationSeconds":14.622697958000005}}
{"parentUuid":"ac3a3ea5-8bea-4c64-b6dc-3efd5061ea67","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01QcaYHrVe7qKdjUKUjMJjSg","type":"message","role":"assistant","content":[{"type":"text","text":"Excellent! I've found crucial information. Let me fetch more details about the GraphQL schema to confirm what fields are available:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":25934,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":25934,"ephemeral_1h_input_tokens":0},"output_tokens":167,"service_tier":"standard"}},"requestId":"req_011CV5wdGyAZeekbYZA9x8PH","type":"assistant","uuid":"2ddf06a0-6ee1-4695-86a5-2c9e1983f607","timestamp":"2025-11-13T13:09:35.732Z"}
{"parentUuid":"2ddf06a0-6ee1-4695-86a5-2c9e1983f607","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01QcaYHrVe7qKdjUKUjMJjSg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WB97t4LJ8M2hrZpQnQCJxG","name":"WebFetch","input":{"url":"https://docs.github.com/en/rest/pulls/comments","prompt":"What fields are returned in the response from GET /repos/OWNER/REPO/pulls/PULL_NUMBER/comments? Specifically list: path, line, original_line, position, original_position, diff_hunk, and any other location-related fields."}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":25934,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":25934,"ephemeral_1h_input_tokens":0},"output_tokens":167,"service_tier":"standard"}},"requestId":"req_011CV5wdGyAZeekbYZA9x8PH","type":"assistant","uuid":"0202e25d-9d68-456e-a764-e085e06aad63","timestamp":"2025-11-13T13:09:37.381Z"}
{"parentUuid":"0202e25d-9d68-456e-a764-e085e06aad63","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WB97t4LJ8M2hrZpQnQCJxG","type":"tool_result","content":"# Fields Returned by GET /repos/OWNER/REPO/pulls/PULL_NUMBER/comments\n\nBased on the GitHub API documentation, the response includes these location-related fields:\n\n**Primary Location Fields:**\n- `path` - \"The relative path of the file to which the comment applies\"\n- `line` - \"The line of the blob to which the comment applies. The last line of the range for a multi-line comment\"\n- `original_line` - \"The line of the blob to which the comment applies. The last line of the range for a multi-line comment\"\n\n**Legacy/Deprecated Fields:**\n- `position` - \"The line index in the diff to which the comment applies. This field is closing down\"\n- `original_position` - \"The index of the original line in the diff to which the comment applies. This field is closing down\"\n\n**Contextual Fields:**\n- `diff_hunk` - \"The diff of the line that the comment refers to\"\n- `side` - Indicates whether the comment applies to the `LEFT` or `RIGHT` side of the diff\n- `start_line` and `original_start_line` - Used for multi-line comments to mark the starting line\n- `start_side` - Specifies the starting side for multi-line comments\n\nAdditionally, the response contains metadata fields like `commit_id`, `original_commit_id`, `url`, `html_url`, and `pull_request_url` for reference purposes."}]},"uuid":"9b80622a-bed6-43e4-a9c0-1d68ecd9c412","timestamp":"2025-11-13T14:08:07.080Z","toolUseResult":{"bytes":440193,"code":200,"codeText":"OK","result":"# Fields Returned by GET /repos/OWNER/REPO/pulls/PULL_NUMBER/comments\n\nBased on the GitHub API documentation, the response includes these location-related fields:\n\n**Primary Location Fields:**\n- `path` - \"The relative path of the file to which the comment applies\"\n- `line` - \"The line of the blob to which the comment applies. The last line of the range for a multi-line comment\"\n- `original_line` - \"The line of the blob to which the comment applies. The last line of the range for a multi-line comment\"\n\n**Legacy/Deprecated Fields:**\n- `position` - \"The line index in the diff to which the comment applies. This field is closing down\"\n- `original_position` - \"The index of the original line in the diff to which the comment applies. This field is closing down\"\n\n**Contextual Fields:**\n- `diff_hunk` - \"The diff of the line that the comment refers to\"\n- `side` - Indicates whether the comment applies to the `LEFT` or `RIGHT` side of the diff\n- `start_line` and `original_start_line` - Used for multi-line comments to mark the starting line\n- `start_side` - Specifies the starting side for multi-line comments\n\nAdditionally, the response contains metadata fields like `commit_id`, `original_commit_id`, `url`, `html_url`, and `pull_request_url` for reference purposes.","durationMs":5180,"url":"https://docs.github.com/en/rest/pulls/comments"}}
{"parentUuid":"9b80622a-bed6-43e4-a9c0-1d68ecd9c412","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_012Q4vTvKnhRWL4qjkJVHLQF","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect! Now let me check what fields are actually being used in the code and test with a real API call to see what's actually returned:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":26757,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":26757,"ephemeral_1h_input_tokens":0},"output_tokens":175,"service_tier":"standard"}},"requestId":"req_011CV626PkRJ1wMuUTjJ23ds","type":"assistant","uuid":"b219843a-7b36-4eb3-8e89-05a1880a8cd0","timestamp":"2025-11-13T14:08:10.970Z"}
{"parentUuid":"b219843a-7b36-4eb3-8e89-05a1880a8cd0","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_012Q4vTvKnhRWL4qjkJVHLQF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VVHaFydm1z5qLMy72L5aYo","name":"Grep","input":{"pattern":"path|original_line|line|position|diff_hunk","path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","output_mode":"content","-C":3}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":26757,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":26757,"ephemeral_1h_input_tokens":0},"output_tokens":175,"service_tier":"standard"}},"requestId":"req_011CV626PkRJ1wMuUTjJ23ds","type":"assistant","uuid":"d7a5d5ed-d8c5-41e1-827f-4d652188b677","timestamp":"2025-11-13T14:08:13.042Z"}
{"parentUuid":"d7a5d5ed-d8c5-41e1-827f-4d652188b677","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VVHaFydm1z5qLMy72L5aYo","type":"tool_result","content":"13-import re\n14-import subprocess\n15-import sys\n16:from pathlib import Path\n17-from urllib.parse import urlparse\n18-from typing import List, Dict, Any, Tuple, Optional\n19-from bs4 import BeautifulSoup\n--\n22-\n23-def load_preamble_config() -> Optional[str]:\n24-    \"\"\"Load custom preamble from dotfile.\"\"\"\n25:    config_path = Path.home() / '.coderabbit-extractor'\n26-    \n27:    if config_path.exists():\n28-        try:\n29:            return config_path.read_text(encoding='utf-8').strip()\n30-        except Exception as e:\n31:            print(f\"Warning: Could not read config file {config_path}: {e}\", file=sys.stderr)\n32-            return None\n33-    \n34-    return None\n--\n111-        return None\n112-\n113-\n114:def fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n115:    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n116-    try:\n117-        # Extract owner/repo/number from URL\n118-        if 'github.com' in pr_url:\n119-            parts = pr_url.replace('https://github.com/', '').split('/')\n120-            if len(parts) >= 4 and parts[2] == 'pull':\n121-                owner, repo, _, pr_number = parts[:4]\n122:                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n123-            else:\n124-                raise ValueError(\"Invalid GitHub PR URL format\")\n125-        else:\n126-            raise ValueError(\"URL must be a GitHub PR URL\")\n127-        \n128-        result = subprocess.run(\n129:            ['gh', 'api', api_path],\n130-            capture_output=True,\n131-            text=True,\n132-            check=True\n--\n134-        data = json.loads(result.stdout)\n135-        return data if isinstance(data, list) else []\n136-    except subprocess.CalledProcessError as e:\n137:        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n138-        return []\n139-    except json.JSONDecodeError as e:\n140:        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n141-        return []\n142-        return []\n143-\n--\n257-    resolved_patterns = [\n258-        # Match individual comment sections that are resolved\n259-        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n260:        # Match line-specific resolved comments\n261-        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n262-    ]\n263-    \n--\n265-    for pattern in resolved_patterns:\n266-        filtered_body = re.sub(pattern, '', filtered_body, flags=re.DOTALL | re.IGNORECASE)\n267-    \n268:    # Also remove any standalone resolution lines\n269-    filtered_body = re.sub(r'^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?', '', filtered_body, flags=re.MULTILINE | re.IGNORECASE)\n270-    \n271-    return filtered_body.strip()\n--\n290-    return False\n291-\n292-\n293:def extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n294:    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n295-    return [\n296-        comment for comment in comments\n297-        if comment.get('user', {}).get('login') == 'coderabbitai[bot]'\n--\n321-    # Remove other quote block artifacts  \n322-    text = re.sub(r'>\\s*Carefully review.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n323-    \n324:    # Remove empty lines that result from artifact removal\n325-    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n326-    \n327-    return text.strip()\n--\n332-    if not diff:\n333-        return diff\n334-\n335:    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n336-    diff = re.sub(r'^@@.*?@@.*?\\n', '', diff, flags=re.MULTILINE)  # Full hunk headers\n337:    diff = re.sub(r'^\\s*@@\\s*\\n', '', diff, flags=re.MULTILINE)    # Lone @@ lines\n338-\n339:    # Remove empty lines at start/end\n340-    diff = diff.strip()\n341-\n342-    return diff\n--\n361-    return text.strip()\n362-\n363-\n364:def extract_file_and_lines_from_body(body: str) -> Tuple[Optional[str], Optional[str]]:\n365:    \"\"\"Extract file path and line range from CodeRabbit comment body.\n366-\n367-    CodeRabbit comments typically include file references like:\n368:    - \"In <filepath> around line X\"\n369:    - \"In <filepath> around lines X to Y\"\n370:    - \"In <filepath> around lines X-Y\"\n371-\n372-    Returns:\n373:        Tuple of (file_path, line_range) or (None, None) if not found\n374-    \"\"\"\n375:    # Pattern to match file path references in CodeRabbit comments\n376:    # Matches: \"In <filepath> around line(s) X\" or \"around lines X to Y\" or \"around lines X-Y\"\n377:    file_pattern = r'In\\s+([^\\s]+(?:\\.ts|\\.tsx|\\.js|\\.jsx|\\.py|\\.java|\\.go|\\.rs|\\.md|\\.json|\\.yaml|\\.yml))\\s+around\\s+lines?\\s+([\\d\\-to\\s]+)'\n378-\n379-    match = re.search(file_pattern, body, re.IGNORECASE)\n380-    if match:\n381:        file_path = match.group(1).strip()\n382:        line_range = match.group(2).strip()\n383-\n384:        # Normalize line range format (convert \"X to Y\" to \"X-Y\")\n385:        line_range = re.sub(r'(\\d+)\\s+to\\s+(\\d+)', r'\\1-\\2', line_range)\n386-\n387:        return (file_path, line_range)\n388-\n389-    return (None, None)\n390-\n--\n459-                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n460-                # extract the raw content from the original body text\n461-                \n462:                # Find the position of this details section in the original body\n463-                details_start = body.find('Nitpick comments')\n464-                if details_start != -1:\n465-                    # Find where this section ends (next major section or end)\n--\n502-        print(f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\", file=sys.stderr)\n503-    \n504-    for file_match in file_matches:\n505:        file_path = file_match.group(1).strip()\n506-        comment_count = int(file_match.group(2))\n507-        file_content = file_match.group(3)  # Raw blockquote content, not parsed by BeautifulSoup\n508-        \n509:        # Now parse line comments within this file content\n510:        # Find line comment patterns: `16-24`: **Title**\n511:        line_pattern = r'`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)'\n512:        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n513-        \n514-        if debug:\n515:            print(f\"DEBUG: File {file_path} has {len(line_matches)} line comments\", file=sys.stderr)\n516-        \n517:        for line_match in line_matches:\n518:            line_range = line_match.group(1).strip()\n519:            title = line_match.group(2).strip()\n520:            content_block = line_match.group(3).strip()\n521-\n522-            # Remove verbose <details> sections before processing\n523-            content_block = remove_details_sections(content_block)\n524-\n525:            # Check if this looks like a line reference\n526:            if re.match(r'^[\\d\\-,\\s]+$', line_range):\n527-                # Extract description (everything before first code block)\n528-                description = content_block\n529-                code_diff = \"\"\n--\n543-                \n544-                # Clean up description\n545-                description = clean_html_artifacts(description)  # Remove HTML comments first\n546:                description_lines = []\n547:                for line in description.split('\\n'):\n548:                    line = line.strip()\n549:                    if line and not line.startswith('Also applies to:') and line != '---':\n550:                        description_lines.append(line)\n551:                description = ' '.join(description_lines)\n552-                \n553-                comments.append({\n554:                    'file': file_path,\n555:                    'lines': line_range,\n556-                    'title': title,\n557-                    'description': description,\n558-                    'code_diff': code_diff\n--\n561-    return comments\n562-\n563-\n564:def group_comments_by_file(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, include_resolved: bool = True) -> Dict[str, List[Dict[str, Any]]]:\n565-    \"\"\"Group all comments by file for better organization.\"\"\"\n566-    file_groups = {}\n567-    \n--\n582-        if 'outside_diff_comments' in sections:\n583-            outside_comments = parse_file_level_comments(sections['outside_diff_comments']['content'], debug)\n584-            for comment in outside_comments:\n585:                file_path = comment['file']\n586:                if file_path not in file_groups:\n587:                    file_groups[file_path] = []\n588-                comment['source'] = 'outside_diff'\n589:                file_groups[file_path].append(comment)\n590-        \n591-        # Add nitpick comments\n592-        if 'nitpick_comments' in sections:\n593-            nitpick_comments = parse_file_level_comments(sections['nitpick_comments']['content'], debug)\n594-            for comment in nitpick_comments:\n595:                file_path = comment['file']\n596:                if file_path not in file_groups:\n597:                    file_groups[file_path] = []\n598-                comment['source'] = 'nitpick'\n599:                file_groups[file_path].append(comment)\n600-    \n601:    # Process inline comments\n602:    if inline_comments:\n603:        for comment in inline_comments:\n604-            body = comment.get('body', '')\n605-\n606:            # Try to extract file path and line range from comment body\n607:            extracted_file, extracted_lines = extract_file_and_lines_from_body(body)\n608-\n609-            # Use extracted values if available, otherwise fall back to comment metadata\n610:            file_path = extracted_file if extracted_file else comment.get('path', 'unknown')\n611:            line_num = comment.get('original_line', 'unknown')\n612-\n613:            # Use extracted line range if available\n614:            if extracted_lines:\n615:                line_range = extracted_lines\n616-            else:\n617:                line_range = str(line_num) if line_num != 'unknown' and line_num != 0 else ''\n618-\n619:            if file_path not in file_groups:\n620:                file_groups[file_path] = []\n621-\n622:            # Convert inline comment to standard format\n623-            comment_type = 'ğŸ› ï¸ Refactor Suggestion' if 'ğŸ› ï¸ Refactor suggestion' in body else 'Comment'\n624-\n625-            # Extract title from body\n--\n643-            if ai_prompt_match:\n644-                ai_prompt = ai_prompt_match.group(1).strip()\n645-            \n646:            file_groups[file_path].append({\n647:                'lines': line_range,\n648-                'title': title,\n649-                'description': main_content,\n650:                'code_diff': '',  # Inline comments don't have separate diffs\n651-                'ai_prompt': ai_prompt,\n652:                'source': 'inline',\n653-                'comment_type': comment_type\n654-            })\n655-    \n656-    return file_groups\n657-\n658-\n659:def format_for_llm(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, preamble: Optional[str] = None, include_resolved: bool = True) -> str:\n660-    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n661-    output = []\n662-    \n--\n672-    output.append(\"\")\n673-    \n674-    # Group all comments by file\n675:    file_groups = group_comments_by_file(coderabbit_reviews, inline_comments, debug, include_resolved)\n676-    \n677-    if not file_groups:\n678-        output.append(\"No actionable comments found.\")\n--\n685-    output.append(\"\")\n686-    \n687-    # Process each file\n688:    for file_path in sorted(file_groups.keys()):\n689:        comments = file_groups[file_path]\n690-        \n691:        output.append(f\"## {file_path}\")\n692-        output.append(f\"**{len(comments)} suggestion(s)**\")\n693-        output.append(\"\")\n694-        \n695:        # Sort comments by priority: AI prompts first, then by line number\n696-        def get_sort_key(comment):\n697-            # Priority 1: Comments with AI prompts (highest priority)\n698-            has_ai_prompt = bool(comment.get('ai_prompt'))\n699-\n700-            # Priority 2: Line number (for logical code flow)\n701:            line_num = 999\n702:            lines = comment.get('lines', '')\n703:            if lines and lines.replace('-', '').isdigit():\n704:                line_num = int(lines.split('-')[0])\n705-\n706:            # Return tuple: (ai_prompt_priority, line_number)\n707-            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n708:            return (0 if has_ai_prompt else 1, line_num)\n709-\n710-        comments_sorted = sorted(comments, key=get_sort_key)\n711-\n--\n715-            if comment.get('ai_prompt'):\n716-                title = f\"ğŸ¤– {title}\"\n717-\n718:            # Only include line numbers if they're meaningful (not empty or \"0\")\n719:            lines = comment.get('lines', '')\n720:            if lines and lines != '0':\n721:                output.append(f\"### {i}. Lines {lines}: {title}\")\n722-            else:\n723-                output.append(f\"### {i}. {title}\")\n724-            output.append(\"\")\n--\n799-\n800-HOW IT WORKS:\n801-  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n802:  2. Extracts both main review summaries and inline code comments\n803:  3. Organizes feedback by file with line-number targeting\n804-  4. Prioritizes actionable items with AI implementation guides\n805-  5. Outputs clean text ready for LLM consumption\n806-\n--\n862-        # Load custom preamble from dotfile\n863-        preamble = load_preamble_config()\n864-        \n865:        # Fetch both reviews and inline comments\n866-        reviews = fetch_pr_reviews(pr_url)\n867:        inline_comments = fetch_pr_inline_comments(pr_url)\n868-        \n869-        # Extract PR info for GraphQL query\n870-        try:\n--\n888-            review_threads = []\n889-        \n890-        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n891:        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n892-        \n893:        # Convert review threads to inline comment format and merge\n894-        if review_threads:\n895-            for thread in review_threads:\n896:                # Convert thread to inline comment format\n897:                inline_comment = {\n898-                    'id': thread['id'],\n899-                    'body': thread['body'],\n900-                    'user': {'login': 'coderabbitai[bot]'},\n901-                    'created_at': thread.get('createdAt', ''),  # Now includes timestamp from GraphQL\n902:                    'path': 'unknown',  # GraphQL doesn't provide file path in this query\n903:                    'original_line': 0,  # GraphQL doesn't provide line number in this query\n904-                    'html_url': f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\"\n905-                }\n906:                coderabbit_inline_comments.append(inline_comment)\n907-        \n908-        # Filter out resolved comments if not including them\n909-        if not args.include_resolved:\n910:            original_count = len(coderabbit_inline_comments)\n911:            coderabbit_inline_comments = filter_resolved_comments(coderabbit_inline_comments)\n912:            filtered_count = original_count - len(coderabbit_inline_comments)\n913-            if filtered_count > 0:\n914-                print(f\"Filtered resolved comments: {filtered_count} resolved comments hidden\", file=sys.stderr)\n915-        # Note: Review-level filtering is handled in group_comments_by_file()\n--\n935-                    \n936-                coderabbit_reviews = [latest_review]\n937-                \n938:                # Filter inline comments to only those posted after the most recent commit\n939-                latest_commit_time = fetch_latest_commit_time(pr_url)\n940:                if latest_commit_time and coderabbit_inline_comments:\n941-                    from datetime import datetime, timedelta\n942-                    \n943-                    try:\n944-                        # Parse ISO format timestamps (GitHub API standard)\n945-                        commit_dt = datetime.fromisoformat(latest_commit_time.replace('Z', '+00:00'))\n946-                        \n947:                        filtered_inline = []\n948:                        for comment in coderabbit_inline_comments:\n949-                            comment_time = comment.get('created_at', '')\n950-                            if comment_time:\n951-                                comment_dt = datetime.fromisoformat(comment_time.replace('Z', '+00:00'))\n952-                                # Include comments posted after the latest commit\n953-                                if comment_dt > commit_dt:\n954:                                    filtered_inline.append(comment)\n955-                        \n956:                        original_count = len(coderabbit_inline_comments)\n957:                        coderabbit_inline_comments = filtered_inline\n958:                        if original_count != len(filtered_inline):\n959:                            print(f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\", file=sys.stderr)\n960-                            \n961-                    except Exception as e:\n962:                        print(f\"Warning: Could not filter inline comments by commit time: {e}\", file=sys.stderr)\n963-        elif args.since_commit:\n964-            # Filter reviews submitted after the specified commit\n965-            # This would require additional logic to compare timestamps with commit dates\n966-            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n967-            sys.exit(1)\n968-        \n969:        if not coderabbit_reviews and not coderabbit_inline_comments:\n970-            print(\"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr)\n971-            sys.exit(1)\n972-        \n--\n976-        elif len(coderabbit_reviews) == 1:\n977-            print(f\"Processing latest CodeRabbit review...\", file=sys.stderr)\n978-            \n979:        if coderabbit_inline_comments:\n980:            print(f\"Found {len(coderabbit_inline_comments)} inline comments\", file=sys.stderr)\n981-        \n982:        formatted_output = format_for_llm(coderabbit_reviews, coderabbit_inline_comments, args.debug, preamble, args.include_resolved)\n983-        print(formatted_output)\n984-        \n985-    except Exception as e:"}]},"uuid":"9aeefdf6-efae-4258-8a8e-e9bdf0d2927f","timestamp":"2025-11-13T14:08:13.474Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"13-import re\n14-import subprocess\n15-import sys\n16:from pathlib import Path\n17-from urllib.parse import urlparse\n18-from typing import List, Dict, Any, Tuple, Optional\n19-from bs4 import BeautifulSoup\n--\n22-\n23-def load_preamble_config() -> Optional[str]:\n24-    \"\"\"Load custom preamble from dotfile.\"\"\"\n25:    config_path = Path.home() / '.coderabbit-extractor'\n26-    \n27:    if config_path.exists():\n28-        try:\n29:            return config_path.read_text(encoding='utf-8').strip()\n30-        except Exception as e:\n31:            print(f\"Warning: Could not read config file {config_path}: {e}\", file=sys.stderr)\n32-            return None\n33-    \n34-    return None\n--\n111-        return None\n112-\n113-\n114:def fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n115:    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n116-    try:\n117-        # Extract owner/repo/number from URL\n118-        if 'github.com' in pr_url:\n119-            parts = pr_url.replace('https://github.com/', '').split('/')\n120-            if len(parts) >= 4 and parts[2] == 'pull':\n121-                owner, repo, _, pr_number = parts[:4]\n122:                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n123-            else:\n124-                raise ValueError(\"Invalid GitHub PR URL format\")\n125-        else:\n126-            raise ValueError(\"URL must be a GitHub PR URL\")\n127-        \n128-        result = subprocess.run(\n129:            ['gh', 'api', api_path],\n130-            capture_output=True,\n131-            text=True,\n132-            check=True\n--\n134-        data = json.loads(result.stdout)\n135-        return data if isinstance(data, list) else []\n136-    except subprocess.CalledProcessError as e:\n137:        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n138-        return []\n139-    except json.JSONDecodeError as e:\n140:        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n141-        return []\n142-        return []\n143-\n--\n257-    resolved_patterns = [\n258-        # Match individual comment sections that are resolved\n259-        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n260:        # Match line-specific resolved comments\n261-        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n262-    ]\n263-    \n--\n265-    for pattern in resolved_patterns:\n266-        filtered_body = re.sub(pattern, '', filtered_body, flags=re.DOTALL | re.IGNORECASE)\n267-    \n268:    # Also remove any standalone resolution lines\n269-    filtered_body = re.sub(r'^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?', '', filtered_body, flags=re.MULTILINE | re.IGNORECASE)\n270-    \n271-    return filtered_body.strip()\n--\n290-    return False\n291-\n292-\n293:def extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n294:    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n295-    return [\n296-        comment for comment in comments\n297-        if comment.get('user', {}).get('login') == 'coderabbitai[bot]'\n--\n321-    # Remove other quote block artifacts  \n322-    text = re.sub(r'>\\s*Carefully review.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n323-    \n324:    # Remove empty lines that result from artifact removal\n325-    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n326-    \n327-    return text.strip()\n--\n332-    if not diff:\n333-        return diff\n334-\n335:    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n336-    diff = re.sub(r'^@@.*?@@.*?\\n', '', diff, flags=re.MULTILINE)  # Full hunk headers\n337:    diff = re.sub(r'^\\s*@@\\s*\\n', '', diff, flags=re.MULTILINE)    # Lone @@ lines\n338-\n339:    # Remove empty lines at start/end\n340-    diff = diff.strip()\n341-\n342-    return diff\n--\n361-    return text.strip()\n362-\n363-\n364:def extract_file_and_lines_from_body(body: str) -> Tuple[Optional[str], Optional[str]]:\n365:    \"\"\"Extract file path and line range from CodeRabbit comment body.\n366-\n367-    CodeRabbit comments typically include file references like:\n368:    - \"In <filepath> around line X\"\n369:    - \"In <filepath> around lines X to Y\"\n370:    - \"In <filepath> around lines X-Y\"\n371-\n372-    Returns:\n373:        Tuple of (file_path, line_range) or (None, None) if not found\n374-    \"\"\"\n375:    # Pattern to match file path references in CodeRabbit comments\n376:    # Matches: \"In <filepath> around line(s) X\" or \"around lines X to Y\" or \"around lines X-Y\"\n377:    file_pattern = r'In\\s+([^\\s]+(?:\\.ts|\\.tsx|\\.js|\\.jsx|\\.py|\\.java|\\.go|\\.rs|\\.md|\\.json|\\.yaml|\\.yml))\\s+around\\s+lines?\\s+([\\d\\-to\\s]+)'\n378-\n379-    match = re.search(file_pattern, body, re.IGNORECASE)\n380-    if match:\n381:        file_path = match.group(1).strip()\n382:        line_range = match.group(2).strip()\n383-\n384:        # Normalize line range format (convert \"X to Y\" to \"X-Y\")\n385:        line_range = re.sub(r'(\\d+)\\s+to\\s+(\\d+)', r'\\1-\\2', line_range)\n386-\n387:        return (file_path, line_range)\n388-\n389-    return (None, None)\n390-\n--\n459-                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n460-                # extract the raw content from the original body text\n461-                \n462:                # Find the position of this details section in the original body\n463-                details_start = body.find('Nitpick comments')\n464-                if details_start != -1:\n465-                    # Find where this section ends (next major section or end)\n--\n502-        print(f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\", file=sys.stderr)\n503-    \n504-    for file_match in file_matches:\n505:        file_path = file_match.group(1).strip()\n506-        comment_count = int(file_match.group(2))\n507-        file_content = file_match.group(3)  # Raw blockquote content, not parsed by BeautifulSoup\n508-        \n509:        # Now parse line comments within this file content\n510:        # Find line comment patterns: `16-24`: **Title**\n511:        line_pattern = r'`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)'\n512:        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n513-        \n514-        if debug:\n515:            print(f\"DEBUG: File {file_path} has {len(line_matches)} line comments\", file=sys.stderr)\n516-        \n517:        for line_match in line_matches:\n518:            line_range = line_match.group(1).strip()\n519:            title = line_match.group(2).strip()\n520:            content_block = line_match.group(3).strip()\n521-\n522-            # Remove verbose <details> sections before processing\n523-            content_block = remove_details_sections(content_block)\n524-\n525:            # Check if this looks like a line reference\n526:            if re.match(r'^[\\d\\-,\\s]+$', line_range):\n527-                # Extract description (everything before first code block)\n528-                description = content_block\n529-                code_diff = \"\"\n--\n543-                \n544-                # Clean up description\n545-                description = clean_html_artifacts(description)  # Remove HTML comments first\n546:                description_lines = []\n547:                for line in description.split('\\n'):\n548:                    line = line.strip()\n549:                    if line and not line.startswith('Also applies to:') and line != '---':\n550:                        description_lines.append(line)\n551:                description = ' '.join(description_lines)\n552-                \n553-                comments.append({\n554:                    'file': file_path,\n555:                    'lines': line_range,\n556-                    'title': title,\n557-                    'description': description,\n558-                    'code_diff': code_diff\n--\n561-    return comments\n562-\n563-\n564:def group_comments_by_file(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, include_resolved: bool = True) -> Dict[str, List[Dict[str, Any]]]:\n565-    \"\"\"Group all comments by file for better organization.\"\"\"\n566-    file_groups = {}\n567-    \n--\n582-        if 'outside_diff_comments' in sections:\n583-            outside_comments = parse_file_level_comments(sections['outside_diff_comments']['content'], debug)\n584-            for comment in outside_comments:\n585:                file_path = comment['file']\n586:                if file_path not in file_groups:\n587:                    file_groups[file_path] = []\n588-                comment['source'] = 'outside_diff'\n589:                file_groups[file_path].append(comment)\n590-        \n591-        # Add nitpick comments\n592-        if 'nitpick_comments' in sections:\n593-            nitpick_comments = parse_file_level_comments(sections['nitpick_comments']['content'], debug)\n594-            for comment in nitpick_comments:\n595:                file_path = comment['file']\n596:                if file_path not in file_groups:\n597:                    file_groups[file_path] = []\n598-                comment['source'] = 'nitpick'\n599:                file_groups[file_path].append(comment)\n600-    \n601:    # Process inline comments\n602:    if inline_comments:\n603:        for comment in inline_comments:\n604-            body = comment.get('body', '')\n605-\n606:            # Try to extract file path and line range from comment body\n607:            extracted_file, extracted_lines = extract_file_and_lines_from_body(body)\n608-\n609-            # Use extracted values if available, otherwise fall back to comment metadata\n610:            file_path = extracted_file if extracted_file else comment.get('path', 'unknown')\n611:            line_num = comment.get('original_line', 'unknown')\n612-\n613:            # Use extracted line range if available\n614:            if extracted_lines:\n615:                line_range = extracted_lines\n616-            else:\n617:                line_range = str(line_num) if line_num != 'unknown' and line_num != 0 else ''\n618-\n619:            if file_path not in file_groups:\n620:                file_groups[file_path] = []\n621-\n622:            # Convert inline comment to standard format\n623-            comment_type = 'ğŸ› ï¸ Refactor Suggestion' if 'ğŸ› ï¸ Refactor suggestion' in body else 'Comment'\n624-\n625-            # Extract title from body\n--\n643-            if ai_prompt_match:\n644-                ai_prompt = ai_prompt_match.group(1).strip()\n645-            \n646:            file_groups[file_path].append({\n647:                'lines': line_range,\n648-                'title': title,\n649-                'description': main_content,\n650:                'code_diff': '',  # Inline comments don't have separate diffs\n651-                'ai_prompt': ai_prompt,\n652:                'source': 'inline',\n653-                'comment_type': comment_type\n654-            })\n655-    \n656-    return file_groups\n657-\n658-\n659:def format_for_llm(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, preamble: Optional[str] = None, include_resolved: bool = True) -> str:\n660-    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n661-    output = []\n662-    \n--\n672-    output.append(\"\")\n673-    \n674-    # Group all comments by file\n675:    file_groups = group_comments_by_file(coderabbit_reviews, inline_comments, debug, include_resolved)\n676-    \n677-    if not file_groups:\n678-        output.append(\"No actionable comments found.\")\n--\n685-    output.append(\"\")\n686-    \n687-    # Process each file\n688:    for file_path in sorted(file_groups.keys()):\n689:        comments = file_groups[file_path]\n690-        \n691:        output.append(f\"## {file_path}\")\n692-        output.append(f\"**{len(comments)} suggestion(s)**\")\n693-        output.append(\"\")\n694-        \n695:        # Sort comments by priority: AI prompts first, then by line number\n696-        def get_sort_key(comment):\n697-            # Priority 1: Comments with AI prompts (highest priority)\n698-            has_ai_prompt = bool(comment.get('ai_prompt'))\n699-\n700-            # Priority 2: Line number (for logical code flow)\n701:            line_num = 999\n702:            lines = comment.get('lines', '')\n703:            if lines and lines.replace('-', '').isdigit():\n704:                line_num = int(lines.split('-')[0])\n705-\n706:            # Return tuple: (ai_prompt_priority, line_number)\n707-            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n708:            return (0 if has_ai_prompt else 1, line_num)\n709-\n710-        comments_sorted = sorted(comments, key=get_sort_key)\n711-\n--\n715-            if comment.get('ai_prompt'):\n716-                title = f\"ğŸ¤– {title}\"\n717-\n718:            # Only include line numbers if they're meaningful (not empty or \"0\")\n719:            lines = comment.get('lines', '')\n720:            if lines and lines != '0':\n721:                output.append(f\"### {i}. Lines {lines}: {title}\")\n722-            else:\n723-                output.append(f\"### {i}. {title}\")\n724-            output.append(\"\")\n--\n799-\n800-HOW IT WORKS:\n801-  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n802:  2. Extracts both main review summaries and inline code comments\n803:  3. Organizes feedback by file with line-number targeting\n804-  4. Prioritizes actionable items with AI implementation guides\n805-  5. Outputs clean text ready for LLM consumption\n806-\n--\n862-        # Load custom preamble from dotfile\n863-        preamble = load_preamble_config()\n864-        \n865:        # Fetch both reviews and inline comments\n866-        reviews = fetch_pr_reviews(pr_url)\n867:        inline_comments = fetch_pr_inline_comments(pr_url)\n868-        \n869-        # Extract PR info for GraphQL query\n870-        try:\n--\n888-            review_threads = []\n889-        \n890-        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n891:        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n892-        \n893:        # Convert review threads to inline comment format and merge\n894-        if review_threads:\n895-            for thread in review_threads:\n896:                # Convert thread to inline comment format\n897:                inline_comment = {\n898-                    'id': thread['id'],\n899-                    'body': thread['body'],\n900-                    'user': {'login': 'coderabbitai[bot]'},\n901-                    'created_at': thread.get('createdAt', ''),  # Now includes timestamp from GraphQL\n902:                    'path': 'unknown',  # GraphQL doesn't provide file path in this query\n903:                    'original_line': 0,  # GraphQL doesn't provide line number in this query\n904-                    'html_url': f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\"\n905-                }\n906:                coderabbit_inline_comments.append(inline_comment)\n907-        \n908-        # Filter out resolved comments if not including them\n909-        if not args.include_resolved:\n910:            original_count = len(coderabbit_inline_comments)\n911:            coderabbit_inline_comments = filter_resolved_comments(coderabbit_inline_comments)\n912:            filtered_count = original_count - len(coderabbit_inline_comments)\n913-            if filtered_count > 0:\n914-                print(f\"Filtered resolved comments: {filtered_count} resolved comments hidden\", file=sys.stderr)\n915-        # Note: Review-level filtering is handled in group_comments_by_file()\n--\n935-                    \n936-                coderabbit_reviews = [latest_review]\n937-                \n938:                # Filter inline comments to only those posted after the most recent commit\n939-                latest_commit_time = fetch_latest_commit_time(pr_url)\n940:                if latest_commit_time and coderabbit_inline_comments:\n941-                    from datetime import datetime, timedelta\n942-                    \n943-                    try:\n944-                        # Parse ISO format timestamps (GitHub API standard)\n945-                        commit_dt = datetime.fromisoformat(latest_commit_time.replace('Z', '+00:00'))\n946-                        \n947:                        filtered_inline = []\n948:                        for comment in coderabbit_inline_comments:\n949-                            comment_time = comment.get('created_at', '')\n950-                            if comment_time:\n951-                                comment_dt = datetime.fromisoformat(comment_time.replace('Z', '+00:00'))\n952-                                # Include comments posted after the latest commit\n953-                                if comment_dt > commit_dt:\n954:                                    filtered_inline.append(comment)\n955-                        \n956:                        original_count = len(coderabbit_inline_comments)\n957:                        coderabbit_inline_comments = filtered_inline\n958:                        if original_count != len(filtered_inline):\n959:                            print(f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\", file=sys.stderr)\n960-                            \n961-                    except Exception as e:\n962:                        print(f\"Warning: Could not filter inline comments by commit time: {e}\", file=sys.stderr)\n963-        elif args.since_commit:\n964-            # Filter reviews submitted after the specified commit\n965-            # This would require additional logic to compare timestamps with commit dates\n966-            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n967-            sys.exit(1)\n968-        \n969:        if not coderabbit_reviews and not coderabbit_inline_comments:\n970-            print(\"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr)\n971-            sys.exit(1)\n972-        \n--\n976-        elif len(coderabbit_reviews) == 1:\n977-            print(f\"Processing latest CodeRabbit review...\", file=sys.stderr)\n978-            \n979:        if coderabbit_inline_comments:\n980:            print(f\"Found {len(coderabbit_inline_comments)} inline comments\", file=sys.stderr)\n981-        \n982:        formatted_output = format_for_llm(coderabbit_reviews, coderabbit_inline_comments, args.debug, preamble, args.include_resolved)\n983-        print(formatted_output)\n984-        \n985-    except Exception as e:","numLines":404}}
{"parentUuid":"9aeefdf6-efae-4258-8a8e-e9bdf0d2927f","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"db734024","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Xm6V3eXDVMjHcFTGsuxeNU","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect! Now I have a complete understanding of the issue. Let me create a comprehensive analysis report:\n\n## Investigation Results: Why Inline Comments Are Not Properly Attributed to Files\n\n### Root Cause Analysis\n\nI've identified the **exact location and cause** of the data loss. The problem is in **lines 893-906** of `extract-coderabbit-feedback.py`.\n\n### The Problem\n\n**Line 902-903: GraphQL threads are being converted with hardcoded 'unknown' values:**\n\n```python\n'path': 'unknown',  # GraphQL doesn't provide file path in this query\n'original_line': 0,  # GraphQL doesn't provide line number in this query\n```\n\n### What's Actually Happening\n\n1. **REST API (`fetch_pr_inline_comments`, lines 114-142)** \n   - **API Endpoint**: `repos/{owner}/{repo}/pulls/{pr_number}/comments`\n   - **Fields Available**: âœ… `path`, âœ… `line`, âœ… `original_line`, âœ… `diff_hunk`, `position`, `original_position`\n   - **What the code does**: Correctly fetches ALL inline comments with full metadata\n   - **Result**: Creates `inline_comments` list with proper `path` and `original_line` data\n\n2. **GraphQL API (`fetch_review_threads_graphql`, lines 145-223)**\n   - **Query Used**: Lines 147-172 query only: `id`, `isResolved`, `resolvedBy`, `comments(body, createdAt, author)`\n   - **Fields Available but NOT Queried**: âŒ `path`, âŒ `originalPosition`, âŒ `diffHunk` (these exist in the schema!)\n   - **What the code does**: Fetches review threads but doesn't request file/line metadata\n   - **Result**: Creates `review_threads` list without location data\n\n3. **The Conversion Bug (lines 893-906)**\n   - GraphQL threads are converted to \"inline comment format\"\n   - **Hardcoded values**: `'path': 'unknown'`, `'original_line': 0`\n   - These converted threads are **APPENDED** to `coderabbit_inline_comments` list\n   - **Critical**: The comment from GraphQL that says \"Critical: Skipped tests...\" gets `path='unknown'`\n\n4. **Processing (lines 601-654 in `group_comments_by_file`)**\n   - Line 610: `file_path = extracted_file if extracted_file else comment.get('path', 'unknown')`\n   - If the comment body doesn't contain a parseable file reference, it falls back to `comment.get('path', 'unknown')`\n   - For GraphQL-sourced comments, this returns `'unknown'`\n   - Line 619: Comments with `path='unknown'` get grouped under the \"unknown\" file key\n\n### Are Comments Being Duplicated?\n\n**Potential issue**: The same comment might exist in BOTH:\n- REST API inline comments (with proper path/line data)\n- GraphQL review threads (without path/line data)\n\nIf duplicated, the GraphQL version (with `path='unknown'`) would **appear as a separate comment** in the output.\n\n### Why GraphQL Doesn't Include File/Line Data\n\n**The comment on line 902-903 is WRONG**. The GraphQL API **DOES** provide this data! According to GitHub's GraphQL schema:\n\n- `PullRequestReviewThread.path` (String!) - \"The path to which the comment applies\"\n- `PullRequestReviewComment.originalPosition` (Int!) - \"The original line index in the diff\"\n- `PullRequestReviewComment.diffHunk` (String!) - \"The diff hunk to which the comment applies\"\n- `PullRequestReviewComment.path` (String!) - \"The path to which the comment applies\"\n\n**The query just doesn't ask for them!**\n\n### Recommended Solution\n\nI recommend **Option C: Enhance GraphQL query** with the following approach:\n\n1. **Update the GraphQL query** (lines 147-172) to include:\n   - `path` on the reviewThread or first comment\n   - `originalPosition` on comments\n   - `diffHunk` on comments (optional, for additional context)\n\n2. **Update the conversion** (lines 897-905) to use the fetched data instead of hardcoding 'unknown'\n\n3. **Add deduplication logic** to prevent the same comment appearing twice (once from REST API, once from GraphQL)\n\n4. **Keep both data sources** because:\n   - REST API: Provides inline review comments\n   - GraphQL API: Provides resolution status (isResolved field) which REST API doesn't have\n\n### Alternative Solutions Considered\n\n**Option A: Use REST API data preferentially**\n- âŒ Would lose resolution status information (only available in GraphQL)\n- âŒ Doesn't solve the root cause\n\n**Option B: Only use REST API, skip GraphQL**\n- âŒ Would lose ability to filter resolved threads via GitHub's native resolution\n- âŒ This was recently added (based on commit history) so removing it would be a regression\n\n**Option C: Enhance GraphQL query (RECOMMENDED)**\n- âœ… Preserves resolution filtering capability\n- âœ… Fixes the 'unknown' file attribution issue\n- âœ… Enables proper deduplication\n- âœ… Uses all available API capabilities\n\n### Implementation Plan\n\nThe fix requires modifying two sections:\n\n1. **GraphQL Query (lines 147-172)**: Add `path`, `originalPosition`, `line` fields to the query\n2. **Conversion Logic (lines 897-905)**: Use the fetched path/line data instead of hardcoding 'unknown'/0\n3. **Deduplication**: Add logic to check if a comment from GraphQL already exists in the REST API results (compare by comment ID or body)\n\nWould you like me to implement this fix?"}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":5846,"cache_read_input_tokens":26757,"cache_creation":{"ephemeral_5m_input_tokens":5846,"ephemeral_1h_input_tokens":0},"output_tokens":1406,"service_tier":"standard"}},"requestId":"req_011CV626s7TaPCky3aDJ4K3s","type":"assistant","uuid":"0256ff07-5897-438c-a48f-ca01c228d3e3","timestamp":"2025-11-13T14:08:47.309Z"}
