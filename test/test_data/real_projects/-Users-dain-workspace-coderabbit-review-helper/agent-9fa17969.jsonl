{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","type":"user","message":{"role":"user","content":"Investigate why \"Outside diff range comments\" from CodeRabbit's main review body might not be appearing in the output of extract-coderabbit-feedback.py:\n\n**Context:**\nUser reports that after a new commit, CodeRabbit posts a main review comment with an \"Outside diff range comments\" section containing important findings like:\n\n```\napps/platform-frontend/src/app/api/organisation/[organisationId]/connections/plaid/save-connections/route.ts (1)\n\n    11-25: Type mismatch between request body types and validation schema.\n    \n    [description of the issue]\n```\n\nThese comments have file paths and line ranges but aren't appearing in the extractor's output.\n\n**Investigation Tasks:**\n\n1. **Review section parsing (lines ~372-442 - parse_review_sections())**:\n   - Does it find and extract \"outside_diff_comments\" section?\n   - What patterns does it look for? (e.g., \"Outside diff range comments\", \"ğŸ“ Remaining comments\")\n   - Is it looking in the right review body?\n\n2. **File-level comment parsing (lines ~445-511 - parse_file_level_comments())**:\n   - How does it parse comments with format: `filename (count)\\n\\n    line-range: title\\n\\n    description`?\n   - Does the regex pattern match the indentation and format shown above?\n   - Are there any edge cases it might miss?\n\n3. **Review filtering logic (lines ~929-970)**:\n   - Default behavior: only process \"latest review with actual content\"\n   - What defines \"latest review\"? Is it by timestamp?\n   - Could \"Outside diff range comments\" be in a different review than inline comments?\n   - Check the condition: `if (len(body) > 100 and 'Actionable comments posted:' in body...)`\n\n4. **Check group_comments_by_file() integration (lines ~583-599)**:\n   - Are outside_diff_comments being added to file_groups?\n   - What's the source tag? ('outside_diff')\n   - Are they being included in the final output?\n\n5. **Potential issues:**\n   - Are outside diff comments only in newer reviews (after latest commit) that aren't selected by default filtering?\n   - Is there a timestamp filtering issue with `--since-commit` or default latest review logic?\n   - Could the review body section parsing be missing these comments due to format differences?\n   - Are these comments being filtered out by `filter_resolved_review_content()` (line 264)?\n\n**Expected findings:**\n- Identify whether the issue is: (a) parsing failure, (b) review selection logic, (c) filtering logic, or (d) output formatting\n- Determine if \"Outside diff range comments\" exists in the review body but isn't extracted\n- Find what conditions need to be met for these comments to appear\n\nReturn:\n- Current behavior of outside_diff_comments processing\n- Why they might not appear in output\n- Where the data is lost or filtered\n- Recommended fix approach"},"uuid":"1a815cc7-16c3-416b-bdb5-604ccb6d3462","timestamp":"2025-11-14T12:29:57.118Z"}
{"parentUuid":"1a815cc7-16c3-416b-bdb5-604ccb6d3462","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01BPXGy9Zce3C5FyPNjBDvkE","type":"message","role":"assistant","content":[{"type":"text","text":"I'll investigate why \"Outside diff range comments\" from CodeRabbit's main review body might not be appearing in the output. Let me start by examining the script structure and the relevant parsing functions."}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8678,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":8678,"ephemeral_1h_input_tokens":0},"output_tokens":121,"service_tier":"standard"}},"requestId":"req_011CV7nR8YXafAKpXWshfrSZ","type":"assistant","uuid":"b8b87785-5feb-4b06-823b-2430b977313d","timestamp":"2025-11-14T12:30:00.237Z"}
{"parentUuid":"b8b87785-5feb-4b06-823b-2430b977313d","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01BPXGy9Zce3C5FyPNjBDvkE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018C63rZaHZpNTqeaV9CVymJ","name":"Read","input":{"file_path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8678,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":8678,"ephemeral_1h_input_tokens":0},"output_tokens":121,"service_tier":"standard"}},"requestId":"req_011CV7nR8YXafAKpXWshfrSZ","type":"assistant","uuid":"ca4148d1-8b2b-4cc4-86ad-a96f50ee807c","timestamp":"2025-11-14T12:30:00.685Z"}
{"parentUuid":"ca4148d1-8b2b-4cc4-86ad-a96f50ee807c","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018C63rZaHZpNTqeaV9CVymJ","type":"tool_result","content":"     1â†’#!/usr/bin/env python3\n     2â†’\"\"\"\n     3â†’ABOUTME: Extract CodeRabbit review feedback from GitHub PRs for LLM consumption\n     4â†’ABOUTME: Uses gh CLI to fetch PR comments and formats them as plain text\n     5â†’\"\"\"\n     6â†’\n     7â†’__version__ = \"1.0.0\"\n     8â†’__author__ = \"Jesse Vincent\"\n     9â†’__license__ = \"MIT\"\n    10â†’import argparse\n    11â†’import json\n    12â†’import re\n    13â†’import subprocess\n    14â†’import sys\n    15â†’from pathlib import Path\n    16â†’from typing import List, Dict, Any, Tuple, Optional\n    17â†’from bs4 import BeautifulSoup\n    18â†’import html\n    19â†’\n    20â†’\n    21â†’def load_preamble_config() -> Optional[str]:\n    22â†’    \"\"\"Load custom preamble from dotfile.\"\"\"\n    23â†’    config_path = Path.home() / \".coderabbit-extractor\"\n    24â†’\n    25â†’    if config_path.exists():\n    26â†’        try:\n    27â†’            return config_path.read_text(encoding=\"utf-8\").strip()\n    28â†’        except Exception as e:\n    29â†’            print(\n    30â†’                f\"Warning: Could not read config file {config_path}: {e}\",\n    31â†’                file=sys.stderr,\n    32â†’            )\n    33â†’            return None\n    34â†’\n    35â†’    return None\n    36â†’\n    37â†’\n    38â†’def parse_pr_input(input_str: str) -> str:\n    39â†’    \"\"\"Parse PR URL or owner/repo/number format into a full GitHub PR URL.\"\"\"\n    40â†’    input_str = input_str.strip()\n    41â†’\n    42â†’    # If it's already a full URL, return as-is\n    43â†’    if input_str.startswith(\"https://github.com/\"):\n    44â†’        return input_str\n    45â†’\n    46â†’    # Parse owner/repo/number format (e.g., \"obra/lace/278\")\n    47â†’    parts = input_str.split(\"/\")\n    48â†’    if len(parts) == 3:\n    49â†’        owner, repo, pr_num = parts\n    50â†’        return f\"https://github.com/{owner}/{repo}/pull/{pr_num}\"\n    51â†’\n    52â†’    raise ValueError(\n    53â†’        f\"Invalid PR format: {input_str}. Use 'owner/repo/number' or full URL\"\n    54â†’    )\n    55â†’\n    56â†’\n    57â†’def extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\n    58â†’    \"\"\"Extract owner, repo, and PR number from GitHub PR URL.\"\"\"\n    59â†’    # Parse URL like https://github.com/owner/repo/pull/123\n    60â†’    if \"github.com\" in pr_url:\n    61â†’        parts = pr_url.replace(\"https://github.com/\", \"\").split(\"/\")\n    62â†’        if len(parts) >= 4 and parts[2] == \"pull\":\n    63â†’            owner, repo, _, pr_number = parts[:4]\n    64â†’            try:\n    65â†’                return owner, repo, int(pr_number)\n    66â†’            except ValueError:\n    67â†’                raise ValueError(f\"Invalid PR number: {pr_number}\")\n    68â†’\n    69â†’    raise ValueError(f\"Could not parse PR URL: {pr_url}\")\n    70â†’\n    71â†’\n    72â†’def fetch_pr_reviews(pr_url: str) -> List[Dict[str, Any]]:\n    73â†’    \"\"\"Fetch PR reviews using gh CLI.\"\"\"\n    74â†’    try:\n    75â†’        result = subprocess.run(\n    76â†’            [\"gh\", \"pr\", \"view\", pr_url, \"--json\", \"reviews\"],\n    77â†’            capture_output=True,\n    78â†’            text=True,\n    79â†’            check=True,\n    80â†’        )\n    81â†’        data = json.loads(result.stdout)\n    82â†’        return data.get(\"reviews\", [])\n    83â†’    except subprocess.CalledProcessError as e:\n    84â†’        print(f\"Error fetching PR: {e.stderr}\", file=sys.stderr)\n    85â†’        sys.exit(1)\n    86â†’    except json.JSONDecodeError as e:\n    87â†’        print(f\"Error parsing JSON response: {e}\", file=sys.stderr)\n    88â†’        sys.exit(1)\n    89â†’\n    90â†’\n    91â†’def fetch_latest_commit_time(pr_url: str) -> Optional[str]:\n    92â†’    \"\"\"Fetch the timestamp of the most recent commit in the PR.\"\"\"\n    93â†’    try:\n    94â†’        # Extract owner/repo/number from URL\n    95â†’        owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n    96â†’\n    97â†’        # Get commits from the PR (with pagination for PRs with >30 commits)\n    98â†’        result = subprocess.run(\n    99â†’            [\n   100â†’                \"gh\",\n   101â†’                \"api\",\n   102â†’                \"--paginate\",\n   103â†’                f\"repos/{owner}/{repo}/pulls/{pr_number}/commits\",\n   104â†’            ],\n   105â†’            capture_output=True,\n   106â†’            text=True,\n   107â†’            check=True,\n   108â†’        )\n   109â†’        commits = json.loads(result.stdout)\n   110â†’\n   111â†’        if commits:\n   112â†’            # Get the most recent commit (last in the list)\n   113â†’            latest_commit = commits[-1]\n   114â†’            return latest_commit[\"commit\"][\"committer\"][\"date\"]\n   115â†’\n   116â†’        return None\n   117â†’    except Exception as e:\n   118â†’        print(f\"Warning: Could not fetch latest commit time: {e}\", file=sys.stderr)\n   119â†’        return None\n   120â†’\n   121â†’\n   122â†’def fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n   123â†’    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n   124â†’    try:\n   125â†’        # Extract owner/repo/number from URL\n   126â†’        if \"github.com\" in pr_url:\n   127â†’            parts = pr_url.replace(\"https://github.com/\", \"\").split(\"/\")\n   128â†’            if len(parts) >= 4 and parts[2] == \"pull\":\n   129â†’                owner, repo, _, pr_number = parts[:4]\n   130â†’                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n   131â†’            else:\n   132â†’                raise ValueError(\"Invalid GitHub PR URL format\")\n   133â†’        else:\n   134â†’            raise ValueError(\"URL must be a GitHub PR URL\")\n   135â†’\n   136â†’        result = subprocess.run(\n   137â†’            [\"gh\", \"api\", \"--paginate\", api_path],\n   138â†’            capture_output=True,\n   139â†’            text=True,\n   140â†’            check=True,\n   141â†’        )\n   142â†’        data = json.loads(result.stdout)\n   143â†’        return data if isinstance(data, list) else []\n   144â†’    except subprocess.CalledProcessError as e:\n   145â†’        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n   146â†’        return []\n   147â†’    except json.JSONDecodeError as e:\n   148â†’        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n   149â†’        return []\n   150â†’        return []\n   151â†’\n   152â†’\n   153â†’def fetch_review_threads_graphql(\n   154â†’    owner: str, repo: str, pr_number: int\n   155â†’) -> List[Dict[str, Any]]:\n   156â†’    \"\"\"Fetch review threads with resolution status using GitHub GraphQL API with pagination.\"\"\"\n   157â†’    query = \"\"\"\n   158â†’    query($owner: String!, $repo: String!, $prNumber: Int!, $cursor: String) {\n   159â†’      repository(owner: $owner, name: $repo) {\n   160â†’        pullRequest(number: $prNumber) {\n   161â†’          reviewThreads(first: 100, after: $cursor) {\n   162â†’            nodes {\n   163â†’              id\n   164â†’              isResolved\n   165â†’              resolvedBy {\n   166â†’                login\n   167â†’              }\n   168â†’              path\n   169â†’              line\n   170â†’              originalLine\n   171â†’              startLine\n   172â†’              comments(first: 1) {\n   173â†’                nodes {\n   174â†’                  body\n   175â†’                  createdAt\n   176â†’                  author {\n   177â†’                    login\n   178â†’                  }\n   179â†’                }\n   180â†’              }\n   181â†’            }\n   182â†’            pageInfo {\n   183â†’              hasNextPage\n   184â†’              endCursor\n   185â†’            }\n   186â†’          }\n   187â†’        }\n   188â†’      }\n   189â†’    }\n   190â†’    \"\"\"\n   191â†’\n   192â†’    try:\n   193â†’        # Use gh api to make GraphQL request with pagination\n   194â†’        import json\n   195â†’\n   196â†’        all_threads = []\n   197â†’        cursor = None\n   198â†’        has_next_page = True\n   199â†’        page_count = 0\n   200â†’\n   201â†’        while has_next_page:\n   202â†’            page_count += 1\n   203â†’\n   204â†’            # Build command with cursor if available\n   205â†’            cmd = [\n   206â†’                \"gh\",\n   207â†’                \"api\",\n   208â†’                \"graphql\",\n   209â†’                \"-F\",\n   210â†’                f\"owner={owner}\",\n   211â†’                \"-F\",\n   212â†’                f\"repo={repo}\",\n   213â†’                \"-F\",\n   214â†’                f\"prNumber={pr_number}\",\n   215â†’                \"-f\",\n   216â†’                f\"query={query}\",\n   217â†’            ]\n   218â†’            if cursor:\n   219â†’                cmd.extend([\"-F\", f\"cursor={cursor}\"])\n   220â†’\n   221â†’            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n   222â†’            data = json.loads(result.stdout)\n   223â†’\n   224â†’            # Extract review threads and pagination info\n   225â†’            threads_data = (\n   226â†’                data.get(\"data\", {})\n   227â†’                .get(\"repository\", {})\n   228â†’                .get(\"pullRequest\", {})\n   229â†’                .get(\"reviewThreads\", {})\n   230â†’            )\n   231â†’            threads = threads_data.get(\"nodes\", [])\n   232â†’            page_info = threads_data.get(\"pageInfo\", {})\n   233â†’\n   234â†’            all_threads.extend(threads)\n   235â†’\n   236â†’            has_next_page = page_info.get(\"hasNextPage\", False)\n   237â†’            cursor = page_info.get(\"endCursor\")\n   238â†’\n   239â†’            if page_count > 1:\n   240â†’                print(\n   241â†’                    f\"Fetched page {page_count} of review threads ({len(threads)} threads)\",\n   242â†’                    file=sys.stderr,\n   243â†’                )\n   244â†’\n   245â†’        # Now filter all collected threads\n   246â†’\n   247â†’        # Filter to only CodeRabbit threads and return as list of dicts\n   248â†’        coderabbit_threads = []\n   249â†’        for thread in all_threads:\n   250â†’            if thread and thread.get(\"comments\", {}).get(\"nodes\", []):\n   251â†’                comment = thread[\"comments\"][\"nodes\"][0]\n   252â†’                author_login = comment.get(\"author\", {}).get(\"login\", \"\")\n   253â†’                # Check for both coderabbitai and coderabbitai[bot] formats\n   254â†’                if author_login in [\"coderabbitai\", \"coderabbitai[bot]\"]:\n   255â†’                    # Extract line range (handle both single line and ranges)\n   256â†’                    start_line = thread.get(\"startLine\")\n   257â†’                    end_line = thread.get(\"line\") or thread.get(\"originalLine\")\n   258â†’\n   259â†’                    coderabbit_threads.append(\n   260â†’                        {\n   261â†’                            \"id\": thread[\"id\"],\n   262â†’                            \"isResolved\": thread.get(\"isResolved\", False),\n   263â†’                            \"resolvedBy\": thread.get(\"resolvedBy\", {}).get(\"login\")\n   264â†’                            if thread.get(\"resolvedBy\")\n   265â†’                            else None,\n   266â†’                            \"body\": comment.get(\"body\", \"\"),\n   267â†’                            \"createdAt\": comment.get(\"createdAt\", \"\"),\n   268â†’                            \"author\": author_login,\n   269â†’                            \"path\": thread.get(\"path\"),\n   270â†’                            \"line\": end_line,\n   271â†’                            \"startLine\": start_line,\n   272â†’                        }\n   273â†’                    )\n   274â†’\n   275â†’        return coderabbit_threads\n   276â†’\n   277â†’    except subprocess.CalledProcessError as e:\n   278â†’        print(\n   279â†’            f\"Warning: Could not fetch review threads via GraphQL: {e.stderr}\",\n   280â†’            file=sys.stderr,\n   281â†’        )\n   282â†’        return []\n   283â†’    except json.JSONDecodeError as e:\n   284â†’        print(f\"Warning: Could not parse GraphQL response: {e}\", file=sys.stderr)\n   285â†’        return []\n   286â†’    except Exception as e:\n   287â†’        print(f\"Warning: Error processing GraphQL data: {e}\", file=sys.stderr)\n   288â†’        return []\n   289â†’\n   290â†’\n   291â†’def extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   292â†’    \"\"\"Filter reviews to only CodeRabbit ones.\"\"\"\n   293â†’    return [\n   294â†’        review\n   295â†’        for review in reviews\n   296â†’        if review.get(\"author\", {}).get(\"login\") == \"coderabbitai\"\n   297â†’    ]\n   298â†’\n   299â†’\n   300â†’def filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   301â†’    \"\"\"Filter out comments that have been marked as resolved.\"\"\"\n   302â†’    return [\n   303â†’        comment\n   304â†’        for comment in comments\n   305â†’        if not is_comment_resolved(comment.get(\"body\", \"\"))\n   306â†’    ]\n   307â†’\n   308â†’\n   309â†’def filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   310â†’    \"\"\"Filter out review threads that have been marked as resolved via GitHub's native resolution.\"\"\"\n   311â†’    return [thread for thread in threads if not thread.get(\"isResolved\", False)]\n   312â†’\n   313â†’\n   314â†’def filter_resolved_review_content(review_body: str) -> str:\n   315â†’    \"\"\"Filter out resolved suggestions from review body content.\"\"\"\n   316â†’    # Look for resolved comment patterns and remove them\n   317â†’    # This handles CodeRabbit's text-based resolution markers\n   318â†’\n   319â†’    # Pattern to match resolved comments within the review body\n   320â†’    # Look for sections that contain resolution markers\n   321â†’    resolved_patterns = [\n   322â†’        # Match individual comment sections that are resolved\n   323â†’        r\"<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>\",\n   324â†’        # Match line-specific resolved comments\n   325â†’        r\"`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)\",\n   326â†’    ]\n   327â†’\n   328â†’    filtered_body = review_body\n   329â†’    for pattern in resolved_patterns:\n   330â†’        filtered_body = re.sub(\n   331â†’            pattern, \"\", filtered_body, flags=re.DOTALL | re.IGNORECASE\n   332â†’        )\n   333â†’\n   334â†’    # Also remove any standalone resolution lines\n   335â†’    filtered_body = re.sub(\n   336â†’        r\"^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?\",\n   337â†’        \"\",\n   338â†’        filtered_body,\n   339â†’        flags=re.MULTILINE | re.IGNORECASE,\n   340â†’    )\n   341â†’\n   342â†’    return filtered_body.strip()\n   343â†’\n   344â†’\n   345â†’def is_comment_resolved(comment_body: str) -> bool:\n   346â†’    \"\"\"Check if a comment has been marked as resolved/addressed.\"\"\"\n   347â†’    # Look for resolution indicators in the comment body\n   348â†’    resolution_patterns = [\n   349â†’        r\"âœ…\\s*Addressed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n   350â†’        r\"âœ…\\s*Resolved\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n   351â†’        r\"âœ…\\s*Fixed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n   352â†’        r\"âœ…\\s*Completed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n   353â†’        r\"\\[x\\]\",  # Checkbox checked\n   354â†’        r\"\\[X\\]\",  # Checkbox checked\n   355â†’    ]\n   356â†’\n   357â†’    for pattern in resolution_patterns:\n   358â†’        if re.search(pattern, comment_body, re.IGNORECASE):\n   359â†’            return True\n   360â†’\n   361â†’    return False\n   362â†’\n   363â†’\n   364â†’def extract_coderabbit_inline_comments(\n   365â†’    comments: List[Dict[str, Any]],\n   366â†’) -> List[Dict[str, Any]]:\n   367â†’    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n   368â†’    return [\n   369â†’        comment\n   370â†’        for comment in comments\n   371â†’        if comment.get(\"user\", {}).get(\"login\") == \"coderabbitai[bot]\"\n   372â†’    ]\n   373â†’\n   374â†’\n   375â†’def parse_review_type(body: str) -> str:\n   376â†’    \"\"\"Determine the type of CodeRabbit review.\"\"\"\n   377â†’    if \"Actionable comments posted:\" in body:\n   378â†’        return \"ACTIONABLE_REVIEW\"\n   379â†’    elif \"walkthrough_start\" in body and \"walkthrough_end\" in body:\n   380â†’        return \"WALKTHROUGH\"\n   381â†’    elif (\n   382â†’        \"<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\" in body\n   383â†’    ):\n   384â†’        return \"SUMMARY\"\n   385â†’    else:\n   386â†’        return \"OTHER\"\n   387â†’\n   388â†’\n   389â†’def clean_html_artifacts(text: str) -> str:\n   390â†’    \"\"\"Remove HTML comments and artifacts from text.\"\"\"\n   391â†’    # Remove HTML comments\n   392â†’    text = re.sub(r\"<!--.*?-->\", \"\", text, flags=re.DOTALL)\n   393â†’\n   394â†’    # Remove CodeRabbit warning blocks\n   395â†’    text = re.sub(\n   396â†’        r\">\\s*â€¼ï¸\\s*\\*\\*IMPORTANT\\*\\*.*?(?=\\n\\n|\\n$)\", \"\", text, flags=re.DOTALL\n   397â†’    )\n   398â†’\n   399â†’    # Remove other quote block artifacts\n   400â†’    text = re.sub(r\">\\s*Carefully review.*?(?=\\n\\n|\\n$)\", \"\", text, flags=re.DOTALL)\n   401â†’\n   402â†’    # Remove empty lines that result from artifact removal\n   403â†’    text = re.sub(r\"\\n\\s*\\n\\s*\\n\", \"\\n\\n\", text)\n   404â†’\n   405â†’    return text.strip()\n   406â†’\n   407â†’\n   408â†’def clean_diff_artifacts(diff: str) -> str:\n   409â†’    \"\"\"Clean up diff artifacts and markers.\"\"\"\n   410â†’    if not diff:\n   411â†’        return diff\n   412â†’\n   413â†’    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n   414â†’    diff = re.sub(r\"^@@.*?@@.*?\\n\", \"\", diff, flags=re.MULTILINE)  # Full hunk headers\n   415â†’    diff = re.sub(r\"^\\s*@@\\s*\\n\", \"\", diff, flags=re.MULTILINE)  # Lone @@ lines\n   416â†’\n   417â†’    # Remove empty lines at start/end\n   418â†’    diff = diff.strip()\n   419â†’\n   420â†’    return diff\n   421â†’\n   422â†’\n   423â†’def remove_details_sections(text: str) -> str:\n   424â†’    \"\"\"Remove verbose <details> sections containing analysis chains and script outputs.\n   425â†’\n   426â†’    Removes complete <details>...</details> blocks to filter out:\n   427â†’    - Analysis/reasoning chains\n   428â†’    - Script execution outputs\n   429â†’    - Internal processing logs\n   430â†’\n   431â†’    This helps keep the output focused on actionable feedback.\n   432â†’    \"\"\"\n   433â†’    # Remove all <details>...</details> blocks including nested content\n   434â†’    text = re.sub(r\"<details>.*?</details>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n   435â†’\n   436â†’    # Clean up any resulting excessive whitespace\n   437â†’    text = re.sub(r\"\\n\\s*\\n\\s*\\n\", \"\\n\\n\", text)\n   438â†’\n   439â†’    return text.strip()\n   440â†’\n   441â†’\n   442â†’def extract_file_and_lines_from_body(body: str) -> Tuple[Optional[str], Optional[str]]:\n   443â†’    \"\"\"Extract file path and line range from CodeRabbit comment body.\n   444â†’\n   445â†’    CodeRabbit comments typically include file references like:\n   446â†’    - \"In <filepath> around line X\"\n   447â†’    - \"In <filepath> around lines X to Y\"\n   448â†’    - \"In <filepath> around lines X-Y\"\n   449â†’\n   450â†’    Returns:\n   451â†’        Tuple of (file_path, line_range) or (None, None) if not found\n   452â†’    \"\"\"\n   453â†’    # Pattern to match file path references in CodeRabbit comments\n   454â†’    # Matches: \"In <filepath> around line(s) X\" or \"around lines X to Y\" or \"around lines X-Y\"\n   455â†’    file_pattern = r\"In\\s+([^\\s]+(?:\\.ts|\\.tsx|\\.js|\\.jsx|\\.py|\\.java|\\.go|\\.rs|\\.md|\\.json|\\.yaml|\\.yml))\\s+around\\s+lines?\\s+([\\d\\-to\\s]+)\"\n   456â†’\n   457â†’    match = re.search(file_pattern, body, re.IGNORECASE)\n   458â†’    if match:\n   459â†’        file_path = match.group(1).strip()\n   460â†’        line_range = match.group(2).strip()\n   461â†’\n   462â†’        # Normalize line range format (convert \"X to Y\" to \"X-Y\")\n   463â†’        line_range = re.sub(r\"(\\d+)\\s+to\\s+(\\d+)\", r\"\\1-\\2\", line_range)\n   464â†’\n   465â†’        return (file_path, line_range)\n   466â†’\n   467â†’    return (None, None)\n   468â†’\n   469â†’\n   470â†’def extract_prompt_for_ai_agents(body: str) -> List[str]:\n   471â†’    \"\"\"Extract 'Prompt for AI Agents' sections from review body.\"\"\"\n   472â†’    prompts = []\n   473â†’\n   474â†’    # Pattern to find ğŸ¤– Prompt for AI Agents sections\n   475â†’    # These appear to be in <summary> tags followed by clipboard content\n   476â†’    pattern = r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*<div[^>]*data-snippet-clipboard-copy-content=\"([^\"]*)\"'\n   477â†’    matches = re.findall(pattern, body, re.DOTALL)\n   478â†’\n   479â†’    for match in matches:\n   480â†’        # Decode HTML entities\n   481â†’        clean_text = (\n   482â†’            match.replace(\"&lt;\", \"<\")\n   483â†’            .replace(\"&gt;\", \">\")\n   484â†’            .replace(\"&amp;\", \"&\")\n   485â†’            .replace(\"&quot;\", '\"')\n   486â†’            .replace(\"\\\\n\", \"\\n\")\n   487â†’            .strip()\n   488â†’        )\n   489â†’        if clean_text:\n   490â†’            prompts.append(clean_text)\n   491â†’\n   492â†’    return prompts\n   493â†’\n   494â†’\n   495â†’# Removed unused functions - now using parse_review_sections instead\n   496â†’\n   497â†’\n   498â†’def parse_review_sections(body: str) -> Dict[str, Any]:\n   499â†’    \"\"\"Parse different sections from CodeRabbit review body using HTML parsing.\"\"\"\n   500â†’    sections = {}\n   501â†’\n   502â†’    # Extract actionable comments count from text\n   503â†’    actionable_match = re.search(r\"Actionable comments posted:\\s*(\\d+)\", body)\n   504â†’    if actionable_match:\n   505â†’        sections[\"actionable_count\"] = int(actionable_match.group(1))\n   506â†’\n   507â†’    # Parse HTML structure - but be careful not to mangle code content\n   508â†’    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n   509â†’    soup = BeautifulSoup(body, \"html.parser\")\n   510â†’\n   511â†’    # Find sections by looking for <details> elements with specific summary text\n   512â†’    details_elements = soup.find_all(\"details\")\n   513â†’\n   514â†’    for details in details_elements:\n   515â†’        summary = details.find(\"summary\")\n   516â†’        if not summary:\n   517â†’            continue\n   518â†’\n   519â†’        summary_text = summary.get_text(strip=True)\n   520â†’\n   521â†’        # Check for outside diff range comments (avoid emoji dependency)\n   522â†’        if \"Outside diff range comments\" in summary_text:\n   523â†’            # Extract count from summary text\n   524â†’            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n   525â†’            if count_match:\n   526â†’                count = int(count_match.group(1))\n   527â†’                blockquote = details.find(\"blockquote\")\n   528â†’                if blockquote:\n   529â†’                    # Get all content inside the blockquote\n   530â†’                    content = str(blockquote)\n   531â†’                    sections[\"outside_diff_comments\"] = {\n   532â†’                        \"count\": count,\n   533â†’                        \"content\": content,\n   534â†’                    }\n   535â†’\n   536â†’        # Check for nitpick comments or duplicate comments\n   537â†’        elif \"Nitpick comments\" in summary_text or \"Duplicate comments\" in summary_text:\n   538â†’            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n   539â†’            if count_match:\n   540â†’                count = int(count_match.group(1))\n   541â†’                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n   542â†’                # extract the raw content from the original body text\n   543â†’\n   544â†’                # Find the position of this details section in the original body\n   545â†’                details_start = body.find(\"Nitpick comments\")\n   546â†’                if details_start != -1:\n   547â†’                    # Find where this section ends (next major section or end)\n   548â†’                    section_markers = [\n   549â†’                        \"ğŸ“œ Review details\",\n   550â†’                        \"ğŸ”‡ Additional comments\",\n   551â†’                        \"</details>\\n\\n</details>\",\n   552â†’                    ]\n   553â†’                    section_end = len(body)\n   554â†’\n   555â†’                    for marker in section_markers:\n   556â†’                        marker_pos = body.find(marker, details_start)\n   557â†’                        if marker_pos != -1:\n   558â†’                            section_end = min(section_end, marker_pos)\n   559â†’\n   560â†’                    # Extract the raw text content\n   561â†’                    raw_content = body[details_start:section_end]\n   562â†’                    sections[\"nitpick_comments\"] = {\n   563â†’                        \"count\": count,\n   564â†’                        \"content\": raw_content,\n   565â†’                    }\n   566â†’\n   567â†’        # Check for review details (usually at the end)\n   568â†’        elif \"Review details\" in summary_text:\n   569â†’            # This section contains metadata, we might want it for context\n   570â†’            blockquote = details.find(\"blockquote\")\n   571â†’            if blockquote:\n   572â†’                content = str(blockquote)\n   573â†’                sections[\"review_details\"] = {\"content\": content}\n   574â†’\n   575â†’    # Extract AI prompts (look for specific patterns)\n   576â†’    sections[\"ai_prompts\"] = extract_prompt_for_ai_agents(body)\n   577â†’\n   578â†’    return sections\n   579â†’\n   580â†’\n   581â†’def parse_file_level_comments(\n   582â†’    content: str, debug: bool = False\n   583â†’) -> List[Dict[str, Any]]:\n   584â†’    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n   585â†’    comments = []\n   586â†’\n   587â†’    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n   588â†’    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n   589â†’    file_pattern = r\"<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>\"\n   590â†’    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n   591â†’\n   592â†’    if debug:\n   593â†’        print(\n   594â†’            f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\",\n   595â†’            file=sys.stderr,\n   596â†’        )\n   597â†’\n   598â†’    for file_match in file_matches:\n   599â†’        file_path = file_match.group(1).strip()\n   600â†’        comment_count = int(file_match.group(2))\n   601â†’        file_content = file_match.group(\n   602â†’            3\n   603â†’        )  # Raw blockquote content, not parsed by BeautifulSoup\n   604â†’\n   605â†’        # Now parse line comments within this file content\n   606â†’        # Find line comment patterns: `16-24`: **Title**\n   607â†’        line_pattern = r\"`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)\"\n   608â†’        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n   609â†’\n   610â†’        if debug:\n   611â†’            print(\n   612â†’                f\"DEBUG: File {file_path} has {len(line_matches)} line comments\",\n   613â†’                file=sys.stderr,\n   614â†’            )\n   615â†’\n   616â†’        for line_match in line_matches:\n   617â†’            line_range = line_match.group(1).strip()\n   618â†’            title = line_match.group(2).strip()\n   619â†’            content_block = line_match.group(3).strip()\n   620â†’\n   621â†’            # Remove verbose <details> sections before processing\n   622â†’            content_block = remove_details_sections(content_block)\n   623â†’\n   624â†’            # Check if this looks like a line reference\n   625â†’            if re.match(r\"^[\\d\\-,\\s]+$\", line_range):\n   626â†’                # Extract description (everything before first code block)\n   627â†’                description = content_block\n   628â†’                code_diff = \"\"\n   629â†’\n   630â†’                code_start = content_block.find(\"```\")\n   631â†’                if code_start != -1:\n   632â†’                    description = content_block[:code_start].strip()\n   633â†’\n   634â†’                    # Extract code diff from raw content (no BeautifulSoup mangling)\n   635â†’                    diff_match = re.search(\n   636â†’                        r\"```diff\\n(.*?)\\n```\", content_block, re.DOTALL\n   637â†’                    )\n   638â†’                    if diff_match:\n   639â†’                        code_diff = diff_match.group(1).strip()\n   640â†’                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n   641â†’                        code_diff = html.unescape(code_diff)\n   642â†’                        code_diff = clean_html_artifacts(\n   643â†’                            code_diff\n   644â†’                        )  # Remove any HTML comments in diffs\n   645â†’                        code_diff = clean_diff_artifacts(\n   646â†’                            code_diff\n   647â†’                        )  # Clean up @@ markers and artifacts\n   648â†’\n   649â†’                # Clean up description\n   650â†’                description = clean_html_artifacts(\n   651â†’                    description\n   652â†’                )  # Remove HTML comments first\n   653â†’                description_lines = []\n   654â†’                for line in description.split(\"\\n\"):\n   655â†’                    line = line.strip()\n   656â†’                    if (\n   657â†’                        line\n   658â†’                        and not line.startswith(\"Also applies to:\")\n   659â†’                        and line != \"---\"\n   660â†’                    ):\n   661â†’                        description_lines.append(line)\n   662â†’                description = \" \".join(description_lines)\n   663â†’\n   664â†’                comments.append(\n   665â†’                    {\n   666â†’                        \"file\": file_path,\n   667â†’                        \"lines\": line_range,\n   668â†’                        \"title\": title,\n   669â†’                        \"description\": description,\n   670â†’                        \"code_diff\": code_diff,\n   671â†’                    }\n   672â†’                )\n   673â†’\n   674â†’    return comments\n   675â†’\n   676â†’\n   677â†’def group_comments_by_file(\n   678â†’    coderabbit_reviews: List[Dict[str, Any]],\n   679â†’    inline_comments: List[Dict[str, Any]] = None,\n   680â†’    debug: bool = False,\n   681â†’    include_resolved: bool = True,\n   682â†’) -> Dict[str, List[Dict[str, Any]]]:\n   683â†’    \"\"\"Group all comments by file for better organization.\"\"\"\n   684â†’    file_groups = {}\n   685â†’\n   686â†’    # Process review body comments\n   687â†’    for review in coderabbit_reviews:\n   688â†’        review_body = review[\"body\"]\n   689â†’\n   690â†’        # Filter out resolved content from review body if requested\n   691â†’        if not include_resolved:\n   692â†’            original_body = review_body\n   693â†’            review_body = filter_resolved_review_content(review_body)\n   694â†’            if debug and len(original_body) != len(review_body):\n   695â†’                print(\n   696â†’                    f\"DEBUG: Filtered resolved content from review body (length: {len(original_body)} -> {len(review_body)})\",\n   697â†’                    file=sys.stderr,\n   698â†’                )\n   699â†’\n   700â†’        sections = parse_review_sections(review_body)\n   701â†’\n   702â†’        # Add outside diff comments\n   703â†’        if \"outside_diff_comments\" in sections:\n   704â†’            outside_comments = parse_file_level_comments(\n   705â†’                sections[\"outside_diff_comments\"][\"content\"], debug\n   706â†’            )\n   707â†’            for comment in outside_comments:\n   708â†’                file_path = comment[\"file\"]\n   709â†’                if file_path not in file_groups:\n   710â†’                    file_groups[file_path] = []\n   711â†’                comment[\"source\"] = \"outside_diff\"\n   712â†’                file_groups[file_path].append(comment)\n   713â†’\n   714â†’        # Add nitpick comments\n   715â†’        if \"nitpick_comments\" in sections:\n   716â†’            nitpick_comments = parse_file_level_comments(\n   717â†’                sections[\"nitpick_comments\"][\"content\"], debug\n   718â†’            )\n   719â†’            for comment in nitpick_comments:\n   720â†’                file_path = comment[\"file\"]\n   721â†’                if file_path not in file_groups:\n   722â†’                    file_groups[file_path] = []\n   723â†’                comment[\"source\"] = \"nitpick\"\n   724â†’                file_groups[file_path].append(comment)\n   725â†’\n   726â†’    # Process inline comments\n   727â†’    if inline_comments:\n   728â†’        for comment in inline_comments:\n   729â†’            body = comment.get(\"body\", \"\")\n   730â†’\n   731â†’            # Try to extract file path and line range from comment body\n   732â†’            extracted_file, extracted_lines = extract_file_and_lines_from_body(body)\n   733â†’\n   734â†’            # Use extracted values if available, otherwise fall back to comment metadata\n   735â†’            file_path = (\n   736â†’                extracted_file if extracted_file else comment.get(\"path\", \"unknown\")\n   737â†’            )\n   738â†’            line_num = comment.get(\"original_line\", \"unknown\")\n   739â†’            start_line = comment.get(\"start_line\")\n   740â†’\n   741â†’            # Determine line range to display\n   742â†’            if extracted_lines:\n   743â†’                # Use extracted line range from body (highest priority)\n   744â†’                line_range = extracted_lines\n   745â†’            elif (\n   746â†’                start_line\n   747â†’                and line_num\n   748â†’                and line_num != \"unknown\"\n   749â†’                and line_num != 0\n   750â†’                and start_line != line_num\n   751â†’            ):\n   752â†’                # Use GraphQL line range (start_line to line_num)\n   753â†’                line_range = f\"{start_line}-{line_num}\"\n   754â†’            elif line_num != \"unknown\" and line_num != 0:\n   755â†’                # Single line from metadata\n   756â†’                line_range = str(line_num)\n   757â†’            else:\n   758â†’                # No line info available\n   759â†’                line_range = \"\"\n   760â†’\n   761â†’            if file_path not in file_groups:\n   762â†’                file_groups[file_path] = []\n   763â†’\n   764â†’            # Convert inline comment to standard format\n   765â†’            comment_type = (\n   766â†’                \"ğŸ› ï¸ Refactor Suggestion\"\n   767â†’                if \"ğŸ› ï¸ Refactor suggestion\" in body\n   768â†’                else \"Comment\"\n   769â†’            )\n   770â†’\n   771â†’            # Extract title from body\n   772â†’            title_match = re.search(r\"\\*\\*(.*?)\\*\\*\", body)\n   773â†’            title = title_match.group(1).strip() if title_match else \"Suggestion\"\n   774â†’\n   775â†’            # Extract description and diff\n   776â†’            if \"<details>\" in body and \"Prompt for AI Agents\" in body:\n   777â†’                main_content = body.split(\"<details>\")[0].strip()\n   778â†’            else:\n   779â†’                main_content = body\n   780â†’\n   781â†’            # Remove verbose <details> sections\n   782â†’            main_content = remove_details_sections(main_content)\n   783â†’            main_content = main_content.replace(\"_ğŸ› ï¸ Refactor suggestion_\", \"\").strip()\n   784â†’            main_content = clean_html_artifacts(main_content)\n   785â†’\n   786â†’            # Extract AI prompt\n   787â†’            ai_prompt = \"\"\n   788â†’            ai_prompt_match = re.search(\n   789â†’                r\"<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*\\n\\n```\\n(.*?)\\n```\",\n   790â†’                body,\n   791â†’                re.DOTALL,\n   792â†’            )\n   793â†’            if ai_prompt_match:\n   794â†’                ai_prompt = ai_prompt_match.group(1).strip()\n   795â†’\n   796â†’            file_groups[file_path].append(\n   797â†’                {\n   798â†’                    \"lines\": line_range,\n   799â†’                    \"title\": title,\n   800â†’                    \"description\": main_content,\n   801â†’                    \"code_diff\": \"\",  # Inline comments don't have separate diffs\n   802â†’                    \"ai_prompt\": ai_prompt,\n   803â†’                    \"source\": \"inline\",\n   804â†’                    \"comment_type\": comment_type,\n   805â†’                }\n   806â†’            )\n   807â†’\n   808â†’    return file_groups\n   809â†’\n   810â†’\n   811â†’def format_for_llm(\n   812â†’    coderabbit_reviews: List[Dict[str, Any]],\n   813â†’    inline_comments: List[Dict[str, Any]] = None,\n   814â†’    debug: bool = False,\n   815â†’    preamble: Optional[str] = None,\n   816â†’    include_resolved: bool = True,\n   817â†’) -> str:\n   818â†’    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n   819â†’    output = []\n   820â†’\n   821â†’    # Add custom preamble if provided\n   822â†’    if preamble:\n   823â†’        output.append(preamble)\n   824â†’        output.append(\"\")\n   825â†’        output.append(\"=\" * 60)\n   826â†’        output.append(\"\")\n   827â†’\n   828â†’    output.append(\"# CodeRabbit Review Feedback\")\n   829â†’    output.append(\"=\" * 40)\n   830â†’    output.append(\"\")\n   831â†’\n   832â†’    # Group all comments by file\n   833â†’    file_groups = group_comments_by_file(\n   834â†’        coderabbit_reviews, inline_comments, debug, include_resolved\n   835â†’    )\n   836â†’\n   837â†’    if not file_groups:\n   838â†’        output.append(\"No actionable comments found.\")\n   839â†’        return \"\\n\".join(output)\n   840â†’\n   841â†’    # Add summary\n   842â†’    total_comments = sum(len(comments) for comments in file_groups.values())\n   843â†’    output.append(f\"**Total files with feedback:** {len(file_groups)}\")\n   844â†’    output.append(f\"**Total comments:** {total_comments}\")\n   845â†’    output.append(\"\")\n   846â†’\n   847â†’    # Process each file\n   848â†’    for file_path in sorted(file_groups.keys()):\n   849â†’        comments = file_groups[file_path]\n   850â†’\n   851â†’        output.append(f\"## {file_path}\")\n   852â†’        output.append(f\"**{len(comments)} suggestion(s)**\")\n   853â†’        output.append(\"\")\n   854â†’\n   855â†’        # Sort comments by priority: AI prompts first, then by line number\n   856â†’        def get_sort_key(comment):\n   857â†’            # Priority 1: Comments with AI prompts (highest priority)\n   858â†’            has_ai_prompt = bool(comment.get(\"ai_prompt\"))\n   859â†’\n   860â†’            # Priority 2: Line number (for logical code flow)\n   861â†’            line_num = 999\n   862â†’            lines = comment.get(\"lines\", \"\")\n   863â†’            if lines and lines.replace(\"-\", \"\").isdigit():\n   864â†’                line_num = int(lines.split(\"-\")[0])\n   865â†’\n   866â†’            # Return tuple: (ai_prompt_priority, line_number)\n   867â†’            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n   868â†’            return (0 if has_ai_prompt else 1, line_num)\n   869â†’\n   870â†’        comments_sorted = sorted(comments, key=get_sort_key)\n   871â†’\n   872â†’        for i, comment in enumerate(comments_sorted, 1):\n   873â†’            # Add AI prompt indicator to title for high-priority items\n   874â†’            title = comment[\"title\"]\n   875â†’            if comment.get(\"ai_prompt\"):\n   876â†’                title = f\"ğŸ¤– {title}\"\n   877â†’\n   878â†’            # Only include line numbers if they're meaningful (not empty or \"0\")\n   879â†’            lines = comment.get(\"lines\", \"\")\n   880â†’            if lines and lines != \"0\":\n   881â†’                output.append(f\"### {i}. Lines {lines}: {title}\")\n   882â†’            else:\n   883â†’                output.append(f\"### {i}. {title}\")\n   884â†’            output.append(\"\")\n   885â†’\n   886â†’            # Show AI prompt, or fallback to description + diff\n   887â†’            has_prompt = bool(comment.get(\"ai_prompt\"))\n   888â†’            has_diff = bool(comment.get(\"code_diff\"))\n   889â†’\n   890â†’            if debug:\n   891â†’                output.append(\n   892â†’                    f\"<!-- DEBUG: has_prompt={has_prompt}, has_diff={has_diff}, source={comment.get('source')} -->\"\n   893â†’                )\n   894â†’\n   895â†’            if has_prompt:\n   896â†’                if debug:\n   897â†’                    output.append(\n   898â†’                        \"<!-- DEBUG: Showing prompt only, skipping description and diff -->\"\n   899â†’                    )\n   900â†’                # AI prompt contains better explanation than description + diff\n   901â†’                output.append(\"**Proposed prompt:**\")\n   902â†’                output.append(\"```\")\n   903â†’                output.append(comment[\"ai_prompt\"])\n   904â†’                output.append(\"```\")\n   905â†’                output.append(\"\")\n   906â†’            else:\n   907â†’                if debug:\n   908â†’                    output.append(\n   909â†’                        \"<!-- DEBUG: No prompt, showing description and diff -->\"\n   910â†’                    )\n   911â†’\n   912â†’                # Show description only if no AI prompt available\n   913â†’                if comment[\"description\"] and len(comment[\"description\"]) > 10:\n   914â†’                    desc = comment[\"description\"]\n   915â†’                    desc = re.sub(\n   916â†’                        r\"^_.*?_\\s*\", \"\", desc\n   917â†’                    )  # Remove _âš ï¸ Potential issue_ etc\n   918â†’                    desc = re.sub(\n   919â†’                        r\"\\*\\*.*?\\*\\*\\s*\", \"\", desc, count=1\n   920â†’                    )  # Remove first **title** if duplicated\n   921â†’                    desc = re.sub(r\"Apply this diff:\\s*\", \"\", desc)\n   922â†’                    desc = desc.strip()\n   923â†’\n   924â†’                    if desc and desc != title:\n   925â†’                        output.append(desc)\n   926â†’                        output.append(\"\")\n   927â†’\n   928â†’                # Show diff only if no AI prompt available\n   929â†’                if has_diff:\n   930â†’                    output.append(\"```diff\")\n   931â†’                    output.append(comment[\"code_diff\"])\n   932â†’                    output.append(\"```\")\n   933â†’                    output.append(\"\")\n   934â†’\n   935â†’            if i < len(comments_sorted):\n   936â†’                output.append(\"-\" * 30)\n   937â†’                output.append(\"\")\n   938â†’\n   939â†’        output.append(\"=\" * 60)\n   940â†’        output.append(\"\")\n   941â†’\n   942â†’    return \"\\n\".join(output)\n   943â†’\n   944â†’\n   945â†’def main():\n   946â†’    \"\"\"Main entry point.\"\"\"\n   947â†’    parser = argparse.ArgumentParser(\n   948â†’        description=\"\"\"\n   949â†’CodeRabbit Review Extractor - Convert GitHub PR reviews to LLM-friendly text\n   950â†’\n   951â†’This tool extracts CodeRabbit automated code review feedback from GitHub pull \n   952â†’requests and formats it as clean, organized text suitable for AI coding agents.\n   953â†’\n   954â†’Perfect for feeding CodeRabbit suggestions to Claude, ChatGPT, or other LLMs\n   955â†’to automatically apply code improvements, refactoring suggestions, and fixes.\n   956â†’        \"\"\".strip(),\n   957â†’        epilog=\"\"\"\n   958â†’EXAMPLES:\n   959â†’  %(prog)s https://github.com/owner/repo/pull/123\n   960â†’  %(prog)s owner/repo/123\n   961â†’  %(prog)s obra/lace/278 --all-reviews\n   962â†’\n   963â†’WHAT IT EXTRACTS:\n   964â†’  â€¢ Refactor suggestions with specific code diffs\n   965â†’  â€¢ AI implementation instructions (ğŸ¤– prompts) with detailed steps\n   966â†’  â€¢ Nitpick comments and code quality improvements\n   967â†’  â€¢ File-organized feedback sorted by priority\n   968â†’  â€¢ Clean format without HTML artifacts or noise\n   969â†’\n   970â†’HOW IT WORKS:\n   971â†’  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n   972â†’  2. Extracts both main review summaries and inline code comments\n   973â†’  3. Organizes feedback by file with line-number targeting\n   974â†’  4. Prioritizes actionable items with AI implementation guides\n   975â†’  5. Outputs clean text ready for LLM consumption\n   976â†’\n   977â†’DEFAULT BEHAVIOR:\n   978â†’  Processes only the LATEST CodeRabbit review to avoid overwhelming output.\n   979â†’  Use --all-reviews for PRs with multiple review iterations if needed.\n   980â†’\n   981â†’REQUIREMENTS:\n   982â†’  â€¢ GitHub CLI (gh) installed and authenticated\n   983â†’  â€¢ Python 3.6+ with beautifulsoup4 package\n   984â†’\n   985â†’CONFIGURATION:\n   986â†’  Create ~/.coderabbit-extractor with custom preamble text to prepend to output.\n   987â†’  Useful for framing feedback (e.g., critical evaluation prompts for AI agents).\n   988â†’        \"\"\",\n   989â†’        formatter_class=argparse.RawDescriptionHelpFormatter,\n   990â†’    )\n   991â†’\n   992â†’    parser.add_argument(\n   993â†’        \"pr_input\",\n   994â†’        help='GitHub PR URL or owner/repo/number format (e.g., \"owner/repo/123\")',\n   995â†’    )\n   996â†’    parser.add_argument(\n   997â†’        \"--include-resolved\",\n   998â†’        action=\"store_true\",\n   999â†’        help=\"Include review suggestions that have been marked as resolved (default: filter them out)\",\n  1000â†’    )\n  1001â†’    parser.add_argument(\n  1002â†’        \"--all-reviews\",\n  1003â†’        action=\"store_true\",\n  1004â†’        help=\"Extract all CodeRabbit reviews (default: latest only)\",\n  1005â†’    )\n  1006â†’    parser.add_argument(\n  1007â†’        \"--since-commit\",\n  1008â†’        metavar=\"SHA\",\n  1009â†’        help=\"Extract reviews submitted after this commit SHA\",\n  1010â†’    )\n  1011â†’    parser.add_argument(\n  1012â†’        \"--debug\",\n  1013â†’        action=\"store_true\",\n  1014â†’        help=\"Show debug annotations in output to trace processing\",\n  1015â†’    )\n  1016â†’    parser.add_argument(\n  1017â†’        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n  1018â†’    )\n  1019â†’\n  1020â†’    # Custom handling for no arguments to show helpful info\n  1021â†’    if len(sys.argv) == 1:\n  1022â†’        print(\"ğŸ¤– CodeRabbit Review Extractor\")\n  1023â†’        print(\"=\" * 40)\n  1024â†’        print()\n  1025â†’        print(\"WHAT THIS DOES:\")\n  1026â†’        print(\n  1027â†’            \"  Converts CodeRabbit GitHub PR reviews into clean text for AI coding agents\"\n  1028â†’        )\n  1029â†’        print(\"  (Claude, ChatGPT, etc.) to automatically apply code suggestions.\")\n  1030â†’        print()\n  1031â†’        print(\"QUICK START:\")\n  1032â†’        print(\n  1033â†’            \"  python3 extract-coderabbit-feedback.py https://github.com/owner/repo/pull/123\"\n  1034â†’        )\n  1035â†’        print(\"  python3 extract-coderabbit-feedback.py owner/repo/123\")\n  1036â†’        print()\n  1037â†’        print(\"For full help and options, run:\")\n  1038â†’        print(\"  python3 extract-coderabbit-feedback.py --help\")\n  1039â†’        sys.exit(1)\n  1040â†’\n  1041â†’    args = parser.parse_args()\n  1042â†’\n  1043â†’    try:\n  1044â†’        pr_url = parse_pr_input(args.pr_input)\n  1045â†’\n  1046â†’        # Load custom preamble from dotfile\n  1047â†’        preamble = load_preamble_config()\n  1048â†’\n  1049â†’        # Fetch both reviews and inline comments\n  1050â†’        reviews = fetch_pr_reviews(pr_url)\n  1051â†’        inline_comments = fetch_pr_inline_comments(pr_url)\n  1052â†’\n  1053â†’        # Extract PR info for GraphQL query\n  1054â†’        try:\n  1055â†’            owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n  1056â†’\n  1057â†’            # Fetch review threads with resolution status via GraphQL\n  1058â†’            review_threads = fetch_review_threads_graphql(owner, repo, pr_number)\n  1059â†’\n  1060â†’            if review_threads and not args.include_resolved:\n  1061â†’                # Filter out resolved threads (GitHub native conversation resolution)\n  1062â†’                original_thread_count = len(review_threads)\n  1063â†’                review_threads = filter_resolved_threads(review_threads)\n  1064â†’                filtered_thread_count = original_thread_count - len(review_threads)\n  1065â†’                if filtered_thread_count > 0:\n  1066â†’                    print(\n  1067â†’                        f\"Filtered resolved review threads: {filtered_thread_count} resolved threads hidden\",\n  1068â†’                        file=sys.stderr,\n  1069â†’                    )\n  1070â†’            elif review_threads:\n  1071â†’                print(\n  1072â†’                    f\"Found {len(review_threads)} review threads (including resolved)\",\n  1073â†’                    file=sys.stderr,\n  1074â†’                )\n  1075â†’\n  1076â†’        except Exception as e:\n  1077â†’            print(\n  1078â†’                f\"Warning: Could not fetch review thread resolution data: {e}\",\n  1079â†’                file=sys.stderr,\n  1080â†’            )\n  1081â†’            review_threads = []\n  1082â†’\n  1083â†’        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n  1084â†’        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n  1085â†’\n  1086â†’        # Build a set of existing comment identifiers for deduplication\n  1087â†’        # Use (path, line, body_start) tuple to identify duplicates\n  1088â†’        existing_comments = set()\n  1089â†’        for comment in coderabbit_inline_comments:\n  1090â†’            path = comment.get(\"path\", \"\")\n  1091â†’            line = comment.get(\"original_line\", 0)\n  1092â†’            body = comment.get(\"body\", \"\")\n  1093â†’            # Use first 100 chars of body as identifier\n  1094â†’            body_start = body[:100] if body else \"\"\n  1095â†’            existing_comments.add((path, line, body_start))\n  1096â†’\n  1097â†’        # Convert review threads to inline comment format and merge\n  1098â†’        if review_threads:\n  1099â†’            for thread in review_threads:\n  1100â†’                # Extract line range if available\n  1101â†’                start_line = thread.get(\"startLine\")\n  1102â†’                end_line = thread.get(\"line\")\n  1103â†’\n  1104â†’                # Determine the line value to use\n  1105â†’                if start_line and end_line and start_line != end_line:\n  1106â†’                    # Multi-line range\n  1107â†’                    line_value = end_line  # Use end line for sorting\n  1108â†’                elif end_line:\n  1109â†’                    # Single line\n  1110â†’                    line_value = end_line\n  1111â†’                else:\n  1112â†’                    line_value = 0\n  1113â†’\n  1114â†’                # Check for duplicates before adding\n  1115â†’                path = thread.get(\"path\", \"unknown\")\n  1116â†’                body = thread.get(\"body\", \"\")\n  1117â†’                body_start = body[:100] if body else \"\"\n  1118â†’                comment_id = (path, line_value, body_start)\n  1119â†’\n  1120â†’                # Skip if this comment already exists (from REST API)\n  1121â†’                if comment_id in existing_comments:\n  1122â†’                    continue\n  1123â†’\n  1124â†’                # Convert thread to inline comment format\n  1125â†’                inline_comment = {\n  1126â†’                    \"id\": thread[\"id\"],\n  1127â†’                    \"body\": body,\n  1128â†’                    \"user\": {\"login\": \"coderabbitai[bot]\"},\n  1129â†’                    \"created_at\": thread.get(\"createdAt\", \"\"),\n  1130â†’                    \"path\": path,  # Use path from GraphQL\n  1131â†’                    \"original_line\": line_value,  # Use line from GraphQL\n  1132â†’                    \"start_line\": start_line,  # Store start_line for range information\n  1133â†’                    \"html_url\": f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\",\n  1134â†’                }\n  1135â†’                coderabbit_inline_comments.append(inline_comment)\n  1136â†’                existing_comments.add(comment_id)  # Add to set to track\n  1137â†’\n  1138â†’        # Filter out resolved comments if not including them\n  1139â†’        if not args.include_resolved:\n  1140â†’            original_count = len(coderabbit_inline_comments)\n  1141â†’            coderabbit_inline_comments = filter_resolved_comments(\n  1142â†’                coderabbit_inline_comments\n  1143â†’            )\n  1144â†’            filtered_count = original_count - len(coderabbit_inline_comments)\n  1145â†’            if filtered_count > 0:\n  1146â†’                print(\n  1147â†’                    f\"Filtered resolved comments: {filtered_count} resolved comments hidden\",\n  1148â†’                    file=sys.stderr,\n  1149â†’                )\n  1150â†’        # Note: Review-level filtering is handled in group_comments_by_file()\n  1151â†’\n  1152â†’        # Filter reviews based on options\n  1153â†’        if not args.all_reviews and not args.since_commit:\n  1154â†’            # Default: latest review with actual content\n  1155â†’            if coderabbit_reviews:\n  1156â†’                # Find the latest review with substantial content (not just continuation)\n  1157â†’                latest_review = None\n  1158â†’                for review in reversed(coderabbit_reviews):\n  1159â†’                    body = review.get(\"body\", \"\")\n  1160â†’                    # Skip continuation reviews that have minimal content\n  1161â†’                    if (\n  1162â†’                        len(body) > 100\n  1163â†’                        and \"Actionable comments posted:\" in body\n  1164â†’                        and not body.strip().startswith(\n  1165â†’                            \"**Review continued from previous batch\"\n  1166â†’                        )\n  1167â†’                    ):\n  1168â†’                        latest_review = review\n  1169â†’                        break\n  1170â†’\n  1171â†’                # Fallback to chronologically latest if no substantial review found\n  1172â†’                if not latest_review:\n  1173â†’                    latest_review = coderabbit_reviews[-1]\n  1174â†’\n  1175â†’                coderabbit_reviews = [latest_review]\n  1176â†’\n  1177â†’                # Filter inline comments to only those posted after the most recent commit\n  1178â†’                latest_commit_time = fetch_latest_commit_time(pr_url)\n  1179â†’                if latest_commit_time and coderabbit_inline_comments:\n  1180â†’                    from datetime import datetime\n  1181â†’\n  1182â†’                    try:\n  1183â†’                        # Parse ISO format timestamps (GitHub API standard)\n  1184â†’                        commit_dt = datetime.fromisoformat(\n  1185â†’                            latest_commit_time.replace(\"Z\", \"+00:00\")\n  1186â†’                        )\n  1187â†’\n  1188â†’                        filtered_inline = []\n  1189â†’                        for comment in coderabbit_inline_comments:\n  1190â†’                            comment_time = comment.get(\"created_at\", \"\")\n  1191â†’                            if comment_time:\n  1192â†’                                comment_dt = datetime.fromisoformat(\n  1193â†’                                    comment_time.replace(\"Z\", \"+00:00\")\n  1194â†’                                )\n  1195â†’                                # Include comments posted after the latest commit\n  1196â†’                                if comment_dt > commit_dt:\n  1197â†’                                    filtered_inline.append(comment)\n  1198â†’\n  1199â†’                        original_count = len(coderabbit_inline_comments)\n  1200â†’                        coderabbit_inline_comments = filtered_inline\n  1201â†’                        if original_count != len(filtered_inline):\n  1202â†’                            print(\n  1203â†’                                f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\",\n  1204â†’                                file=sys.stderr,\n  1205â†’                            )\n  1206â†’\n  1207â†’                    except Exception as e:\n  1208â†’                        print(\n  1209â†’                            f\"Warning: Could not filter inline comments by commit time: {e}\",\n  1210â†’                            file=sys.stderr,\n  1211â†’                        )\n  1212â†’        elif args.since_commit:\n  1213â†’            # Filter reviews submitted after the specified commit\n  1214â†’            # This would require additional logic to compare timestamps with commit dates\n  1215â†’            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n  1216â†’            sys.exit(1)\n  1217â†’\n  1218â†’        if not coderabbit_reviews and not coderabbit_inline_comments:\n  1219â†’            print(\n  1220â†’                \"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr\n  1221â†’            )\n  1222â†’            sys.exit(1)\n  1223â†’\n  1224â†’        # Show processing info\n  1225â†’        if len(coderabbit_reviews) > 1:\n  1226â†’            print(\n  1227â†’                f\"Processing {len(coderabbit_reviews)} CodeRabbit reviews...\",\n  1228â†’                file=sys.stderr,\n  1229â†’            )\n  1230â†’        elif len(coderabbit_reviews) == 1:\n  1231â†’            print(\"Processing latest CodeRabbit review...\", file=sys.stderr)\n  1232â†’\n  1233â†’        if coderabbit_inline_comments:\n  1234â†’            print(\n  1235â†’                f\"Found {len(coderabbit_inline_comments)} inline comments\",\n  1236â†’                file=sys.stderr,\n  1237â†’            )\n  1238â†’\n  1239â†’        formatted_output = format_for_llm(\n  1240â†’            coderabbit_reviews,\n  1241â†’            coderabbit_inline_comments,\n  1242â†’            args.debug,\n  1243â†’            preamble,\n  1244â†’            args.include_resolved,\n  1245â†’        )\n  1246â†’        print(formatted_output)\n  1247â†’\n  1248â†’    except Exception as e:\n  1249â†’        print(f\"Error: {e}\", file=sys.stderr)\n  1250â†’        sys.exit(1)\n  1251â†’\n  1252â†’\n  1253â†’if __name__ == \"__main__\":\n  1254â†’    main()\n  1255â†’\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"61f9c704-b756-4f92-b8e8-24568fb98e88","timestamp":"2025-11-14T12:30:00.932Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","content":"#!/usr/bin/env python3\n\"\"\"\nABOUTME: Extract CodeRabbit review feedback from GitHub PRs for LLM consumption\nABOUTME: Uses gh CLI to fetch PR comments and formats them as plain text\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Jesse Vincent\"\n__license__ = \"MIT\"\nimport argparse\nimport json\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom bs4 import BeautifulSoup\nimport html\n\n\ndef load_preamble_config() -> Optional[str]:\n    \"\"\"Load custom preamble from dotfile.\"\"\"\n    config_path = Path.home() / \".coderabbit-extractor\"\n\n    if config_path.exists():\n        try:\n            return config_path.read_text(encoding=\"utf-8\").strip()\n        except Exception as e:\n            print(\n                f\"Warning: Could not read config file {config_path}: {e}\",\n                file=sys.stderr,\n            )\n            return None\n\n    return None\n\n\ndef parse_pr_input(input_str: str) -> str:\n    \"\"\"Parse PR URL or owner/repo/number format into a full GitHub PR URL.\"\"\"\n    input_str = input_str.strip()\n\n    # If it's already a full URL, return as-is\n    if input_str.startswith(\"https://github.com/\"):\n        return input_str\n\n    # Parse owner/repo/number format (e.g., \"obra/lace/278\")\n    parts = input_str.split(\"/\")\n    if len(parts) == 3:\n        owner, repo, pr_num = parts\n        return f\"https://github.com/{owner}/{repo}/pull/{pr_num}\"\n\n    raise ValueError(\n        f\"Invalid PR format: {input_str}. Use 'owner/repo/number' or full URL\"\n    )\n\n\ndef extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\n    \"\"\"Extract owner, repo, and PR number from GitHub PR URL.\"\"\"\n    # Parse URL like https://github.com/owner/repo/pull/123\n    if \"github.com\" in pr_url:\n        parts = pr_url.replace(\"https://github.com/\", \"\").split(\"/\")\n        if len(parts) >= 4 and parts[2] == \"pull\":\n            owner, repo, _, pr_number = parts[:4]\n            try:\n                return owner, repo, int(pr_number)\n            except ValueError:\n                raise ValueError(f\"Invalid PR number: {pr_number}\")\n\n    raise ValueError(f\"Could not parse PR URL: {pr_url}\")\n\n\ndef fetch_pr_reviews(pr_url: str) -> List[Dict[str, Any]]:\n    \"\"\"Fetch PR reviews using gh CLI.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"gh\", \"pr\", \"view\", pr_url, \"--json\", \"reviews\"],\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n        data = json.loads(result.stdout)\n        return data.get(\"reviews\", [])\n    except subprocess.CalledProcessError as e:\n        print(f\"Error fetching PR: {e.stderr}\", file=sys.stderr)\n        sys.exit(1)\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON response: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\ndef fetch_latest_commit_time(pr_url: str) -> Optional[str]:\n    \"\"\"Fetch the timestamp of the most recent commit in the PR.\"\"\"\n    try:\n        # Extract owner/repo/number from URL\n        owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n\n        # Get commits from the PR (with pagination for PRs with >30 commits)\n        result = subprocess.run(\n            [\n                \"gh\",\n                \"api\",\n                \"--paginate\",\n                f\"repos/{owner}/{repo}/pulls/{pr_number}/commits\",\n            ],\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n        commits = json.loads(result.stdout)\n\n        if commits:\n            # Get the most recent commit (last in the list)\n            latest_commit = commits[-1]\n            return latest_commit[\"commit\"][\"committer\"][\"date\"]\n\n        return None\n    except Exception as e:\n        print(f\"Warning: Could not fetch latest commit time: {e}\", file=sys.stderr)\n        return None\n\n\ndef fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n    try:\n        # Extract owner/repo/number from URL\n        if \"github.com\" in pr_url:\n            parts = pr_url.replace(\"https://github.com/\", \"\").split(\"/\")\n            if len(parts) >= 4 and parts[2] == \"pull\":\n                owner, repo, _, pr_number = parts[:4]\n                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n            else:\n                raise ValueError(\"Invalid GitHub PR URL format\")\n        else:\n            raise ValueError(\"URL must be a GitHub PR URL\")\n\n        result = subprocess.run(\n            [\"gh\", \"api\", \"--paginate\", api_path],\n            capture_output=True,\n            text=True,\n            check=True,\n        )\n        data = json.loads(result.stdout)\n        return data if isinstance(data, list) else []\n    except subprocess.CalledProcessError as e:\n        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n        return []\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n        return []\n        return []\n\n\ndef fetch_review_threads_graphql(\n    owner: str, repo: str, pr_number: int\n) -> List[Dict[str, Any]]:\n    \"\"\"Fetch review threads with resolution status using GitHub GraphQL API with pagination.\"\"\"\n    query = \"\"\"\n    query($owner: String!, $repo: String!, $prNumber: Int!, $cursor: String) {\n      repository(owner: $owner, name: $repo) {\n        pullRequest(number: $prNumber) {\n          reviewThreads(first: 100, after: $cursor) {\n            nodes {\n              id\n              isResolved\n              resolvedBy {\n                login\n              }\n              path\n              line\n              originalLine\n              startLine\n              comments(first: 1) {\n                nodes {\n                  body\n                  createdAt\n                  author {\n                    login\n                  }\n                }\n              }\n            }\n            pageInfo {\n              hasNextPage\n              endCursor\n            }\n          }\n        }\n      }\n    }\n    \"\"\"\n\n    try:\n        # Use gh api to make GraphQL request with pagination\n        import json\n\n        all_threads = []\n        cursor = None\n        has_next_page = True\n        page_count = 0\n\n        while has_next_page:\n            page_count += 1\n\n            # Build command with cursor if available\n            cmd = [\n                \"gh\",\n                \"api\",\n                \"graphql\",\n                \"-F\",\n                f\"owner={owner}\",\n                \"-F\",\n                f\"repo={repo}\",\n                \"-F\",\n                f\"prNumber={pr_number}\",\n                \"-f\",\n                f\"query={query}\",\n            ]\n            if cursor:\n                cmd.extend([\"-F\", f\"cursor={cursor}\"])\n\n            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n            data = json.loads(result.stdout)\n\n            # Extract review threads and pagination info\n            threads_data = (\n                data.get(\"data\", {})\n                .get(\"repository\", {})\n                .get(\"pullRequest\", {})\n                .get(\"reviewThreads\", {})\n            )\n            threads = threads_data.get(\"nodes\", [])\n            page_info = threads_data.get(\"pageInfo\", {})\n\n            all_threads.extend(threads)\n\n            has_next_page = page_info.get(\"hasNextPage\", False)\n            cursor = page_info.get(\"endCursor\")\n\n            if page_count > 1:\n                print(\n                    f\"Fetched page {page_count} of review threads ({len(threads)} threads)\",\n                    file=sys.stderr,\n                )\n\n        # Now filter all collected threads\n\n        # Filter to only CodeRabbit threads and return as list of dicts\n        coderabbit_threads = []\n        for thread in all_threads:\n            if thread and thread.get(\"comments\", {}).get(\"nodes\", []):\n                comment = thread[\"comments\"][\"nodes\"][0]\n                author_login = comment.get(\"author\", {}).get(\"login\", \"\")\n                # Check for both coderabbitai and coderabbitai[bot] formats\n                if author_login in [\"coderabbitai\", \"coderabbitai[bot]\"]:\n                    # Extract line range (handle both single line and ranges)\n                    start_line = thread.get(\"startLine\")\n                    end_line = thread.get(\"line\") or thread.get(\"originalLine\")\n\n                    coderabbit_threads.append(\n                        {\n                            \"id\": thread[\"id\"],\n                            \"isResolved\": thread.get(\"isResolved\", False),\n                            \"resolvedBy\": thread.get(\"resolvedBy\", {}).get(\"login\")\n                            if thread.get(\"resolvedBy\")\n                            else None,\n                            \"body\": comment.get(\"body\", \"\"),\n                            \"createdAt\": comment.get(\"createdAt\", \"\"),\n                            \"author\": author_login,\n                            \"path\": thread.get(\"path\"),\n                            \"line\": end_line,\n                            \"startLine\": start_line,\n                        }\n                    )\n\n        return coderabbit_threads\n\n    except subprocess.CalledProcessError as e:\n        print(\n            f\"Warning: Could not fetch review threads via GraphQL: {e.stderr}\",\n            file=sys.stderr,\n        )\n        return []\n    except json.JSONDecodeError as e:\n        print(f\"Warning: Could not parse GraphQL response: {e}\", file=sys.stderr)\n        return []\n    except Exception as e:\n        print(f\"Warning: Error processing GraphQL data: {e}\", file=sys.stderr)\n        return []\n\n\ndef extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter reviews to only CodeRabbit ones.\"\"\"\n    return [\n        review\n        for review in reviews\n        if review.get(\"author\", {}).get(\"login\") == \"coderabbitai\"\n    ]\n\n\ndef filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter out comments that have been marked as resolved.\"\"\"\n    return [\n        comment\n        for comment in comments\n        if not is_comment_resolved(comment.get(\"body\", \"\"))\n    ]\n\n\ndef filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter out review threads that have been marked as resolved via GitHub's native resolution.\"\"\"\n    return [thread for thread in threads if not thread.get(\"isResolved\", False)]\n\n\ndef filter_resolved_review_content(review_body: str) -> str:\n    \"\"\"Filter out resolved suggestions from review body content.\"\"\"\n    # Look for resolved comment patterns and remove them\n    # This handles CodeRabbit's text-based resolution markers\n\n    # Pattern to match resolved comments within the review body\n    # Look for sections that contain resolution markers\n    resolved_patterns = [\n        # Match individual comment sections that are resolved\n        r\"<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>\",\n        # Match line-specific resolved comments\n        r\"`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)\",\n    ]\n\n    filtered_body = review_body\n    for pattern in resolved_patterns:\n        filtered_body = re.sub(\n            pattern, \"\", filtered_body, flags=re.DOTALL | re.IGNORECASE\n        )\n\n    # Also remove any standalone resolution lines\n    filtered_body = re.sub(\n        r\"^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?\",\n        \"\",\n        filtered_body,\n        flags=re.MULTILINE | re.IGNORECASE,\n    )\n\n    return filtered_body.strip()\n\n\ndef is_comment_resolved(comment_body: str) -> bool:\n    \"\"\"Check if a comment has been marked as resolved/addressed.\"\"\"\n    # Look for resolution indicators in the comment body\n    resolution_patterns = [\n        r\"âœ…\\s*Addressed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n        r\"âœ…\\s*Resolved\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n        r\"âœ…\\s*Fixed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n        r\"âœ…\\s*Completed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?\",\n        r\"\\[x\\]\",  # Checkbox checked\n        r\"\\[X\\]\",  # Checkbox checked\n    ]\n\n    for pattern in resolution_patterns:\n        if re.search(pattern, comment_body, re.IGNORECASE):\n            return True\n\n    return False\n\n\ndef extract_coderabbit_inline_comments(\n    comments: List[Dict[str, Any]],\n) -> List[Dict[str, Any]]:\n    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n    return [\n        comment\n        for comment in comments\n        if comment.get(\"user\", {}).get(\"login\") == \"coderabbitai[bot]\"\n    ]\n\n\ndef parse_review_type(body: str) -> str:\n    \"\"\"Determine the type of CodeRabbit review.\"\"\"\n    if \"Actionable comments posted:\" in body:\n        return \"ACTIONABLE_REVIEW\"\n    elif \"walkthrough_start\" in body and \"walkthrough_end\" in body:\n        return \"WALKTHROUGH\"\n    elif (\n        \"<!-- This is an auto-generated comment: summarize by coderabbit.ai -->\" in body\n    ):\n        return \"SUMMARY\"\n    else:\n        return \"OTHER\"\n\n\ndef clean_html_artifacts(text: str) -> str:\n    \"\"\"Remove HTML comments and artifacts from text.\"\"\"\n    # Remove HTML comments\n    text = re.sub(r\"<!--.*?-->\", \"\", text, flags=re.DOTALL)\n\n    # Remove CodeRabbit warning blocks\n    text = re.sub(\n        r\">\\s*â€¼ï¸\\s*\\*\\*IMPORTANT\\*\\*.*?(?=\\n\\n|\\n$)\", \"\", text, flags=re.DOTALL\n    )\n\n    # Remove other quote block artifacts\n    text = re.sub(r\">\\s*Carefully review.*?(?=\\n\\n|\\n$)\", \"\", text, flags=re.DOTALL)\n\n    # Remove empty lines that result from artifact removal\n    text = re.sub(r\"\\n\\s*\\n\\s*\\n\", \"\\n\\n\", text)\n\n    return text.strip()\n\n\ndef clean_diff_artifacts(diff: str) -> str:\n    \"\"\"Clean up diff artifacts and markers.\"\"\"\n    if not diff:\n        return diff\n\n    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n    diff = re.sub(r\"^@@.*?@@.*?\\n\", \"\", diff, flags=re.MULTILINE)  # Full hunk headers\n    diff = re.sub(r\"^\\s*@@\\s*\\n\", \"\", diff, flags=re.MULTILINE)  # Lone @@ lines\n\n    # Remove empty lines at start/end\n    diff = diff.strip()\n\n    return diff\n\n\ndef remove_details_sections(text: str) -> str:\n    \"\"\"Remove verbose <details> sections containing analysis chains and script outputs.\n\n    Removes complete <details>...</details> blocks to filter out:\n    - Analysis/reasoning chains\n    - Script execution outputs\n    - Internal processing logs\n\n    This helps keep the output focused on actionable feedback.\n    \"\"\"\n    # Remove all <details>...</details> blocks including nested content\n    text = re.sub(r\"<details>.*?</details>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n\n    # Clean up any resulting excessive whitespace\n    text = re.sub(r\"\\n\\s*\\n\\s*\\n\", \"\\n\\n\", text)\n\n    return text.strip()\n\n\ndef extract_file_and_lines_from_body(body: str) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"Extract file path and line range from CodeRabbit comment body.\n\n    CodeRabbit comments typically include file references like:\n    - \"In <filepath> around line X\"\n    - \"In <filepath> around lines X to Y\"\n    - \"In <filepath> around lines X-Y\"\n\n    Returns:\n        Tuple of (file_path, line_range) or (None, None) if not found\n    \"\"\"\n    # Pattern to match file path references in CodeRabbit comments\n    # Matches: \"In <filepath> around line(s) X\" or \"around lines X to Y\" or \"around lines X-Y\"\n    file_pattern = r\"In\\s+([^\\s]+(?:\\.ts|\\.tsx|\\.js|\\.jsx|\\.py|\\.java|\\.go|\\.rs|\\.md|\\.json|\\.yaml|\\.yml))\\s+around\\s+lines?\\s+([\\d\\-to\\s]+)\"\n\n    match = re.search(file_pattern, body, re.IGNORECASE)\n    if match:\n        file_path = match.group(1).strip()\n        line_range = match.group(2).strip()\n\n        # Normalize line range format (convert \"X to Y\" to \"X-Y\")\n        line_range = re.sub(r\"(\\d+)\\s+to\\s+(\\d+)\", r\"\\1-\\2\", line_range)\n\n        return (file_path, line_range)\n\n    return (None, None)\n\n\ndef extract_prompt_for_ai_agents(body: str) -> List[str]:\n    \"\"\"Extract 'Prompt for AI Agents' sections from review body.\"\"\"\n    prompts = []\n\n    # Pattern to find ğŸ¤– Prompt for AI Agents sections\n    # These appear to be in <summary> tags followed by clipboard content\n    pattern = r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*<div[^>]*data-snippet-clipboard-copy-content=\"([^\"]*)\"'\n    matches = re.findall(pattern, body, re.DOTALL)\n\n    for match in matches:\n        # Decode HTML entities\n        clean_text = (\n            match.replace(\"&lt;\", \"<\")\n            .replace(\"&gt;\", \">\")\n            .replace(\"&amp;\", \"&\")\n            .replace(\"&quot;\", '\"')\n            .replace(\"\\\\n\", \"\\n\")\n            .strip()\n        )\n        if clean_text:\n            prompts.append(clean_text)\n\n    return prompts\n\n\n# Removed unused functions - now using parse_review_sections instead\n\n\ndef parse_review_sections(body: str) -> Dict[str, Any]:\n    \"\"\"Parse different sections from CodeRabbit review body using HTML parsing.\"\"\"\n    sections = {}\n\n    # Extract actionable comments count from text\n    actionable_match = re.search(r\"Actionable comments posted:\\s*(\\d+)\", body)\n    if actionable_match:\n        sections[\"actionable_count\"] = int(actionable_match.group(1))\n\n    # Parse HTML structure - but be careful not to mangle code content\n    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n    soup = BeautifulSoup(body, \"html.parser\")\n\n    # Find sections by looking for <details> elements with specific summary text\n    details_elements = soup.find_all(\"details\")\n\n    for details in details_elements:\n        summary = details.find(\"summary\")\n        if not summary:\n            continue\n\n        summary_text = summary.get_text(strip=True)\n\n        # Check for outside diff range comments (avoid emoji dependency)\n        if \"Outside diff range comments\" in summary_text:\n            # Extract count from summary text\n            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n            if count_match:\n                count = int(count_match.group(1))\n                blockquote = details.find(\"blockquote\")\n                if blockquote:\n                    # Get all content inside the blockquote\n                    content = str(blockquote)\n                    sections[\"outside_diff_comments\"] = {\n                        \"count\": count,\n                        \"content\": content,\n                    }\n\n        # Check for nitpick comments or duplicate comments\n        elif \"Nitpick comments\" in summary_text or \"Duplicate comments\" in summary_text:\n            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n            if count_match:\n                count = int(count_match.group(1))\n                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n                # extract the raw content from the original body text\n\n                # Find the position of this details section in the original body\n                details_start = body.find(\"Nitpick comments\")\n                if details_start != -1:\n                    # Find where this section ends (next major section or end)\n                    section_markers = [\n                        \"ğŸ“œ Review details\",\n                        \"ğŸ”‡ Additional comments\",\n                        \"</details>\\n\\n</details>\",\n                    ]\n                    section_end = len(body)\n\n                    for marker in section_markers:\n                        marker_pos = body.find(marker, details_start)\n                        if marker_pos != -1:\n                            section_end = min(section_end, marker_pos)\n\n                    # Extract the raw text content\n                    raw_content = body[details_start:section_end]\n                    sections[\"nitpick_comments\"] = {\n                        \"count\": count,\n                        \"content\": raw_content,\n                    }\n\n        # Check for review details (usually at the end)\n        elif \"Review details\" in summary_text:\n            # This section contains metadata, we might want it for context\n            blockquote = details.find(\"blockquote\")\n            if blockquote:\n                content = str(blockquote)\n                sections[\"review_details\"] = {\"content\": content}\n\n    # Extract AI prompts (look for specific patterns)\n    sections[\"ai_prompts\"] = extract_prompt_for_ai_agents(body)\n\n    return sections\n\n\ndef parse_file_level_comments(\n    content: str, debug: bool = False\n) -> List[Dict[str, Any]]:\n    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n    comments = []\n\n    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n    file_pattern = r\"<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>\"\n    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n\n    if debug:\n        print(\n            f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\",\n            file=sys.stderr,\n        )\n\n    for file_match in file_matches:\n        file_path = file_match.group(1).strip()\n        comment_count = int(file_match.group(2))\n        file_content = file_match.group(\n            3\n        )  # Raw blockquote content, not parsed by BeautifulSoup\n\n        # Now parse line comments within this file content\n        # Find line comment patterns: `16-24`: **Title**\n        line_pattern = r\"`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)\"\n        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n\n        if debug:\n            print(\n                f\"DEBUG: File {file_path} has {len(line_matches)} line comments\",\n                file=sys.stderr,\n            )\n\n        for line_match in line_matches:\n            line_range = line_match.group(1).strip()\n            title = line_match.group(2).strip()\n            content_block = line_match.group(3).strip()\n\n            # Remove verbose <details> sections before processing\n            content_block = remove_details_sections(content_block)\n\n            # Check if this looks like a line reference\n            if re.match(r\"^[\\d\\-,\\s]+$\", line_range):\n                # Extract description (everything before first code block)\n                description = content_block\n                code_diff = \"\"\n\n                code_start = content_block.find(\"```\")\n                if code_start != -1:\n                    description = content_block[:code_start].strip()\n\n                    # Extract code diff from raw content (no BeautifulSoup mangling)\n                    diff_match = re.search(\n                        r\"```diff\\n(.*?)\\n```\", content_block, re.DOTALL\n                    )\n                    if diff_match:\n                        code_diff = diff_match.group(1).strip()\n                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n                        code_diff = html.unescape(code_diff)\n                        code_diff = clean_html_artifacts(\n                            code_diff\n                        )  # Remove any HTML comments in diffs\n                        code_diff = clean_diff_artifacts(\n                            code_diff\n                        )  # Clean up @@ markers and artifacts\n\n                # Clean up description\n                description = clean_html_artifacts(\n                    description\n                )  # Remove HTML comments first\n                description_lines = []\n                for line in description.split(\"\\n\"):\n                    line = line.strip()\n                    if (\n                        line\n                        and not line.startswith(\"Also applies to:\")\n                        and line != \"---\"\n                    ):\n                        description_lines.append(line)\n                description = \" \".join(description_lines)\n\n                comments.append(\n                    {\n                        \"file\": file_path,\n                        \"lines\": line_range,\n                        \"title\": title,\n                        \"description\": description,\n                        \"code_diff\": code_diff,\n                    }\n                )\n\n    return comments\n\n\ndef group_comments_by_file(\n    coderabbit_reviews: List[Dict[str, Any]],\n    inline_comments: List[Dict[str, Any]] = None,\n    debug: bool = False,\n    include_resolved: bool = True,\n) -> Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Group all comments by file for better organization.\"\"\"\n    file_groups = {}\n\n    # Process review body comments\n    for review in coderabbit_reviews:\n        review_body = review[\"body\"]\n\n        # Filter out resolved content from review body if requested\n        if not include_resolved:\n            original_body = review_body\n            review_body = filter_resolved_review_content(review_body)\n            if debug and len(original_body) != len(review_body):\n                print(\n                    f\"DEBUG: Filtered resolved content from review body (length: {len(original_body)} -> {len(review_body)})\",\n                    file=sys.stderr,\n                )\n\n        sections = parse_review_sections(review_body)\n\n        # Add outside diff comments\n        if \"outside_diff_comments\" in sections:\n            outside_comments = parse_file_level_comments(\n                sections[\"outside_diff_comments\"][\"content\"], debug\n            )\n            for comment in outside_comments:\n                file_path = comment[\"file\"]\n                if file_path not in file_groups:\n                    file_groups[file_path] = []\n                comment[\"source\"] = \"outside_diff\"\n                file_groups[file_path].append(comment)\n\n        # Add nitpick comments\n        if \"nitpick_comments\" in sections:\n            nitpick_comments = parse_file_level_comments(\n                sections[\"nitpick_comments\"][\"content\"], debug\n            )\n            for comment in nitpick_comments:\n                file_path = comment[\"file\"]\n                if file_path not in file_groups:\n                    file_groups[file_path] = []\n                comment[\"source\"] = \"nitpick\"\n                file_groups[file_path].append(comment)\n\n    # Process inline comments\n    if inline_comments:\n        for comment in inline_comments:\n            body = comment.get(\"body\", \"\")\n\n            # Try to extract file path and line range from comment body\n            extracted_file, extracted_lines = extract_file_and_lines_from_body(body)\n\n            # Use extracted values if available, otherwise fall back to comment metadata\n            file_path = (\n                extracted_file if extracted_file else comment.get(\"path\", \"unknown\")\n            )\n            line_num = comment.get(\"original_line\", \"unknown\")\n            start_line = comment.get(\"start_line\")\n\n            # Determine line range to display\n            if extracted_lines:\n                # Use extracted line range from body (highest priority)\n                line_range = extracted_lines\n            elif (\n                start_line\n                and line_num\n                and line_num != \"unknown\"\n                and line_num != 0\n                and start_line != line_num\n            ):\n                # Use GraphQL line range (start_line to line_num)\n                line_range = f\"{start_line}-{line_num}\"\n            elif line_num != \"unknown\" and line_num != 0:\n                # Single line from metadata\n                line_range = str(line_num)\n            else:\n                # No line info available\n                line_range = \"\"\n\n            if file_path not in file_groups:\n                file_groups[file_path] = []\n\n            # Convert inline comment to standard format\n            comment_type = (\n                \"ğŸ› ï¸ Refactor Suggestion\"\n                if \"ğŸ› ï¸ Refactor suggestion\" in body\n                else \"Comment\"\n            )\n\n            # Extract title from body\n            title_match = re.search(r\"\\*\\*(.*?)\\*\\*\", body)\n            title = title_match.group(1).strip() if title_match else \"Suggestion\"\n\n            # Extract description and diff\n            if \"<details>\" in body and \"Prompt for AI Agents\" in body:\n                main_content = body.split(\"<details>\")[0].strip()\n            else:\n                main_content = body\n\n            # Remove verbose <details> sections\n            main_content = remove_details_sections(main_content)\n            main_content = main_content.replace(\"_ğŸ› ï¸ Refactor suggestion_\", \"\").strip()\n            main_content = clean_html_artifacts(main_content)\n\n            # Extract AI prompt\n            ai_prompt = \"\"\n            ai_prompt_match = re.search(\n                r\"<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*\\n\\n```\\n(.*?)\\n```\",\n                body,\n                re.DOTALL,\n            )\n            if ai_prompt_match:\n                ai_prompt = ai_prompt_match.group(1).strip()\n\n            file_groups[file_path].append(\n                {\n                    \"lines\": line_range,\n                    \"title\": title,\n                    \"description\": main_content,\n                    \"code_diff\": \"\",  # Inline comments don't have separate diffs\n                    \"ai_prompt\": ai_prompt,\n                    \"source\": \"inline\",\n                    \"comment_type\": comment_type,\n                }\n            )\n\n    return file_groups\n\n\ndef format_for_llm(\n    coderabbit_reviews: List[Dict[str, Any]],\n    inline_comments: List[Dict[str, Any]] = None,\n    debug: bool = False,\n    preamble: Optional[str] = None,\n    include_resolved: bool = True,\n) -> str:\n    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n    output = []\n\n    # Add custom preamble if provided\n    if preamble:\n        output.append(preamble)\n        output.append(\"\")\n        output.append(\"=\" * 60)\n        output.append(\"\")\n\n    output.append(\"# CodeRabbit Review Feedback\")\n    output.append(\"=\" * 40)\n    output.append(\"\")\n\n    # Group all comments by file\n    file_groups = group_comments_by_file(\n        coderabbit_reviews, inline_comments, debug, include_resolved\n    )\n\n    if not file_groups:\n        output.append(\"No actionable comments found.\")\n        return \"\\n\".join(output)\n\n    # Add summary\n    total_comments = sum(len(comments) for comments in file_groups.values())\n    output.append(f\"**Total files with feedback:** {len(file_groups)}\")\n    output.append(f\"**Total comments:** {total_comments}\")\n    output.append(\"\")\n\n    # Process each file\n    for file_path in sorted(file_groups.keys()):\n        comments = file_groups[file_path]\n\n        output.append(f\"## {file_path}\")\n        output.append(f\"**{len(comments)} suggestion(s)**\")\n        output.append(\"\")\n\n        # Sort comments by priority: AI prompts first, then by line number\n        def get_sort_key(comment):\n            # Priority 1: Comments with AI prompts (highest priority)\n            has_ai_prompt = bool(comment.get(\"ai_prompt\"))\n\n            # Priority 2: Line number (for logical code flow)\n            line_num = 999\n            lines = comment.get(\"lines\", \"\")\n            if lines and lines.replace(\"-\", \"\").isdigit():\n                line_num = int(lines.split(\"-\")[0])\n\n            # Return tuple: (ai_prompt_priority, line_number)\n            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n            return (0 if has_ai_prompt else 1, line_num)\n\n        comments_sorted = sorted(comments, key=get_sort_key)\n\n        for i, comment in enumerate(comments_sorted, 1):\n            # Add AI prompt indicator to title for high-priority items\n            title = comment[\"title\"]\n            if comment.get(\"ai_prompt\"):\n                title = f\"ğŸ¤– {title}\"\n\n            # Only include line numbers if they're meaningful (not empty or \"0\")\n            lines = comment.get(\"lines\", \"\")\n            if lines and lines != \"0\":\n                output.append(f\"### {i}. Lines {lines}: {title}\")\n            else:\n                output.append(f\"### {i}. {title}\")\n            output.append(\"\")\n\n            # Show AI prompt, or fallback to description + diff\n            has_prompt = bool(comment.get(\"ai_prompt\"))\n            has_diff = bool(comment.get(\"code_diff\"))\n\n            if debug:\n                output.append(\n                    f\"<!-- DEBUG: has_prompt={has_prompt}, has_diff={has_diff}, source={comment.get('source')} -->\"\n                )\n\n            if has_prompt:\n                if debug:\n                    output.append(\n                        \"<!-- DEBUG: Showing prompt only, skipping description and diff -->\"\n                    )\n                # AI prompt contains better explanation than description + diff\n                output.append(\"**Proposed prompt:**\")\n                output.append(\"```\")\n                output.append(comment[\"ai_prompt\"])\n                output.append(\"```\")\n                output.append(\"\")\n            else:\n                if debug:\n                    output.append(\n                        \"<!-- DEBUG: No prompt, showing description and diff -->\"\n                    )\n\n                # Show description only if no AI prompt available\n                if comment[\"description\"] and len(comment[\"description\"]) > 10:\n                    desc = comment[\"description\"]\n                    desc = re.sub(\n                        r\"^_.*?_\\s*\", \"\", desc\n                    )  # Remove _âš ï¸ Potential issue_ etc\n                    desc = re.sub(\n                        r\"\\*\\*.*?\\*\\*\\s*\", \"\", desc, count=1\n                    )  # Remove first **title** if duplicated\n                    desc = re.sub(r\"Apply this diff:\\s*\", \"\", desc)\n                    desc = desc.strip()\n\n                    if desc and desc != title:\n                        output.append(desc)\n                        output.append(\"\")\n\n                # Show diff only if no AI prompt available\n                if has_diff:\n                    output.append(\"```diff\")\n                    output.append(comment[\"code_diff\"])\n                    output.append(\"```\")\n                    output.append(\"\")\n\n            if i < len(comments_sorted):\n                output.append(\"-\" * 30)\n                output.append(\"\")\n\n        output.append(\"=\" * 60)\n        output.append(\"\")\n\n    return \"\\n\".join(output)\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"\"\"\nCodeRabbit Review Extractor - Convert GitHub PR reviews to LLM-friendly text\n\nThis tool extracts CodeRabbit automated code review feedback from GitHub pull \nrequests and formats it as clean, organized text suitable for AI coding agents.\n\nPerfect for feeding CodeRabbit suggestions to Claude, ChatGPT, or other LLMs\nto automatically apply code improvements, refactoring suggestions, and fixes.\n        \"\"\".strip(),\n        epilog=\"\"\"\nEXAMPLES:\n  %(prog)s https://github.com/owner/repo/pull/123\n  %(prog)s owner/repo/123\n  %(prog)s obra/lace/278 --all-reviews\n\nWHAT IT EXTRACTS:\n  â€¢ Refactor suggestions with specific code diffs\n  â€¢ AI implementation instructions (ğŸ¤– prompts) with detailed steps\n  â€¢ Nitpick comments and code quality improvements\n  â€¢ File-organized feedback sorted by priority\n  â€¢ Clean format without HTML artifacts or noise\n\nHOW IT WORKS:\n  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n  2. Extracts both main review summaries and inline code comments\n  3. Organizes feedback by file with line-number targeting\n  4. Prioritizes actionable items with AI implementation guides\n  5. Outputs clean text ready for LLM consumption\n\nDEFAULT BEHAVIOR:\n  Processes only the LATEST CodeRabbit review to avoid overwhelming output.\n  Use --all-reviews for PRs with multiple review iterations if needed.\n\nREQUIREMENTS:\n  â€¢ GitHub CLI (gh) installed and authenticated\n  â€¢ Python 3.6+ with beautifulsoup4 package\n\nCONFIGURATION:\n  Create ~/.coderabbit-extractor with custom preamble text to prepend to output.\n  Useful for framing feedback (e.g., critical evaluation prompts for AI agents).\n        \"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n\n    parser.add_argument(\n        \"pr_input\",\n        help='GitHub PR URL or owner/repo/number format (e.g., \"owner/repo/123\")',\n    )\n    parser.add_argument(\n        \"--include-resolved\",\n        action=\"store_true\",\n        help=\"Include review suggestions that have been marked as resolved (default: filter them out)\",\n    )\n    parser.add_argument(\n        \"--all-reviews\",\n        action=\"store_true\",\n        help=\"Extract all CodeRabbit reviews (default: latest only)\",\n    )\n    parser.add_argument(\n        \"--since-commit\",\n        metavar=\"SHA\",\n        help=\"Extract reviews submitted after this commit SHA\",\n    )\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Show debug annotations in output to trace processing\",\n    )\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n    )\n\n    # Custom handling for no arguments to show helpful info\n    if len(sys.argv) == 1:\n        print(\"ğŸ¤– CodeRabbit Review Extractor\")\n        print(\"=\" * 40)\n        print()\n        print(\"WHAT THIS DOES:\")\n        print(\n            \"  Converts CodeRabbit GitHub PR reviews into clean text for AI coding agents\"\n        )\n        print(\"  (Claude, ChatGPT, etc.) to automatically apply code suggestions.\")\n        print()\n        print(\"QUICK START:\")\n        print(\n            \"  python3 extract-coderabbit-feedback.py https://github.com/owner/repo/pull/123\"\n        )\n        print(\"  python3 extract-coderabbit-feedback.py owner/repo/123\")\n        print()\n        print(\"For full help and options, run:\")\n        print(\"  python3 extract-coderabbit-feedback.py --help\")\n        sys.exit(1)\n\n    args = parser.parse_args()\n\n    try:\n        pr_url = parse_pr_input(args.pr_input)\n\n        # Load custom preamble from dotfile\n        preamble = load_preamble_config()\n\n        # Fetch both reviews and inline comments\n        reviews = fetch_pr_reviews(pr_url)\n        inline_comments = fetch_pr_inline_comments(pr_url)\n\n        # Extract PR info for GraphQL query\n        try:\n            owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n\n            # Fetch review threads with resolution status via GraphQL\n            review_threads = fetch_review_threads_graphql(owner, repo, pr_number)\n\n            if review_threads and not args.include_resolved:\n                # Filter out resolved threads (GitHub native conversation resolution)\n                original_thread_count = len(review_threads)\n                review_threads = filter_resolved_threads(review_threads)\n                filtered_thread_count = original_thread_count - len(review_threads)\n                if filtered_thread_count > 0:\n                    print(\n                        f\"Filtered resolved review threads: {filtered_thread_count} resolved threads hidden\",\n                        file=sys.stderr,\n                    )\n            elif review_threads:\n                print(\n                    f\"Found {len(review_threads)} review threads (including resolved)\",\n                    file=sys.stderr,\n                )\n\n        except Exception as e:\n            print(\n                f\"Warning: Could not fetch review thread resolution data: {e}\",\n                file=sys.stderr,\n            )\n            review_threads = []\n\n        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n\n        # Build a set of existing comment identifiers for deduplication\n        # Use (path, line, body_start) tuple to identify duplicates\n        existing_comments = set()\n        for comment in coderabbit_inline_comments:\n            path = comment.get(\"path\", \"\")\n            line = comment.get(\"original_line\", 0)\n            body = comment.get(\"body\", \"\")\n            # Use first 100 chars of body as identifier\n            body_start = body[:100] if body else \"\"\n            existing_comments.add((path, line, body_start))\n\n        # Convert review threads to inline comment format and merge\n        if review_threads:\n            for thread in review_threads:\n                # Extract line range if available\n                start_line = thread.get(\"startLine\")\n                end_line = thread.get(\"line\")\n\n                # Determine the line value to use\n                if start_line and end_line and start_line != end_line:\n                    # Multi-line range\n                    line_value = end_line  # Use end line for sorting\n                elif end_line:\n                    # Single line\n                    line_value = end_line\n                else:\n                    line_value = 0\n\n                # Check for duplicates before adding\n                path = thread.get(\"path\", \"unknown\")\n                body = thread.get(\"body\", \"\")\n                body_start = body[:100] if body else \"\"\n                comment_id = (path, line_value, body_start)\n\n                # Skip if this comment already exists (from REST API)\n                if comment_id in existing_comments:\n                    continue\n\n                # Convert thread to inline comment format\n                inline_comment = {\n                    \"id\": thread[\"id\"],\n                    \"body\": body,\n                    \"user\": {\"login\": \"coderabbitai[bot]\"},\n                    \"created_at\": thread.get(\"createdAt\", \"\"),\n                    \"path\": path,  # Use path from GraphQL\n                    \"original_line\": line_value,  # Use line from GraphQL\n                    \"start_line\": start_line,  # Store start_line for range information\n                    \"html_url\": f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\",\n                }\n                coderabbit_inline_comments.append(inline_comment)\n                existing_comments.add(comment_id)  # Add to set to track\n\n        # Filter out resolved comments if not including them\n        if not args.include_resolved:\n            original_count = len(coderabbit_inline_comments)\n            coderabbit_inline_comments = filter_resolved_comments(\n                coderabbit_inline_comments\n            )\n            filtered_count = original_count - len(coderabbit_inline_comments)\n            if filtered_count > 0:\n                print(\n                    f\"Filtered resolved comments: {filtered_count} resolved comments hidden\",\n                    file=sys.stderr,\n                )\n        # Note: Review-level filtering is handled in group_comments_by_file()\n\n        # Filter reviews based on options\n        if not args.all_reviews and not args.since_commit:\n            # Default: latest review with actual content\n            if coderabbit_reviews:\n                # Find the latest review with substantial content (not just continuation)\n                latest_review = None\n                for review in reversed(coderabbit_reviews):\n                    body = review.get(\"body\", \"\")\n                    # Skip continuation reviews that have minimal content\n                    if (\n                        len(body) > 100\n                        and \"Actionable comments posted:\" in body\n                        and not body.strip().startswith(\n                            \"**Review continued from previous batch\"\n                        )\n                    ):\n                        latest_review = review\n                        break\n\n                # Fallback to chronologically latest if no substantial review found\n                if not latest_review:\n                    latest_review = coderabbit_reviews[-1]\n\n                coderabbit_reviews = [latest_review]\n\n                # Filter inline comments to only those posted after the most recent commit\n                latest_commit_time = fetch_latest_commit_time(pr_url)\n                if latest_commit_time and coderabbit_inline_comments:\n                    from datetime import datetime\n\n                    try:\n                        # Parse ISO format timestamps (GitHub API standard)\n                        commit_dt = datetime.fromisoformat(\n                            latest_commit_time.replace(\"Z\", \"+00:00\")\n                        )\n\n                        filtered_inline = []\n                        for comment in coderabbit_inline_comments:\n                            comment_time = comment.get(\"created_at\", \"\")\n                            if comment_time:\n                                comment_dt = datetime.fromisoformat(\n                                    comment_time.replace(\"Z\", \"+00:00\")\n                                )\n                                # Include comments posted after the latest commit\n                                if comment_dt > commit_dt:\n                                    filtered_inline.append(comment)\n\n                        original_count = len(coderabbit_inline_comments)\n                        coderabbit_inline_comments = filtered_inline\n                        if original_count != len(filtered_inline):\n                            print(\n                                f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\",\n                                file=sys.stderr,\n                            )\n\n                    except Exception as e:\n                        print(\n                            f\"Warning: Could not filter inline comments by commit time: {e}\",\n                            file=sys.stderr,\n                        )\n        elif args.since_commit:\n            # Filter reviews submitted after the specified commit\n            # This would require additional logic to compare timestamps with commit dates\n            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n            sys.exit(1)\n\n        if not coderabbit_reviews and not coderabbit_inline_comments:\n            print(\n                \"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr\n            )\n            sys.exit(1)\n\n        # Show processing info\n        if len(coderabbit_reviews) > 1:\n            print(\n                f\"Processing {len(coderabbit_reviews)} CodeRabbit reviews...\",\n                file=sys.stderr,\n            )\n        elif len(coderabbit_reviews) == 1:\n            print(\"Processing latest CodeRabbit review...\", file=sys.stderr)\n\n        if coderabbit_inline_comments:\n            print(\n                f\"Found {len(coderabbit_inline_comments)} inline comments\",\n                file=sys.stderr,\n            )\n\n        formatted_output = format_for_llm(\n            coderabbit_reviews,\n            coderabbit_inline_comments,\n            args.debug,\n            preamble,\n            args.include_resolved,\n        )\n        print(formatted_output)\n\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n","numLines":1255,"startLine":1,"totalLines":1255}}}
{"parentUuid":"61f9c704-b756-4f92-b8e8-24568fb98e88","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01GfWpxu4Wjpbt8sdgZcf913","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me examine the key sections to understand how \"Outside diff range comments\" are processed:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":17075,"cache_read_input_tokens":8678,"cache_creation":{"ephemeral_5m_input_tokens":17075,"ephemeral_1h_input_tokens":0},"output_tokens":172,"service_tier":"standard"}},"requestId":"req_011CV7nRR7S23VCs3gTQcimL","type":"assistant","uuid":"936c12b8-8b37-48b8-9b71-ca059bf13cd0","timestamp":"2025-11-14T12:30:04.479Z"}
{"parentUuid":"936c12b8-8b37-48b8-9b71-ca059bf13cd0","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01GfWpxu4Wjpbt8sdgZcf913","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01S52TWe9TtA7MwSy2WyTZ7m","name":"Grep","input":{"pattern":"Outside diff range","path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","output_mode":"content","-B":3,"-A":15}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":17075,"cache_read_input_tokens":8678,"cache_creation":{"ephemeral_5m_input_tokens":17075,"ephemeral_1h_input_tokens":0},"output_tokens":172,"service_tier":"standard"}},"requestId":"req_011CV7nRR7S23VCs3gTQcimL","type":"assistant","uuid":"4c9abc6f-ec43-49e6-ab07-ee9ed2a24802","timestamp":"2025-11-14T12:30:06.059Z"}
{"parentUuid":"4c9abc6f-ec43-49e6-ab07-ee9ed2a24802","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01S52TWe9TtA7MwSy2WyTZ7m","type":"tool_result","content":"519-        summary_text = summary.get_text(strip=True)\n520-\n521-        # Check for outside diff range comments (avoid emoji dependency)\n522:        if \"Outside diff range comments\" in summary_text:\n523-            # Extract count from summary text\n524-            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n525-            if count_match:\n526-                count = int(count_match.group(1))\n527-                blockquote = details.find(\"blockquote\")\n528-                if blockquote:\n529-                    # Get all content inside the blockquote\n530-                    content = str(blockquote)\n531-                    sections[\"outside_diff_comments\"] = {\n532-                        \"count\": count,\n533-                        \"content\": content,\n534-                    }\n535-\n536-        # Check for nitpick comments or duplicate comments\n537-        elif \"Nitpick comments\" in summary_text or \"Duplicate comments\" in summary_text:"}]},"uuid":"d349f37c-084e-40e5-a82c-3d6cdb85fc0d","timestamp":"2025-11-14T12:30:06.553Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"519-        summary_text = summary.get_text(strip=True)\n520-\n521-        # Check for outside diff range comments (avoid emoji dependency)\n522:        if \"Outside diff range comments\" in summary_text:\n523-            # Extract count from summary text\n524-            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n525-            if count_match:\n526-                count = int(count_match.group(1))\n527-                blockquote = details.find(\"blockquote\")\n528-                if blockquote:\n529-                    # Get all content inside the blockquote\n530-                    content = str(blockquote)\n531-                    sections[\"outside_diff_comments\"] = {\n532-                        \"count\": count,\n533-                        \"content\": content,\n534-                    }\n535-\n536-        # Check for nitpick comments or duplicate comments\n537-        elif \"Nitpick comments\" in summary_text or \"Duplicate comments\" in summary_text:","numLines":19}}
{"parentUuid":"d349f37c-084e-40e5-a82c-3d6cdb85fc0d","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01CQtz6iAN1Wf1bdr2RC4rX4","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check how these outside_diff_comments are integrated into the file grouping:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":640,"cache_read_input_tokens":25753,"cache_creation":{"ephemeral_5m_input_tokens":640,"ephemeral_1h_input_tokens":0},"output_tokens":174,"service_tier":"standard"}},"requestId":"req_011CV7nRptR73o8Tbk13vW4w","type":"assistant","uuid":"cd027abb-4937-440e-af92-7a18ef1bf8d6","timestamp":"2025-11-14T12:30:10.004Z"}
{"parentUuid":"cd027abb-4937-440e-af92-7a18ef1bf8d6","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01CQtz6iAN1Wf1bdr2RC4rX4","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014dwMzx4ZqZWXuXDvBU8NUS","name":"Grep","input":{"pattern":"outside_diff_comments","path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","output_mode":"content","-B":3,"-A":10}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":640,"cache_read_input_tokens":25753,"cache_creation":{"ephemeral_5m_input_tokens":640,"ephemeral_1h_input_tokens":0},"output_tokens":174,"service_tier":"standard"}},"requestId":"req_011CV7nRptR73o8Tbk13vW4w","type":"assistant","uuid":"754c13a7-a1bb-4b02-bd96-85433a0ddd3e","timestamp":"2025-11-14T12:30:10.905Z"}
{"parentUuid":"754c13a7-a1bb-4b02-bd96-85433a0ddd3e","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014dwMzx4ZqZWXuXDvBU8NUS","type":"tool_result","content":"528-                if blockquote:\n529-                    # Get all content inside the blockquote\n530-                    content = str(blockquote)\n531:                    sections[\"outside_diff_comments\"] = {\n532-                        \"count\": count,\n533-                        \"content\": content,\n534-                    }\n535-\n536-        # Check for nitpick comments or duplicate comments\n537-        elif \"Nitpick comments\" in summary_text or \"Duplicate comments\" in summary_text:\n538-            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n539-            if count_match:\n540-                count = int(count_match.group(1))\n541-                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n--\n700-        sections = parse_review_sections(review_body)\n701-\n702-        # Add outside diff comments\n703:        if \"outside_diff_comments\" in sections:\n704-            outside_comments = parse_file_level_comments(\n705:                sections[\"outside_diff_comments\"][\"content\"], debug\n706-            )\n707-            for comment in outside_comments:\n708-                file_path = comment[\"file\"]\n709-                if file_path not in file_groups:\n710-                    file_groups[file_path] = []\n711-                comment[\"source\"] = \"outside_diff\"\n712-                file_groups[file_path].append(comment)\n713-\n714-        # Add nitpick comments\n715-        if \"nitpick_comments\" in sections:"}]},"uuid":"82e52c33-6218-47a5-9656-4d4580b404c6","timestamp":"2025-11-14T12:30:11.370Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"528-                if blockquote:\n529-                    # Get all content inside the blockquote\n530-                    content = str(blockquote)\n531:                    sections[\"outside_diff_comments\"] = {\n532-                        \"count\": count,\n533-                        \"content\": content,\n534-                    }\n535-\n536-        # Check for nitpick comments or duplicate comments\n537-        elif \"Nitpick comments\" in summary_text or \"Duplicate comments\" in summary_text:\n538-            count_match = re.search(r\"\\((\\d+)\\)\", summary_text)\n539-            if count_match:\n540-                count = int(count_match.group(1))\n541-                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n--\n700-        sections = parse_review_sections(review_body)\n701-\n702-        # Add outside diff comments\n703:        if \"outside_diff_comments\" in sections:\n704-            outside_comments = parse_file_level_comments(\n705:                sections[\"outside_diff_comments\"][\"content\"], debug\n706-            )\n707-            for comment in outside_comments:\n708-                file_path = comment[\"file\"]\n709-                if file_path not in file_groups:\n710-                    file_groups[file_path] = []\n711-                comment[\"source\"] = \"outside_diff\"\n712-                file_groups[file_path].append(comment)\n713-\n714-        # Add nitpick comments\n715-        if \"nitpick_comments\" in sections:","numLines":31}}
{"parentUuid":"82e52c33-6218-47a5-9656-4d4580b404c6","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01KuKRvuav3GCBXRKk7C9dyn","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me examine the file-level comment parsing to see if the pattern matches the described format:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":778,"cache_read_input_tokens":26393,"cache_creation":{"ephemeral_5m_input_tokens":778,"ephemeral_1h_input_tokens":0},"output_tokens":134,"service_tier":"standard"}},"requestId":"req_011CV7nSBSgfMJomaFycNLC7","type":"assistant","uuid":"3bc8ef3e-380e-44a7-8baa-10f3de30bc19","timestamp":"2025-11-14T12:30:14.778Z"}
{"parentUuid":"3bc8ef3e-380e-44a7-8baa-10f3de30bc19","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01KuKRvuav3GCBXRKk7C9dyn","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01V9nKuWibwHqueq3FjviqXa","name":"Read","input":{"file_path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","offset":581,"limit":100}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":778,"cache_read_input_tokens":26393,"cache_creation":{"ephemeral_5m_input_tokens":778,"ephemeral_1h_input_tokens":0},"output_tokens":134,"service_tier":"standard"}},"requestId":"req_011CV7nSBSgfMJomaFycNLC7","type":"assistant","uuid":"e8c2a3ca-3f0e-4ced-9b20-2f1986b8903d","timestamp":"2025-11-14T12:30:15.548Z"}
{"parentUuid":"e8c2a3ca-3f0e-4ced-9b20-2f1986b8903d","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01V9nKuWibwHqueq3FjviqXa","type":"tool_result","content":"   581â†’def parse_file_level_comments(\n   582â†’    content: str, debug: bool = False\n   583â†’) -> List[Dict[str, Any]]:\n   584â†’    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n   585â†’    comments = []\n   586â†’\n   587â†’    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n   588â†’    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n   589â†’    file_pattern = r\"<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>\"\n   590â†’    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n   591â†’\n   592â†’    if debug:\n   593â†’        print(\n   594â†’            f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\",\n   595â†’            file=sys.stderr,\n   596â†’        )\n   597â†’\n   598â†’    for file_match in file_matches:\n   599â†’        file_path = file_match.group(1).strip()\n   600â†’        comment_count = int(file_match.group(2))\n   601â†’        file_content = file_match.group(\n   602â†’            3\n   603â†’        )  # Raw blockquote content, not parsed by BeautifulSoup\n   604â†’\n   605â†’        # Now parse line comments within this file content\n   606â†’        # Find line comment patterns: `16-24`: **Title**\n   607â†’        line_pattern = r\"`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)\"\n   608â†’        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n   609â†’\n   610â†’        if debug:\n   611â†’            print(\n   612â†’                f\"DEBUG: File {file_path} has {len(line_matches)} line comments\",\n   613â†’                file=sys.stderr,\n   614â†’            )\n   615â†’\n   616â†’        for line_match in line_matches:\n   617â†’            line_range = line_match.group(1).strip()\n   618â†’            title = line_match.group(2).strip()\n   619â†’            content_block = line_match.group(3).strip()\n   620â†’\n   621â†’            # Remove verbose <details> sections before processing\n   622â†’            content_block = remove_details_sections(content_block)\n   623â†’\n   624â†’            # Check if this looks like a line reference\n   625â†’            if re.match(r\"^[\\d\\-,\\s]+$\", line_range):\n   626â†’                # Extract description (everything before first code block)\n   627â†’                description = content_block\n   628â†’                code_diff = \"\"\n   629â†’\n   630â†’                code_start = content_block.find(\"```\")\n   631â†’                if code_start != -1:\n   632â†’                    description = content_block[:code_start].strip()\n   633â†’\n   634â†’                    # Extract code diff from raw content (no BeautifulSoup mangling)\n   635â†’                    diff_match = re.search(\n   636â†’                        r\"```diff\\n(.*?)\\n```\", content_block, re.DOTALL\n   637â†’                    )\n   638â†’                    if diff_match:\n   639â†’                        code_diff = diff_match.group(1).strip()\n   640â†’                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n   641â†’                        code_diff = html.unescape(code_diff)\n   642â†’                        code_diff = clean_html_artifacts(\n   643â†’                            code_diff\n   644â†’                        )  # Remove any HTML comments in diffs\n   645â†’                        code_diff = clean_diff_artifacts(\n   646â†’                            code_diff\n   647â†’                        )  # Clean up @@ markers and artifacts\n   648â†’\n   649â†’                # Clean up description\n   650â†’                description = clean_html_artifacts(\n   651â†’                    description\n   652â†’                )  # Remove HTML comments first\n   653â†’                description_lines = []\n   654â†’                for line in description.split(\"\\n\"):\n   655â†’                    line = line.strip()\n   656â†’                    if (\n   657â†’                        line\n   658â†’                        and not line.startswith(\"Also applies to:\")\n   659â†’                        and line != \"---\"\n   660â†’                    ):\n   661â†’                        description_lines.append(line)\n   662â†’                description = \" \".join(description_lines)\n   663â†’\n   664â†’                comments.append(\n   665â†’                    {\n   666â†’                        \"file\": file_path,\n   667â†’                        \"lines\": line_range,\n   668â†’                        \"title\": title,\n   669â†’                        \"description\": description,\n   670â†’                        \"code_diff\": code_diff,\n   671â†’                    }\n   672â†’                )\n   673â†’\n   674â†’    return comments\n   675â†’\n   676â†’\n   677â†’def group_comments_by_file(\n   678â†’    coderabbit_reviews: List[Dict[str, Any]],\n   679â†’    inline_comments: List[Dict[str, Any]] = None,\n   680â†’    debug: bool = False,\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"334e39b0-8ae2-45b0-9d10-aca0fb037881","timestamp":"2025-11-14T12:30:15.673Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","content":"def parse_file_level_comments(\n    content: str, debug: bool = False\n) -> List[Dict[str, Any]]:\n    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n    comments = []\n\n    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n    file_pattern = r\"<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>\"\n    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n\n    if debug:\n        print(\n            f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\",\n            file=sys.stderr,\n        )\n\n    for file_match in file_matches:\n        file_path = file_match.group(1).strip()\n        comment_count = int(file_match.group(2))\n        file_content = file_match.group(\n            3\n        )  # Raw blockquote content, not parsed by BeautifulSoup\n\n        # Now parse line comments within this file content\n        # Find line comment patterns: `16-24`: **Title**\n        line_pattern = r\"`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)\"\n        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n\n        if debug:\n            print(\n                f\"DEBUG: File {file_path} has {len(line_matches)} line comments\",\n                file=sys.stderr,\n            )\n\n        for line_match in line_matches:\n            line_range = line_match.group(1).strip()\n            title = line_match.group(2).strip()\n            content_block = line_match.group(3).strip()\n\n            # Remove verbose <details> sections before processing\n            content_block = remove_details_sections(content_block)\n\n            # Check if this looks like a line reference\n            if re.match(r\"^[\\d\\-,\\s]+$\", line_range):\n                # Extract description (everything before first code block)\n                description = content_block\n                code_diff = \"\"\n\n                code_start = content_block.find(\"```\")\n                if code_start != -1:\n                    description = content_block[:code_start].strip()\n\n                    # Extract code diff from raw content (no BeautifulSoup mangling)\n                    diff_match = re.search(\n                        r\"```diff\\n(.*?)\\n```\", content_block, re.DOTALL\n                    )\n                    if diff_match:\n                        code_diff = diff_match.group(1).strip()\n                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n                        code_diff = html.unescape(code_diff)\n                        code_diff = clean_html_artifacts(\n                            code_diff\n                        )  # Remove any HTML comments in diffs\n                        code_diff = clean_diff_artifacts(\n                            code_diff\n                        )  # Clean up @@ markers and artifacts\n\n                # Clean up description\n                description = clean_html_artifacts(\n                    description\n                )  # Remove HTML comments first\n                description_lines = []\n                for line in description.split(\"\\n\"):\n                    line = line.strip()\n                    if (\n                        line\n                        and not line.startswith(\"Also applies to:\")\n                        and line != \"---\"\n                    ):\n                        description_lines.append(line)\n                description = \" \".join(description_lines)\n\n                comments.append(\n                    {\n                        \"file\": file_path,\n                        \"lines\": line_range,\n                        \"title\": title,\n                        \"description\": description,\n                        \"code_diff\": code_diff,\n                    }\n                )\n\n    return comments\n\n\ndef group_comments_by_file(\n    coderabbit_reviews: List[Dict[str, Any]],\n    inline_comments: List[Dict[str, Any]] = None,\n    debug: bool = False,","numLines":100,"startLine":581,"totalLines":1255}}}
{"parentUuid":"334e39b0-8ae2-45b0-9d10-aca0fb037881","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01AWZge4LJaFa556aiiZA9T3","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the review filtering logic to understand when reviews are processed:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":1914,"cache_read_input_tokens":27171,"cache_creation":{"ephemeral_5m_input_tokens":1914,"ephemeral_1h_input_tokens":0},"output_tokens":130,"service_tier":"standard"}},"requestId":"req_011CV7nSVtSje5WtfxCR5jhq","type":"assistant","uuid":"85811aed-c13d-4e9f-b202-573ec0f6d9cd","timestamp":"2025-11-14T12:30:19.305Z"}
{"parentUuid":"85811aed-c13d-4e9f-b202-573ec0f6d9cd","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01AWZge4LJaFa556aiiZA9T3","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CtSJEJTzjg6wrvA6veYKfU","name":"Read","input":{"file_path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","offset":1152,"limit":70}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":1914,"cache_read_input_tokens":27171,"cache_creation":{"ephemeral_5m_input_tokens":1914,"ephemeral_1h_input_tokens":0},"output_tokens":130,"service_tier":"standard"}},"requestId":"req_011CV7nSVtSje5WtfxCR5jhq","type":"assistant","uuid":"0c8add33-c25f-4355-823c-6b70e402edda","timestamp":"2025-11-14T12:30:20.004Z"}
{"parentUuid":"0c8add33-c25f-4355-823c-6b70e402edda","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CtSJEJTzjg6wrvA6veYKfU","type":"tool_result","content":"  1152â†’        # Filter reviews based on options\n  1153â†’        if not args.all_reviews and not args.since_commit:\n  1154â†’            # Default: latest review with actual content\n  1155â†’            if coderabbit_reviews:\n  1156â†’                # Find the latest review with substantial content (not just continuation)\n  1157â†’                latest_review = None\n  1158â†’                for review in reversed(coderabbit_reviews):\n  1159â†’                    body = review.get(\"body\", \"\")\n  1160â†’                    # Skip continuation reviews that have minimal content\n  1161â†’                    if (\n  1162â†’                        len(body) > 100\n  1163â†’                        and \"Actionable comments posted:\" in body\n  1164â†’                        and not body.strip().startswith(\n  1165â†’                            \"**Review continued from previous batch\"\n  1166â†’                        )\n  1167â†’                    ):\n  1168â†’                        latest_review = review\n  1169â†’                        break\n  1170â†’\n  1171â†’                # Fallback to chronologically latest if no substantial review found\n  1172â†’                if not latest_review:\n  1173â†’                    latest_review = coderabbit_reviews[-1]\n  1174â†’\n  1175â†’                coderabbit_reviews = [latest_review]\n  1176â†’\n  1177â†’                # Filter inline comments to only those posted after the most recent commit\n  1178â†’                latest_commit_time = fetch_latest_commit_time(pr_url)\n  1179â†’                if latest_commit_time and coderabbit_inline_comments:\n  1180â†’                    from datetime import datetime\n  1181â†’\n  1182â†’                    try:\n  1183â†’                        # Parse ISO format timestamps (GitHub API standard)\n  1184â†’                        commit_dt = datetime.fromisoformat(\n  1185â†’                            latest_commit_time.replace(\"Z\", \"+00:00\")\n  1186â†’                        )\n  1187â†’\n  1188â†’                        filtered_inline = []\n  1189â†’                        for comment in coderabbit_inline_comments:\n  1190â†’                            comment_time = comment.get(\"created_at\", \"\")\n  1191â†’                            if comment_time:\n  1192â†’                                comment_dt = datetime.fromisoformat(\n  1193â†’                                    comment_time.replace(\"Z\", \"+00:00\")\n  1194â†’                                )\n  1195â†’                                # Include comments posted after the latest commit\n  1196â†’                                if comment_dt > commit_dt:\n  1197â†’                                    filtered_inline.append(comment)\n  1198â†’\n  1199â†’                        original_count = len(coderabbit_inline_comments)\n  1200â†’                        coderabbit_inline_comments = filtered_inline\n  1201â†’                        if original_count != len(filtered_inline):\n  1202â†’                            print(\n  1203â†’                                f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\",\n  1204â†’                                file=sys.stderr,\n  1205â†’                            )\n  1206â†’\n  1207â†’                    except Exception as e:\n  1208â†’                        print(\n  1209â†’                            f\"Warning: Could not filter inline comments by commit time: {e}\",\n  1210â†’                            file=sys.stderr,\n  1211â†’                        )\n  1212â†’        elif args.since_commit:\n  1213â†’            # Filter reviews submitted after the specified commit\n  1214â†’            # This would require additional logic to compare timestamps with commit dates\n  1215â†’            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n  1216â†’            sys.exit(1)\n  1217â†’\n  1218â†’        if not coderabbit_reviews and not coderabbit_inline_comments:\n  1219â†’            print(\n  1220â†’                \"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr\n  1221â†’            )\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"b0d1e447-2c50-4e67-9ef6-f0562fff4b1b","timestamp":"2025-11-14T12:30:20.041Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","content":"        # Filter reviews based on options\n        if not args.all_reviews and not args.since_commit:\n            # Default: latest review with actual content\n            if coderabbit_reviews:\n                # Find the latest review with substantial content (not just continuation)\n                latest_review = None\n                for review in reversed(coderabbit_reviews):\n                    body = review.get(\"body\", \"\")\n                    # Skip continuation reviews that have minimal content\n                    if (\n                        len(body) > 100\n                        and \"Actionable comments posted:\" in body\n                        and not body.strip().startswith(\n                            \"**Review continued from previous batch\"\n                        )\n                    ):\n                        latest_review = review\n                        break\n\n                # Fallback to chronologically latest if no substantial review found\n                if not latest_review:\n                    latest_review = coderabbit_reviews[-1]\n\n                coderabbit_reviews = [latest_review]\n\n                # Filter inline comments to only those posted after the most recent commit\n                latest_commit_time = fetch_latest_commit_time(pr_url)\n                if latest_commit_time and coderabbit_inline_comments:\n                    from datetime import datetime\n\n                    try:\n                        # Parse ISO format timestamps (GitHub API standard)\n                        commit_dt = datetime.fromisoformat(\n                            latest_commit_time.replace(\"Z\", \"+00:00\")\n                        )\n\n                        filtered_inline = []\n                        for comment in coderabbit_inline_comments:\n                            comment_time = comment.get(\"created_at\", \"\")\n                            if comment_time:\n                                comment_dt = datetime.fromisoformat(\n                                    comment_time.replace(\"Z\", \"+00:00\")\n                                )\n                                # Include comments posted after the latest commit\n                                if comment_dt > commit_dt:\n                                    filtered_inline.append(comment)\n\n                        original_count = len(coderabbit_inline_comments)\n                        coderabbit_inline_comments = filtered_inline\n                        if original_count != len(filtered_inline):\n                            print(\n                                f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\",\n                                file=sys.stderr,\n                            )\n\n                    except Exception as e:\n                        print(\n                            f\"Warning: Could not filter inline comments by commit time: {e}\",\n                            file=sys.stderr,\n                        )\n        elif args.since_commit:\n            # Filter reviews submitted after the specified commit\n            # This would require additional logic to compare timestamps with commit dates\n            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n            sys.exit(1)\n\n        if not coderabbit_reviews and not coderabbit_inline_comments:\n            print(\n                \"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr\n            )","numLines":70,"startLine":1152,"totalLines":1255}}}
{"parentUuid":"b0d1e447-2c50-4e67-9ef6-f0562fff4b1b","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"fix/comment-attribution-and-pagination","agentId":"9fa17969","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01SDvcPqaBYZ6EV4b88fmqY7","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me look at the user's example format more closely. Looking at the original context, they mentioned the format includes indentation. Let me check if the parsing pattern matches that:\n\nBased on my investigation, I've identified **the root cause** of why \"Outside diff range comments\" are not appearing in the output. Here's what I found:\n\n## Current Behaviour Analysis\n\n### 1. **Section Parsing (lines 498-578 - `parse_review_sections()`)**\nâœ… **WORKING**: The function correctly:\n- Identifies \"Outside diff range comments\" sections (line 522)\n- Extracts the count from the summary text \n- Extracts the `<blockquote>` content\n- Stores it in `sections[\"outside_diff_comments\"]`\n\n### 2. **File-Level Comment Parsing (lines 581-674 - `parse_file_level_comments()`)**\nâŒ **PROBLEMATIC**: This is where the issue lies. The function expects a very specific HTML structure:\n\n```regex\n<summary>filename (count)</summary><blockquote>content</blockquote></details>\n```\n\nHowever, based on the user's example, the \"Outside diff range comments\" section appears to have a **different format** with plain text and indentation:\n\n```\napps/platform-frontend/src/app/api/organisation/[organisationId]/connections/plaid/save-connections/route.ts (1)\n\n    11-25: Type mismatch between request body types and validation schema.\n    \n    [description of the issue]\n```\n\nThe current parser expects:\n- Nested `<details>` elements with `<summary>` tags for each file\n- Line comments in format: `` `line-range`: **Title** ``\n\nBut the user's format shows:\n- File paths followed by count in parentheses (not wrapped in HTML tags)\n- Indented line ranges with colons (spaces before the line range)\n- No bold markdown for the title\n\n### 3. **Review Selection Logic (lines 1153-1175)**\nâš ï¸ **POTENTIAL ISSUE**: The default behaviour only processes:\n- The latest review containing `\"Actionable comments posted:\"` in the body\n- Reviews with `len(body) > 100`\n\n**Critical Question**: Does the \"Outside diff range comments\" section appear in:\na) The same review body as inline comments? \nb) A separate, newer review posted after the latest commit?\n\nIf it's in a newer review, the current filtering would exclude it.\n\n### 4. **Integration in `group_comments_by_file()` (lines 703-712)**\nâœ… **WORKING**: If comments are parsed successfully, they are correctly:\n- Added to `file_groups` by file path\n- Tagged with `source: \"outside_diff\"`\n- Included in final output\n\n## Root Cause\n\n**The parsing pattern in `parse_file_level_comments()` does not match the actual format of \"Outside diff range comments\".**\n\nSpecifically:\n1. Line 589: `file_pattern` expects `<summary>filename (count)</summary>` but the actual format may be plain text: `filename (count)`\n2. Line 607: `line_pattern` expects `` `line-range`: **title** `` but the actual format appears to be `    line-range: title` (with leading spaces, no backticks, no bold)\n\n## Verification Needed\n\nTo confirm the exact issue, I need to check:\n\n1. **What does the actual HTML structure look like in CodeRabbit's \"Outside diff range comments\" section?**\n   - Is it wrapped in nested `<details>` tags?\n   - Or is it plain text within a single `<blockquote>`?\n\n2. **Does the section use the same structure as \"Nitpick comments\"?**\n   - Looking at line 537, there's special handling for \"Nitpick comments\" that extracts raw content from the body instead of using BeautifulSoup\n   - But \"Outside diff range comments\" uses BeautifulSoup's blockquote content (line 527-530)\n\n## Recommended Fix Approach\n\nThe fix should:\n\n1. **Update the parsing pattern** to handle both HTML-wrapped and plain-text formats in outside diff comments\n2. **Add debug logging** to show what content is being extracted from the section\n3. **Consider extracting raw text** similar to how Nitpick comments are handled (lines 545-565)\n4. **Add a fallback pattern** that matches the indented plain-text format:\n   ```regex\n   ^\\s+(\\d+(?:-\\d+)?): (.+)$\n   ```\n\nWould you like me to:\n1. Create a test to reproduce the issue with sample HTML?\n2. Implement the fix to support the plain-text format?\n3. Add debug output to help diagnose the issue with real PR data?"}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1424,"cache_read_input_tokens":29085,"cache_creation":{"ephemeral_5m_input_tokens":1424,"ephemeral_1h_input_tokens":0},"output_tokens":1130,"service_tier":"standard"}},"requestId":"req_011CV7nSpbb2s1n3fqroRm1x","type":"assistant","uuid":"81ffe2c0-301d-4ed1-b26e-2d9ff0623469","timestamp":"2025-11-14T12:30:51.701Z"}
