{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","type":"user","message":{"role":"user","content":"Examine the extract-coderabbit-feedback.py file to understand:\n1. Where HTML cleaning and filtering currently happens\n2. How inline comments and review body content are processed\n3. The best location to add <details> tag filtering to remove verbose analysis chains and script outputs\n4. Whether filtering should happen at parse time or format time\n\nLook specifically at:\n- clean_html_artifacts function\n- parse_file_level_comments function\n- group_comments_by_file function\n- Any other functions that process comment bodies\n\nReturn a summary of:\n- Current HTML cleaning approach\n- Best place(s) to add <details> filtering\n- Whether we need one filter or multiple filters at different stages"},"uuid":"2c9d3f93-a884-46a3-b0bd-ee50e8b60bc3","timestamp":"2025-11-13T11:23:43.887Z"}
{"parentUuid":"2c9d3f93-a884-46a3-b0bd-ee50e8b60bc3","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PAyVPVgcb9AudFuqgty8vo","type":"message","role":"assistant","content":[{"type":"text","text":"I'll examine the extract-coderabbit-feedback.py file to understand the current HTML processing and identify the best place to add <details> tag filtering."}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8138,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":8138,"ephemeral_1h_input_tokens":0},"output_tokens":135,"service_tier":"standard"}},"requestId":"req_011CV5oZLBbeK6TMXFY1gVyU","type":"assistant","uuid":"f42473ed-9514-42a8-a3e0-c64bdbde318a","timestamp":"2025-11-13T11:23:45.548Z"}
{"parentUuid":"f42473ed-9514-42a8-a3e0-c64bdbde318a","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PAyVPVgcb9AudFuqgty8vo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DN7hAYrmeNA4RwzZeZ4UP4","name":"Glob","input":{"pattern":"**/extract-coderabbit-feedback.py"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8138,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":8138,"ephemeral_1h_input_tokens":0},"output_tokens":135,"service_tier":"standard"}},"requestId":"req_011CV5oZLBbeK6TMXFY1gVyU","type":"assistant","uuid":"23a64aed-324b-4939-badd-eb5b23bfd7a9","timestamp":"2025-11-13T11:23:45.657Z"}
{"parentUuid":"23a64aed-324b-4939-badd-eb5b23bfd7a9","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PAyVPVgcb9AudFuqgty8vo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019LhwJpZFmUThSkpFBqQv6t","name":"Glob","input":{"pattern":"**/*.py"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8138,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":8138,"ephemeral_1h_input_tokens":0},"output_tokens":135,"service_tier":"standard"}},"requestId":"req_011CV5oZLBbeK6TMXFY1gVyU","type":"assistant","uuid":"10e70923-fe48-4c47-b6a9-fafd37e15281","timestamp":"2025-11-13T11:23:45.775Z"}
{"parentUuid":"10e70923-fe48-4c47-b6a9-fafd37e15281","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DN7hAYrmeNA4RwzZeZ4UP4","type":"tool_result","content":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py"}]},"uuid":"732b5554-67fa-4b4c-8a26-4af481772018","timestamp":"2025-11-13T11:23:45.823Z","toolUseResult":{"filenames":["/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py"],"durationMs":7,"numFiles":1,"truncated":false}}
{"parentUuid":"732b5554-67fa-4b4c-8a26-4af481772018","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019LhwJpZFmUThSkpFBqQv6t","type":"tool_result","content":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py"}]},"uuid":"29c16949-0c09-4b9e-80cb-3ed74f2238d7","timestamp":"2025-11-13T11:23:45.825Z","toolUseResult":{"filenames":["/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py"],"durationMs":9,"numFiles":1,"truncated":false}}
{"parentUuid":"29c16949-0c09-4b9e-80cb-3ed74f2238d7","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01Lx1v1tzRezgPbSXiHNVV9G","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me read the extract-coderabbit-feedback.py file:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":451,"cache_read_input_tokens":8138,"cache_creation":{"ephemeral_5m_input_tokens":451,"ephemeral_1h_input_tokens":0},"output_tokens":97,"service_tier":"standard"}},"requestId":"req_011CV5oZUTBNWmE1csphjo3y","type":"assistant","uuid":"b152fc4c-2f39-4dbf-9426-b33efa37a263","timestamp":"2025-11-13T11:23:47.087Z"}
{"parentUuid":"b152fc4c-2f39-4dbf-9426-b33efa37a263","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01Lx1v1tzRezgPbSXiHNVV9G","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EowWxqVJ44wxJdn2uaWzC9","name":"Read","input":{"file_path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":451,"cache_read_input_tokens":8138,"cache_creation":{"ephemeral_5m_input_tokens":451,"ephemeral_1h_input_tokens":0},"output_tokens":97,"service_tier":"standard"}},"requestId":"req_011CV5oZUTBNWmE1csphjo3y","type":"assistant","uuid":"15d2268f-ac84-4e5e-bd7c-27606a20c898","timestamp":"2025-11-13T11:23:47.270Z"}
{"parentUuid":"15d2268f-ac84-4e5e-bd7c-27606a20c898","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EowWxqVJ44wxJdn2uaWzC9","type":"tool_result","content":"     1â†’#!/usr/bin/env python3\n     2â†’\"\"\"\n     3â†’ABOUTME: Extract CodeRabbit review feedback from GitHub PRs for LLM consumption\n     4â†’ABOUTME: Uses gh CLI to fetch PR comments and formats them as plain text\n     5â†’\"\"\"\n     6â†’\n     7â†’__version__ = \"1.0.0\"\n     8â†’__author__ = \"Jesse Vincent\"\n     9â†’__license__ = \"MIT\"\n    10â†’import argparse\n    11â†’import json\n    12â†’import os\n    13â†’import re\n    14â†’import subprocess\n    15â†’import sys\n    16â†’from pathlib import Path\n    17â†’from urllib.parse import urlparse\n    18â†’from typing import List, Dict, Any, Tuple, Optional\n    19â†’from bs4 import BeautifulSoup\n    20â†’import html\n    21â†’\n    22â†’\n    23â†’def load_preamble_config() -> Optional[str]:\n    24â†’    \"\"\"Load custom preamble from dotfile.\"\"\"\n    25â†’    config_path = Path.home() / '.coderabbit-extractor'\n    26â†’    \n    27â†’    if config_path.exists():\n    28â†’        try:\n    29â†’            return config_path.read_text(encoding='utf-8').strip()\n    30â†’        except Exception as e:\n    31â†’            print(f\"Warning: Could not read config file {config_path}: {e}\", file=sys.stderr)\n    32â†’            return None\n    33â†’    \n    34â†’    return None\n    35â†’\n    36â†’\n    37â†’def parse_pr_input(input_str: str) -> str:\n    38â†’    \"\"\"Parse PR URL or owner/repo/number format into a full GitHub PR URL.\"\"\"\n    39â†’    input_str = input_str.strip()\n    40â†’    \n    41â†’    # If it's already a full URL, return as-is\n    42â†’    if input_str.startswith('https://github.com/'):\n    43â†’        return input_str\n    44â†’    \n    45â†’    # Parse owner/repo/number format (e.g., \"obra/lace/278\")\n    46â†’    parts = input_str.split('/')\n    47â†’    if len(parts) == 3:\n    48â†’        owner, repo, pr_num = parts\n    49â†’        return f\"https://github.com/{owner}/{repo}/pull/{pr_num}\"\n    50â†’    \n    51â†’    raise ValueError(f\"Invalid PR format: {input_str}. Use 'owner/repo/number' or full URL\")\n    52â†’\n    53â†’\n    54â†’def extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\n    55â†’    \"\"\"Extract owner, repo, and PR number from GitHub PR URL.\"\"\"\n    56â†’    # Parse URL like https://github.com/owner/repo/pull/123\n    57â†’    if 'github.com' in pr_url:\n    58â†’        parts = pr_url.replace('https://github.com/', '').split('/')\n    59â†’        if len(parts) >= 4 and parts[2] == 'pull':\n    60â†’            owner, repo, _, pr_number = parts[:4]\n    61â†’            try:\n    62â†’                return owner, repo, int(pr_number)\n    63â†’            except ValueError:\n    64â†’                raise ValueError(f\"Invalid PR number: {pr_number}\")\n    65â†’    \n    66â†’    raise ValueError(f\"Could not parse PR URL: {pr_url}\")\n    67â†’\n    68â†’\n    69â†’def fetch_pr_reviews(pr_url: str) -> List[Dict[str, Any]]:\n    70â†’    \"\"\"Fetch PR reviews using gh CLI.\"\"\"\n    71â†’    try:\n    72â†’        result = subprocess.run(\n    73â†’            ['gh', 'pr', 'view', pr_url, '--json', 'reviews'],\n    74â†’            capture_output=True,\n    75â†’            text=True,\n    76â†’            check=True\n    77â†’        )\n    78â†’        data = json.loads(result.stdout)\n    79â†’        return data.get('reviews', [])\n    80â†’    except subprocess.CalledProcessError as e:\n    81â†’        print(f\"Error fetching PR: {e.stderr}\", file=sys.stderr)\n    82â†’        sys.exit(1)\n    83â†’    except json.JSONDecodeError as e:\n    84â†’        print(f\"Error parsing JSON response: {e}\", file=sys.stderr)\n    85â†’        sys.exit(1)\n    86â†’\n    87â†’\n    88â†’def fetch_latest_commit_time(pr_url: str) -> Optional[str]:\n    89â†’    \"\"\"Fetch the timestamp of the most recent commit in the PR.\"\"\"\n    90â†’    try:\n    91â†’        # Extract owner/repo/number from URL\n    92â†’        owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n    93â†’        \n    94â†’        # Get commits from the PR\n    95â†’        result = subprocess.run(\n    96â†’            ['gh', 'api', f'repos/{owner}/{repo}/pulls/{pr_number}/commits'],\n    97â†’            capture_output=True,\n    98â†’            text=True,\n    99â†’            check=True\n   100â†’        )\n   101â†’        commits = json.loads(result.stdout)\n   102â†’        \n   103â†’        if commits:\n   104â†’            # Get the most recent commit (last in the list)\n   105â†’            latest_commit = commits[-1]\n   106â†’            return latest_commit['commit']['committer']['date']\n   107â†’        \n   108â†’        return None\n   109â†’    except Exception as e:\n   110â†’        print(f\"Warning: Could not fetch latest commit time: {e}\", file=sys.stderr)\n   111â†’        return None\n   112â†’\n   113â†’\n   114â†’def fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n   115â†’    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n   116â†’    try:\n   117â†’        # Extract owner/repo/number from URL\n   118â†’        if 'github.com' in pr_url:\n   119â†’            parts = pr_url.replace('https://github.com/', '').split('/')\n   120â†’            if len(parts) >= 4 and parts[2] == 'pull':\n   121â†’                owner, repo, _, pr_number = parts[:4]\n   122â†’                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n   123â†’            else:\n   124â†’                raise ValueError(\"Invalid GitHub PR URL format\")\n   125â†’        else:\n   126â†’            raise ValueError(\"URL must be a GitHub PR URL\")\n   127â†’        \n   128â†’        result = subprocess.run(\n   129â†’            ['gh', 'api', api_path],\n   130â†’            capture_output=True,\n   131â†’            text=True,\n   132â†’            check=True\n   133â†’        )\n   134â†’        data = json.loads(result.stdout)\n   135â†’        return data if isinstance(data, list) else []\n   136â†’    except subprocess.CalledProcessError as e:\n   137â†’        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n   138â†’        return []\n   139â†’    except json.JSONDecodeError as e:\n   140â†’        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n   141â†’        return []\n   142â†’        return []\n   143â†’\n   144â†’\n   145â†’def fetch_review_threads_graphql(owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:\n   146â†’    \"\"\"Fetch review threads with resolution status using GitHub GraphQL API.\"\"\"\n   147â†’    query = '''\n   148â†’    query($owner: String!, $repo: String!, $prNumber: Int!) {\n   149â†’      repository(owner: $owner, name: $repo) {\n   150â†’        pullRequest(number: $prNumber) {\n   151â†’          reviewThreads(first: 100) {\n   152â†’            nodes {\n   153â†’              id\n   154â†’              isResolved\n   155â†’              resolvedBy {\n   156â†’                login\n   157â†’              }\n   158â†’              comments(first: 1) {\n   159â†’                nodes {\n   160â†’                  body\n   161â†’                  createdAt\n   162â†’                  author {\n   163â†’                    login\n   164â†’                  }\n   165â†’                }\n   166â†’              }\n   167â†’            }\n   168â†’          }\n   169â†’        }\n   170â†’      }\n   171â†’    }\n   172â†’    '''\n   173â†’    \n   174â†’    variables = {\n   175â†’        \"owner\": owner,\n   176â†’        \"repo\": repo, \n   177â†’        \"prNumber\": pr_number\n   178â†’    }\n   179â†’    \n   180â†’    try:\n   181â†’        # Use gh api to make GraphQL request\n   182â†’        import json\n   183â†’        result = subprocess.run([\n   184â†’            'gh', 'api', 'graphql', \n   185â†’            '-F', f'owner={owner}',\n   186â†’            '-F', f'repo={repo}', \n   187â†’            '-F', f'prNumber={pr_number}',\n   188â†’            '-f', f'query={query}'\n   189â†’        ], capture_output=True, text=True, check=True)\n   190â†’        \n   191â†’        data = json.loads(result.stdout)\n   192â†’        \n   193â†’        # Extract review threads\n   194â†’        threads = data.get('data', {}).get('repository', {}).get('pullRequest', {}).get('reviewThreads', {}).get('nodes', [])\n   195â†’        \n   196â†’        # Filter to only CodeRabbit threads and return as list of dicts\n   197â†’        coderabbit_threads = []\n   198â†’        for thread in threads:\n   199â†’            if thread and thread.get('comments', {}).get('nodes', []):\n   200â†’                comment = thread['comments']['nodes'][0]\n   201â†’                author_login = comment.get('author', {}).get('login', '')\n   202â†’                # Check for both coderabbitai and coderabbitai[bot] formats\n   203â†’                if author_login in ['coderabbitai', 'coderabbitai[bot]']:\n   204â†’                    coderabbit_threads.append({\n   205â†’                        'id': thread['id'],\n   206â†’                        'isResolved': thread.get('isResolved', False),\n   207â†’                        'resolvedBy': thread.get('resolvedBy', {}).get('login') if thread.get('resolvedBy') else None,\n   208â†’                        'body': comment.get('body', ''),\n   209â†’                        'createdAt': comment.get('createdAt', ''),\n   210â†’                        'author': author_login\n   211â†’                    })\n   212â†’        \n   213â†’        return coderabbit_threads\n   214â†’        \n   215â†’    except subprocess.CalledProcessError as e:\n   216â†’        print(f\"Warning: Could not fetch review threads via GraphQL: {e.stderr}\", file=sys.stderr)\n   217â†’        return []\n   218â†’    except json.JSONDecodeError as e:\n   219â†’        print(f\"Warning: Could not parse GraphQL response: {e}\", file=sys.stderr)\n   220â†’        return []\n   221â†’    except Exception as e:\n   222â†’        print(f\"Warning: Error processing GraphQL data: {e}\", file=sys.stderr)\n   223â†’        return []\n   224â†’\n   225â†’\n   226â†’def extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   227â†’    \"\"\"Filter reviews to only CodeRabbit ones.\"\"\"\n   228â†’    return [\n   229â†’        review for review in reviews\n   230â†’        if review.get('author', {}).get('login') == 'coderabbitai'\n   231â†’    ]\n   232â†’\n   233â†’\n   234â†’def filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   235â†’    \"\"\"Filter out comments that have been marked as resolved.\"\"\"\n   236â†’    return [\n   237â†’        comment for comment in comments\n   238â†’        if not is_comment_resolved(comment.get('body', ''))\n   239â†’    ]\n   240â†’\n   241â†’\n   242â†’def filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   243â†’    \"\"\"Filter out review threads that have been marked as resolved via GitHub's native resolution.\"\"\"\n   244â†’    return [\n   245â†’        thread for thread in threads\n   246â†’        if not thread.get('isResolved', False)\n   247â†’    ]\n   248â†’\n   249â†’\n   250â†’def filter_resolved_review_content(review_body: str) -> str:\n   251â†’    \"\"\"Filter out resolved suggestions from review body content.\"\"\"\n   252â†’    # Look for resolved comment patterns and remove them\n   253â†’    # This handles CodeRabbit's text-based resolution markers\n   254â†’    \n   255â†’    # Pattern to match resolved comments within the review body\n   256â†’    # Look for sections that contain resolution markers\n   257â†’    resolved_patterns = [\n   258â†’        # Match individual comment sections that are resolved\n   259â†’        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n   260â†’        # Match line-specific resolved comments\n   261â†’        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n   262â†’    ]\n   263â†’    \n   264â†’    filtered_body = review_body\n   265â†’    for pattern in resolved_patterns:\n   266â†’        filtered_body = re.sub(pattern, '', filtered_body, flags=re.DOTALL | re.IGNORECASE)\n   267â†’    \n   268â†’    # Also remove any standalone resolution lines\n   269â†’    filtered_body = re.sub(r'^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?', '', filtered_body, flags=re.MULTILINE | re.IGNORECASE)\n   270â†’    \n   271â†’    return filtered_body.strip()\n   272â†’\n   273â†’\n   274â†’def is_comment_resolved(comment_body: str) -> bool:\n   275â†’    \"\"\"Check if a comment has been marked as resolved/addressed.\"\"\"\n   276â†’    # Look for resolution indicators in the comment body\n   277â†’    resolution_patterns = [\n   278â†’        r'âœ…\\s*Addressed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   279â†’        r'âœ…\\s*Resolved\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   280â†’        r'âœ…\\s*Fixed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   281â†’        r'âœ…\\s*Completed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n   282â†’        r'\\[x\\]',  # Checkbox checked\n   283â†’        r'\\[X\\]',  # Checkbox checked\n   284â†’    ]\n   285â†’    \n   286â†’    for pattern in resolution_patterns:\n   287â†’        if re.search(pattern, comment_body, re.IGNORECASE):\n   288â†’            return True\n   289â†’    \n   290â†’    return False\n   291â†’\n   292â†’\n   293â†’def extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n   294â†’    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n   295â†’    return [\n   296â†’        comment for comment in comments\n   297â†’        if comment.get('user', {}).get('login') == 'coderabbitai[bot]'\n   298â†’    ]\n   299â†’\n   300â†’\n   301â†’def parse_review_type(body: str) -> str:\n   302â†’    \"\"\"Determine the type of CodeRabbit review.\"\"\"\n   303â†’    if 'Actionable comments posted:' in body:\n   304â†’        return 'ACTIONABLE_REVIEW'\n   305â†’    elif 'walkthrough_start' in body and 'walkthrough_end' in body:\n   306â†’        return 'WALKTHROUGH'\n   307â†’    elif '<!-- This is an auto-generated comment: summarize by coderabbit.ai -->' in body:\n   308â†’        return 'SUMMARY'\n   309â†’    else:\n   310â†’        return 'OTHER'\n   311â†’\n   312â†’\n   313â†’def clean_html_artifacts(text: str) -> str:\n   314â†’    \"\"\"Remove HTML comments and artifacts from text.\"\"\"\n   315â†’    # Remove HTML comments\n   316â†’    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n   317â†’    \n   318â†’    # Remove CodeRabbit warning blocks\n   319â†’    text = re.sub(r'>\\s*â€¼ï¸\\s*\\*\\*IMPORTANT\\*\\*.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n   320â†’    \n   321â†’    # Remove other quote block artifacts  \n   322â†’    text = re.sub(r'>\\s*Carefully review.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n   323â†’    \n   324â†’    # Remove empty lines that result from artifact removal\n   325â†’    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n   326â†’    \n   327â†’    return text.strip()\n   328â†’\n   329â†’\n   330â†’def clean_diff_artifacts(diff: str) -> str:\n   331â†’    \"\"\"Clean up diff artifacts and markers.\"\"\"\n   332â†’    if not diff:\n   333â†’        return diff\n   334â†’        \n   335â†’    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n   336â†’    diff = re.sub(r'^@@.*?@@.*?\\n', '', diff, flags=re.MULTILINE)  # Full hunk headers\n   337â†’    diff = re.sub(r'^\\s*@@\\s*\\n', '', diff, flags=re.MULTILINE)    # Lone @@ lines\n   338â†’    \n   339â†’    # Remove empty lines at start/end\n   340â†’    diff = diff.strip()\n   341â†’    \n   342â†’    return diff\n   343â†’\n   344â†’\n   345â†’def extract_prompt_for_ai_agents(body: str) -> List[str]:\n   346â†’    \"\"\"Extract 'Prompt for AI Agents' sections from review body.\"\"\"\n   347â†’    prompts = []\n   348â†’    \n   349â†’    # Pattern to find ğŸ¤– Prompt for AI Agents sections\n   350â†’    # These appear to be in <summary> tags followed by clipboard content\n   351â†’    pattern = r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*<div[^>]*data-snippet-clipboard-copy-content=\"([^\"]*)\"'\n   352â†’    matches = re.findall(pattern, body, re.DOTALL)\n   353â†’    \n   354â†’    for match in matches:\n   355â†’        # Decode HTML entities\n   356â†’        clean_text = (match\n   357â†’                     .replace('&lt;', '<')\n   358â†’                     .replace('&gt;', '>')\n   359â†’                     .replace('&amp;', '&')\n   360â†’                     .replace('&quot;', '\"')\n   361â†’                     .replace('\\\\n', '\\n')\n   362â†’                     .strip())\n   363â†’        if clean_text:\n   364â†’            prompts.append(clean_text)\n   365â†’    \n   366â†’    return prompts\n   367â†’\n   368â†’\n   369â†’# Removed unused functions - now using parse_review_sections instead\n   370â†’\n   371â†’\n   372â†’def parse_review_sections(body: str) -> Dict[str, Any]:\n   373â†’    \"\"\"Parse different sections from CodeRabbit review body using HTML parsing.\"\"\"\n   374â†’    sections = {}\n   375â†’    \n   376â†’    # Extract actionable comments count from text\n   377â†’    actionable_match = re.search(r'Actionable comments posted:\\s*(\\d+)', body)\n   378â†’    if actionable_match:\n   379â†’        sections['actionable_count'] = int(actionable_match.group(1))\n   380â†’    \n   381â†’    # Parse HTML structure - but be careful not to mangle code content\n   382â†’    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n   383â†’    soup = BeautifulSoup(body, 'html.parser')\n   384â†’    \n   385â†’    # Find sections by looking for <details> elements with specific summary text\n   386â†’    details_elements = soup.find_all('details')\n   387â†’    \n   388â†’    for details in details_elements:\n   389â†’        summary = details.find('summary')\n   390â†’        if not summary:\n   391â†’            continue\n   392â†’            \n   393â†’        summary_text = summary.get_text(strip=True)\n   394â†’        \n   395â†’        # Check for outside diff range comments (avoid emoji dependency)\n   396â†’        if 'Outside diff range comments' in summary_text:\n   397â†’            # Extract count from summary text\n   398â†’            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n   399â†’            if count_match:\n   400â†’                count = int(count_match.group(1))\n   401â†’                blockquote = details.find('blockquote')\n   402â†’                if blockquote:\n   403â†’                    # Get all content inside the blockquote\n   404â†’                    content = str(blockquote)\n   405â†’                    sections['outside_diff_comments'] = {'count': count, 'content': content}\n   406â†’        \n   407â†’        # Check for nitpick comments or duplicate comments  \n   408â†’        elif 'Nitpick comments' in summary_text or 'Duplicate comments' in summary_text:\n   409â†’            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n   410â†’            if count_match:\n   411â†’                count = int(count_match.group(1))\n   412â†’                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n   413â†’                # extract the raw content from the original body text\n   414â†’                \n   415â†’                # Find the position of this details section in the original body\n   416â†’                details_start = body.find('Nitpick comments')\n   417â†’                if details_start != -1:\n   418â†’                    # Find where this section ends (next major section or end)\n   419â†’                    section_markers = ['ğŸ“œ Review details', 'ğŸ”‡ Additional comments', '</details>\\n\\n</details>']\n   420â†’                    section_end = len(body)\n   421â†’                    \n   422â†’                    for marker in section_markers:\n   423â†’                        marker_pos = body.find(marker, details_start)\n   424â†’                        if marker_pos != -1:\n   425â†’                            section_end = min(section_end, marker_pos)\n   426â†’                    \n   427â†’                    # Extract the raw text content\n   428â†’                    raw_content = body[details_start:section_end]\n   429â†’                    sections['nitpick_comments'] = {'count': count, 'content': raw_content}\n   430â†’        \n   431â†’        # Check for review details (usually at the end)\n   432â†’        elif 'Review details' in summary_text:\n   433â†’            # This section contains metadata, we might want it for context\n   434â†’            blockquote = details.find('blockquote')\n   435â†’            if blockquote:\n   436â†’                content = str(blockquote)\n   437â†’                sections['review_details'] = {'content': content}\n   438â†’    \n   439â†’    # Extract AI prompts (look for specific patterns)\n   440â†’    sections['ai_prompts'] = extract_prompt_for_ai_agents(body)\n   441â†’    \n   442â†’    return sections\n   443â†’\n   444â†’\n   445â†’def parse_file_level_comments(content: str, debug: bool = False) -> List[Dict[str, Any]]:\n   446â†’    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n   447â†’    comments = []\n   448â†’    \n   449â†’    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n   450â†’    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n   451â†’    file_pattern = r'<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>'\n   452â†’    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n   453â†’    \n   454â†’    if debug:\n   455â†’        print(f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\", file=sys.stderr)\n   456â†’    \n   457â†’    for file_match in file_matches:\n   458â†’        file_path = file_match.group(1).strip()\n   459â†’        comment_count = int(file_match.group(2))\n   460â†’        file_content = file_match.group(3)  # Raw blockquote content, not parsed by BeautifulSoup\n   461â†’        \n   462â†’        # Now parse line comments within this file content\n   463â†’        # Find line comment patterns: `16-24`: **Title**\n   464â†’        line_pattern = r'`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)'\n   465â†’        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n   466â†’        \n   467â†’        if debug:\n   468â†’            print(f\"DEBUG: File {file_path} has {len(line_matches)} line comments\", file=sys.stderr)\n   469â†’        \n   470â†’        for line_match in line_matches:\n   471â†’            line_range = line_match.group(1).strip()\n   472â†’            title = line_match.group(2).strip() \n   473â†’            content_block = line_match.group(3).strip()\n   474â†’            \n   475â†’            # Check if this looks like a line reference\n   476â†’            if re.match(r'^[\\d\\-,\\s]+$', line_range):\n   477â†’                # Extract description (everything before first code block)\n   478â†’                description = content_block\n   479â†’                code_diff = \"\"\n   480â†’                \n   481â†’                code_start = content_block.find('```')\n   482â†’                if code_start != -1:\n   483â†’                    description = content_block[:code_start].strip()\n   484â†’                    \n   485â†’                    # Extract code diff from raw content (no BeautifulSoup mangling)\n   486â†’                    diff_match = re.search(r'```diff\\n(.*?)\\n```', content_block, re.DOTALL)\n   487â†’                    if diff_match:\n   488â†’                        code_diff = diff_match.group(1).strip()\n   489â†’                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n   490â†’                        code_diff = html.unescape(code_diff)\n   491â†’                        code_diff = clean_html_artifacts(code_diff)  # Remove any HTML comments in diffs\n   492â†’                        code_diff = clean_diff_artifacts(code_diff)  # Clean up @@ markers and artifacts\n   493â†’                \n   494â†’                # Clean up description\n   495â†’                description = clean_html_artifacts(description)  # Remove HTML comments first\n   496â†’                description_lines = []\n   497â†’                for line in description.split('\\n'):\n   498â†’                    line = line.strip()\n   499â†’                    if line and not line.startswith('Also applies to:') and line != '---':\n   500â†’                        description_lines.append(line)\n   501â†’                description = ' '.join(description_lines)\n   502â†’                \n   503â†’                comments.append({\n   504â†’                    'file': file_path,\n   505â†’                    'lines': line_range,\n   506â†’                    'title': title,\n   507â†’                    'description': description,\n   508â†’                    'code_diff': code_diff\n   509â†’                })\n   510â†’    \n   511â†’    return comments\n   512â†’\n   513â†’\n   514â†’def group_comments_by_file(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, include_resolved: bool = True) -> Dict[str, List[Dict[str, Any]]]:\n   515â†’    \"\"\"Group all comments by file for better organization.\"\"\"\n   516â†’    file_groups = {}\n   517â†’    \n   518â†’    # Process review body comments\n   519â†’    for review in coderabbit_reviews:\n   520â†’        review_body = review['body']\n   521â†’        \n   522â†’        # Filter out resolved content from review body if requested\n   523â†’        if not include_resolved:\n   524â†’            original_body = review_body\n   525â†’            review_body = filter_resolved_review_content(review_body)\n   526â†’            if debug and len(original_body) != len(review_body):\n   527â†’                print(f\"DEBUG: Filtered resolved content from review body (length: {len(original_body)} -> {len(review_body)})\", file=sys.stderr)\n   528â†’        \n   529â†’        sections = parse_review_sections(review_body)\n   530â†’        \n   531â†’        # Add outside diff comments\n   532â†’        if 'outside_diff_comments' in sections:\n   533â†’            outside_comments = parse_file_level_comments(sections['outside_diff_comments']['content'], debug)\n   534â†’            for comment in outside_comments:\n   535â†’                file_path = comment['file']\n   536â†’                if file_path not in file_groups:\n   537â†’                    file_groups[file_path] = []\n   538â†’                comment['source'] = 'outside_diff'\n   539â†’                file_groups[file_path].append(comment)\n   540â†’        \n   541â†’        # Add nitpick comments\n   542â†’        if 'nitpick_comments' in sections:\n   543â†’            nitpick_comments = parse_file_level_comments(sections['nitpick_comments']['content'], debug)\n   544â†’            for comment in nitpick_comments:\n   545â†’                file_path = comment['file']\n   546â†’                if file_path not in file_groups:\n   547â†’                    file_groups[file_path] = []\n   548â†’                comment['source'] = 'nitpick'\n   549â†’                file_groups[file_path].append(comment)\n   550â†’    \n   551â†’    # Process inline comments\n   552â†’    if inline_comments:\n   553â†’        for comment in inline_comments:\n   554â†’            file_path = comment.get('path', 'unknown')\n   555â†’            line_num = comment.get('original_line', 'unknown')\n   556â†’            \n   557â†’            if file_path not in file_groups:\n   558â†’                file_groups[file_path] = []\n   559â†’                \n   560â†’            # Convert inline comment to standard format\n   561â†’            body = comment.get('body', '')\n   562â†’            comment_type = 'ğŸ› ï¸ Refactor Suggestion' if 'ğŸ› ï¸ Refactor suggestion' in body else 'Comment'\n   563â†’            \n   564â†’            # Extract title from body\n   565â†’            title_match = re.search(r'\\*\\*(.*?)\\*\\*', body)\n   566â†’            title = title_match.group(1).strip() if title_match else \"Suggestion\"\n   567â†’            \n   568â†’            # Extract description and diff\n   569â†’            if '<details>' in body and 'Prompt for AI Agents' in body:\n   570â†’                main_content = body.split('<details>')[0].strip()\n   571â†’            else:\n   572â†’                main_content = body\n   573â†’                \n   574â†’            main_content = main_content.replace('_ğŸ› ï¸ Refactor suggestion_', '').strip()\n   575â†’            main_content = clean_html_artifacts(main_content)\n   576â†’            \n   577â†’            # Extract AI prompt\n   578â†’            ai_prompt = \"\"\n   579â†’            ai_prompt_match = re.search(r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*\\n\\n```\\n(.*?)\\n```', body, re.DOTALL)\n   580â†’            if ai_prompt_match:\n   581â†’                ai_prompt = ai_prompt_match.group(1).strip()\n   582â†’            \n   583â†’            file_groups[file_path].append({\n   584â†’                'lines': str(line_num),\n   585â†’                'title': title,\n   586â†’                'description': main_content,\n   587â†’                'code_diff': '',  # Inline comments don't have separate diffs\n   588â†’                'ai_prompt': ai_prompt,\n   589â†’                'source': 'inline',\n   590â†’                'comment_type': comment_type\n   591â†’            })\n   592â†’    \n   593â†’    return file_groups\n   594â†’\n   595â†’\n   596â†’def format_for_llm(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, preamble: Optional[str] = None, include_resolved: bool = True) -> str:\n   597â†’    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n   598â†’    output = []\n   599â†’    \n   600â†’    # Add custom preamble if provided\n   601â†’    if preamble:\n   602â†’        output.append(preamble)\n   603â†’        output.append(\"\")\n   604â†’        output.append(\"=\" * 60)\n   605â†’        output.append(\"\")\n   606â†’    \n   607â†’    output.append(\"# CodeRabbit Review Feedback\")\n   608â†’    output.append(\"=\" * 40)\n   609â†’    output.append(\"\")\n   610â†’    \n   611â†’    # Group all comments by file\n   612â†’    file_groups = group_comments_by_file(coderabbit_reviews, inline_comments, debug, include_resolved)\n   613â†’    \n   614â†’    if not file_groups:\n   615â†’        output.append(\"No actionable comments found.\")\n   616â†’        return \"\\n\".join(output)\n   617â†’    \n   618â†’    # Add summary\n   619â†’    total_comments = sum(len(comments) for comments in file_groups.values())\n   620â†’    output.append(f\"**Total files with feedback:** {len(file_groups)}\")\n   621â†’    output.append(f\"**Total comments:** {total_comments}\")\n   622â†’    output.append(\"\")\n   623â†’    \n   624â†’    # Process each file\n   625â†’    for file_path in sorted(file_groups.keys()):\n   626â†’        comments = file_groups[file_path]\n   627â†’        \n   628â†’        output.append(f\"## {file_path}\")\n   629â†’        output.append(f\"**{len(comments)} suggestion(s)**\")\n   630â†’        output.append(\"\")\n   631â†’        \n   632â†’        # Sort comments by priority: AI prompts first, then by line number\n   633â†’        def get_sort_key(comment):\n   634â†’            # Priority 1: Comments with AI prompts (highest priority)\n   635â†’            has_ai_prompt = bool(comment.get('ai_prompt'))\n   636â†’            \n   637â†’            # Priority 2: Line number (for logical code flow)\n   638â†’            line_num = 999\n   639â†’            if comment['lines'].replace('-', '').isdigit():\n   640â†’                line_num = int(comment['lines'].split('-')[0])\n   641â†’                \n   642â†’            # Return tuple: (ai_prompt_priority, line_number)\n   643â†’            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n   644â†’            return (0 if has_ai_prompt else 1, line_num)\n   645â†’        \n   646â†’        comments_sorted = sorted(comments, key=get_sort_key)\n   647â†’        \n   648â†’        for i, comment in enumerate(comments_sorted, 1):\n   649â†’            # Add AI prompt indicator to title for high-priority items\n   650â†’            title = comment['title']\n   651â†’            if comment.get('ai_prompt'):\n   652â†’                title = f\"ğŸ¤– {title}\"\n   653â†’                \n   654â†’            output.append(f\"### {i}. Lines {comment['lines']}: {title}\")\n   655â†’            output.append(\"\")\n   656â†’            \n   657â†’            # Show AI prompt, or fallback to description + diff\n   658â†’            has_prompt = bool(comment.get('ai_prompt'))\n   659â†’            has_diff = bool(comment.get('code_diff'))\n   660â†’            \n   661â†’            if debug:\n   662â†’                output.append(f\"<!-- DEBUG: has_prompt={has_prompt}, has_diff={has_diff}, source={comment.get('source')} -->\")\n   663â†’            \n   664â†’            if has_prompt:\n   665â†’                if debug:\n   666â†’                    output.append(\"<!-- DEBUG: Showing prompt only, skipping description and diff -->\")\n   667â†’                # AI prompt contains better explanation than description + diff\n   668â†’                output.append(\"**Proposed prompt:**\")\n   669â†’                output.append(\"```\")\n   670â†’                output.append(comment['ai_prompt'])\n   671â†’                output.append(\"```\")\n   672â†’                output.append(\"\")\n   673â†’            else:\n   674â†’                if debug:\n   675â†’                    output.append(\"<!-- DEBUG: No prompt, showing description and diff -->\")\n   676â†’                \n   677â†’                # Show description only if no AI prompt available\n   678â†’                if comment['description'] and len(comment['description']) > 10:\n   679â†’                    desc = comment['description']\n   680â†’                    desc = re.sub(r'^_.*?_\\s*', '', desc)  # Remove _âš ï¸ Potential issue_ etc\n   681â†’                    desc = re.sub(r'\\*\\*.*?\\*\\*\\s*', '', desc, count=1)  # Remove first **title** if duplicated\n   682â†’                    desc = re.sub(r'Apply this diff:\\s*', '', desc)\n   683â†’                    desc = desc.strip()\n   684â†’                    \n   685â†’                    if desc and desc != title:\n   686â†’                        output.append(desc)\n   687â†’                        output.append(\"\")\n   688â†’                \n   689â†’                # Show diff only if no AI prompt available\n   690â†’                if has_diff:\n   691â†’                    output.append(\"```diff\")\n   692â†’                    output.append(comment['code_diff'])\n   693â†’                    output.append(\"```\")\n   694â†’                    output.append(\"\")\n   695â†’            \n   696â†’            if i < len(comments_sorted):\n   697â†’                output.append(\"-\" * 30)\n   698â†’                output.append(\"\")\n   699â†’        \n   700â†’        output.append(\"=\" * 60)\n   701â†’        output.append(\"\")\n   702â†’    \n   703â†’    return \"\\n\".join(output)\n   704â†’\n   705â†’\n   706â†’def main():\n   707â†’    \"\"\"Main entry point.\"\"\"\n   708â†’    parser = argparse.ArgumentParser(\n   709â†’        description=\"\"\"\n   710â†’CodeRabbit Review Extractor - Convert GitHub PR reviews to LLM-friendly text\n   711â†’\n   712â†’This tool extracts CodeRabbit automated code review feedback from GitHub pull \n   713â†’requests and formats it as clean, organized text suitable for AI coding agents.\n   714â†’\n   715â†’Perfect for feeding CodeRabbit suggestions to Claude, ChatGPT, or other LLMs\n   716â†’to automatically apply code improvements, refactoring suggestions, and fixes.\n   717â†’        \"\"\".strip(),\n   718â†’        epilog=\"\"\"\n   719â†’EXAMPLES:\n   720â†’  %(prog)s https://github.com/owner/repo/pull/123\n   721â†’  %(prog)s owner/repo/123  \n   722â†’  %(prog)s obra/lace/278 --all-reviews\n   723â†’\n   724â†’WHAT IT EXTRACTS:\n   725â†’  â€¢ Refactor suggestions with specific code diffs\n   726â†’  â€¢ AI implementation instructions (ğŸ¤– prompts) with detailed steps  \n   727â†’  â€¢ Nitpick comments and code quality improvements\n   728â†’  â€¢ File-organized feedback sorted by priority\n   729â†’  â€¢ Clean format without HTML artifacts or noise\n   730â†’\n   731â†’HOW IT WORKS:\n   732â†’  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n   733â†’  2. Extracts both main review summaries and inline code comments\n   734â†’  3. Organizes feedback by file with line-number targeting\n   735â†’  4. Prioritizes actionable items with AI implementation guides\n   736â†’  5. Outputs clean text ready for LLM consumption\n   737â†’\n   738â†’DEFAULT BEHAVIOR:\n   739â†’  Processes only the LATEST CodeRabbit review to avoid overwhelming output.\n   740â†’  Use --all-reviews for PRs with multiple review iterations if needed.\n   741â†’\n   742â†’REQUIREMENTS:\n   743â†’  â€¢ GitHub CLI (gh) installed and authenticated\n   744â†’  â€¢ Python 3.6+ with beautifulsoup4 package\n   745â†’\n   746â†’CONFIGURATION:\n   747â†’  Create ~/.coderabbit-extractor with custom preamble text to prepend to output.\n   748â†’  Useful for framing feedback (e.g., critical evaluation prompts for AI agents).\n   749â†’        \"\"\",\n   750â†’        formatter_class=argparse.RawDescriptionHelpFormatter\n   751â†’    )\n   752â†’    \n   753â†’    parser.add_argument('pr_input', \n   754â†’                       help='GitHub PR URL or owner/repo/number format (e.g., \"owner/repo/123\")')\n   755â†’    parser.add_argument('--include-resolved', \n   756â†’                       action='store_true',\n   757â†’                       help='Include review suggestions that have been marked as resolved (default: filter them out)')\n   758â†’    parser.add_argument('--all-reviews', \n   759â†’                       action='store_true',\n   760â†’                       help='Extract all CodeRabbit reviews (default: latest only)')\n   761â†’    parser.add_argument('--since-commit',\n   762â†’                       metavar='SHA', \n   763â†’                       help='Extract reviews submitted after this commit SHA')\n   764â†’    parser.add_argument('--debug',\n   765â†’                       action='store_true',\n   766â†’                       help='Show debug annotations in output to trace processing')\n   767â†’    parser.add_argument('--version',\n   768â†’                       action='version',\n   769â†’                       version=f'%(prog)s {__version__}')\n   770â†’    \n   771â†’    # Custom handling for no arguments to show helpful info\n   772â†’    if len(sys.argv) == 1:\n   773â†’        print(\"ğŸ¤– CodeRabbit Review Extractor\")\n   774â†’        print(\"=\" * 40)\n   775â†’        print()\n   776â†’        print(\"WHAT THIS DOES:\")\n   777â†’        print(\"  Converts CodeRabbit GitHub PR reviews into clean text for AI coding agents\")\n   778â†’        print(\"  (Claude, ChatGPT, etc.) to automatically apply code suggestions.\")\n   779â†’        print()\n   780â†’        print(\"QUICK START:\")\n   781â†’        print(\"  python3 extract-coderabbit-feedback.py https://github.com/owner/repo/pull/123\")\n   782â†’        print(\"  python3 extract-coderabbit-feedback.py owner/repo/123\")\n   783â†’        print()\n   784â†’        print(\"For full help and options, run:\")\n   785â†’        print(\"  python3 extract-coderabbit-feedback.py --help\")\n   786â†’        sys.exit(1)\n   787â†’    \n   788â†’    args = parser.parse_args()\n   789â†’    \n   790â†’    try:\n   791â†’        pr_url = parse_pr_input(args.pr_input)\n   792â†’        \n   793â†’        # Load custom preamble from dotfile\n   794â†’        preamble = load_preamble_config()\n   795â†’        \n   796â†’        # Fetch both reviews and inline comments\n   797â†’        reviews = fetch_pr_reviews(pr_url)\n   798â†’        inline_comments = fetch_pr_inline_comments(pr_url)\n   799â†’        \n   800â†’        # Extract PR info for GraphQL query\n   801â†’        try:\n   802â†’            owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n   803â†’            \n   804â†’            # Fetch review threads with resolution status via GraphQL\n   805â†’            review_threads = fetch_review_threads_graphql(owner, repo, pr_number)\n   806â†’            \n   807â†’            if review_threads and not args.include_resolved:\n   808â†’                # Filter out resolved threads (GitHub native conversation resolution)\n   809â†’                original_thread_count = len(review_threads)\n   810â†’                review_threads = filter_resolved_threads(review_threads)\n   811â†’                filtered_thread_count = original_thread_count - len(review_threads)\n   812â†’                if filtered_thread_count > 0:\n   813â†’                    print(f\"Filtered resolved review threads: {filtered_thread_count} resolved threads hidden\", file=sys.stderr)\n   814â†’            elif review_threads:\n   815â†’                print(f\"Found {len(review_threads)} review threads (including resolved)\", file=sys.stderr)\n   816â†’                \n   817â†’        except Exception as e:\n   818â†’            print(f\"Warning: Could not fetch review thread resolution data: {e}\", file=sys.stderr)\n   819â†’            review_threads = []\n   820â†’        \n   821â†’        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n   822â†’        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n   823â†’        \n   824â†’        # Convert review threads to inline comment format and merge\n   825â†’        if review_threads:\n   826â†’            for thread in review_threads:\n   827â†’                # Convert thread to inline comment format\n   828â†’                inline_comment = {\n   829â†’                    'id': thread['id'],\n   830â†’                    'body': thread['body'],\n   831â†’                    'user': {'login': 'coderabbitai[bot]'},\n   832â†’                    'created_at': thread.get('createdAt', ''),  # Now includes timestamp from GraphQL\n   833â†’                    'path': 'unknown',  # GraphQL doesn't provide file path in this query\n   834â†’                    'original_line': 0,  # GraphQL doesn't provide line number in this query\n   835â†’                    'html_url': f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\"\n   836â†’                }\n   837â†’                coderabbit_inline_comments.append(inline_comment)\n   838â†’        \n   839â†’        # Filter out resolved comments if not including them\n   840â†’        if not args.include_resolved:\n   841â†’            original_count = len(coderabbit_inline_comments)\n   842â†’            coderabbit_inline_comments = filter_resolved_comments(coderabbit_inline_comments)\n   843â†’            filtered_count = original_count - len(coderabbit_inline_comments)\n   844â†’            if filtered_count > 0:\n   845â†’                print(f\"Filtered resolved comments: {filtered_count} resolved comments hidden\", file=sys.stderr)\n   846â†’        # Note: Review-level filtering is handled in group_comments_by_file()\n   847â†’        \n   848â†’        # Filter reviews based on options\n   849â†’        if not args.all_reviews and not args.since_commit:\n   850â†’            # Default: latest review with actual content\n   851â†’            if coderabbit_reviews:\n   852â†’                # Find the latest review with substantial content (not just continuation)\n   853â†’                latest_review = None\n   854â†’                for review in reversed(coderabbit_reviews):\n   855â†’                    body = review.get('body', '')\n   856â†’                    # Skip continuation reviews that have minimal content\n   857â†’                    if (len(body) > 100 and \n   858â†’                        'Actionable comments posted:' in body and\n   859â†’                        not body.strip().startswith('**Review continued from previous batch')):\n   860â†’                        latest_review = review\n   861â†’                        break\n   862â†’                \n   863â†’                # Fallback to chronologically latest if no substantial review found\n   864â†’                if not latest_review:\n   865â†’                    latest_review = coderabbit_reviews[-1]\n   866â†’                    \n   867â†’                coderabbit_reviews = [latest_review]\n   868â†’                \n   869â†’                # Filter inline comments to only those posted after the most recent commit\n   870â†’                latest_commit_time = fetch_latest_commit_time(pr_url)\n   871â†’                if latest_commit_time and coderabbit_inline_comments:\n   872â†’                    from datetime import datetime, timedelta\n   873â†’                    \n   874â†’                    try:\n   875â†’                        # Parse ISO format timestamps (GitHub API standard)\n   876â†’                        commit_dt = datetime.fromisoformat(latest_commit_time.replace('Z', '+00:00'))\n   877â†’                        \n   878â†’                        filtered_inline = []\n   879â†’                        for comment in coderabbit_inline_comments:\n   880â†’                            comment_time = comment.get('created_at', '')\n   881â†’                            if comment_time:\n   882â†’                                comment_dt = datetime.fromisoformat(comment_time.replace('Z', '+00:00'))\n   883â†’                                # Include comments posted after the latest commit\n   884â†’                                if comment_dt > commit_dt:\n   885â†’                                    filtered_inline.append(comment)\n   886â†’                        \n   887â†’                        original_count = len(coderabbit_inline_comments)\n   888â†’                        coderabbit_inline_comments = filtered_inline\n   889â†’                        if original_count != len(filtered_inline):\n   890â†’                            print(f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\", file=sys.stderr)\n   891â†’                            \n   892â†’                    except Exception as e:\n   893â†’                        print(f\"Warning: Could not filter inline comments by commit time: {e}\", file=sys.stderr)\n   894â†’        elif args.since_commit:\n   895â†’            # Filter reviews submitted after the specified commit\n   896â†’            # This would require additional logic to compare timestamps with commit dates\n   897â†’            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n   898â†’            sys.exit(1)\n   899â†’        \n   900â†’        if not coderabbit_reviews and not coderabbit_inline_comments:\n   901â†’            print(\"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr)\n   902â†’            sys.exit(1)\n   903â†’        \n   904â†’        # Show processing info\n   905â†’        if len(coderabbit_reviews) > 1:\n   906â†’            print(f\"Processing {len(coderabbit_reviews)} CodeRabbit reviews...\", file=sys.stderr)\n   907â†’        elif len(coderabbit_reviews) == 1:\n   908â†’            print(f\"Processing latest CodeRabbit review...\", file=sys.stderr)\n   909â†’            \n   910â†’        if coderabbit_inline_comments:\n   911â†’            print(f\"Found {len(coderabbit_inline_comments)} inline comments\", file=sys.stderr)\n   912â†’        \n   913â†’        formatted_output = format_for_llm(coderabbit_reviews, coderabbit_inline_comments, args.debug, preamble, args.include_resolved)\n   914â†’        print(formatted_output)\n   915â†’        \n   916â†’    except Exception as e:\n   917â†’        print(f\"Error: {e}\", file=sys.stderr)\n   918â†’        sys.exit(1)\n   919â†’\n   920â†’\n   921â†’if __name__ == \"__main__\":\n   922â†’    main()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"0cdf09eb-bae1-44c8-b81f-1ceaa6c1ce04","timestamp":"2025-11-13T11:23:47.666Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","content":"#!/usr/bin/env python3\n\"\"\"\nABOUTME: Extract CodeRabbit review feedback from GitHub PRs for LLM consumption\nABOUTME: Uses gh CLI to fetch PR comments and formats them as plain text\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Jesse Vincent\"\n__license__ = \"MIT\"\nimport argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom urllib.parse import urlparse\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom bs4 import BeautifulSoup\nimport html\n\n\ndef load_preamble_config() -> Optional[str]:\n    \"\"\"Load custom preamble from dotfile.\"\"\"\n    config_path = Path.home() / '.coderabbit-extractor'\n    \n    if config_path.exists():\n        try:\n            return config_path.read_text(encoding='utf-8').strip()\n        except Exception as e:\n            print(f\"Warning: Could not read config file {config_path}: {e}\", file=sys.stderr)\n            return None\n    \n    return None\n\n\ndef parse_pr_input(input_str: str) -> str:\n    \"\"\"Parse PR URL or owner/repo/number format into a full GitHub PR URL.\"\"\"\n    input_str = input_str.strip()\n    \n    # If it's already a full URL, return as-is\n    if input_str.startswith('https://github.com/'):\n        return input_str\n    \n    # Parse owner/repo/number format (e.g., \"obra/lace/278\")\n    parts = input_str.split('/')\n    if len(parts) == 3:\n        owner, repo, pr_num = parts\n        return f\"https://github.com/{owner}/{repo}/pull/{pr_num}\"\n    \n    raise ValueError(f\"Invalid PR format: {input_str}. Use 'owner/repo/number' or full URL\")\n\n\ndef extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\n    \"\"\"Extract owner, repo, and PR number from GitHub PR URL.\"\"\"\n    # Parse URL like https://github.com/owner/repo/pull/123\n    if 'github.com' in pr_url:\n        parts = pr_url.replace('https://github.com/', '').split('/')\n        if len(parts) >= 4 and parts[2] == 'pull':\n            owner, repo, _, pr_number = parts[:4]\n            try:\n                return owner, repo, int(pr_number)\n            except ValueError:\n                raise ValueError(f\"Invalid PR number: {pr_number}\")\n    \n    raise ValueError(f\"Could not parse PR URL: {pr_url}\")\n\n\ndef fetch_pr_reviews(pr_url: str) -> List[Dict[str, Any]]:\n    \"\"\"Fetch PR reviews using gh CLI.\"\"\"\n    try:\n        result = subprocess.run(\n            ['gh', 'pr', 'view', pr_url, '--json', 'reviews'],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        data = json.loads(result.stdout)\n        return data.get('reviews', [])\n    except subprocess.CalledProcessError as e:\n        print(f\"Error fetching PR: {e.stderr}\", file=sys.stderr)\n        sys.exit(1)\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON response: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\ndef fetch_latest_commit_time(pr_url: str) -> Optional[str]:\n    \"\"\"Fetch the timestamp of the most recent commit in the PR.\"\"\"\n    try:\n        # Extract owner/repo/number from URL\n        owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n        \n        # Get commits from the PR\n        result = subprocess.run(\n            ['gh', 'api', f'repos/{owner}/{repo}/pulls/{pr_number}/commits'],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        commits = json.loads(result.stdout)\n        \n        if commits:\n            # Get the most recent commit (last in the list)\n            latest_commit = commits[-1]\n            return latest_commit['commit']['committer']['date']\n        \n        return None\n    except Exception as e:\n        print(f\"Warning: Could not fetch latest commit time: {e}\", file=sys.stderr)\n        return None\n\n\ndef fetch_pr_inline_comments(pr_url: str) -> List[Dict[str, Any]]:\n    \"\"\"Fetch PR inline review comments using GitHub API via gh CLI.\"\"\"\n    try:\n        # Extract owner/repo/number from URL\n        if 'github.com' in pr_url:\n            parts = pr_url.replace('https://github.com/', '').split('/')\n            if len(parts) >= 4 and parts[2] == 'pull':\n                owner, repo, _, pr_number = parts[:4]\n                api_path = f\"repos/{owner}/{repo}/pulls/{pr_number}/comments\"\n            else:\n                raise ValueError(\"Invalid GitHub PR URL format\")\n        else:\n            raise ValueError(\"URL must be a GitHub PR URL\")\n        \n        result = subprocess.run(\n            ['gh', 'api', api_path],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        data = json.loads(result.stdout)\n        return data if isinstance(data, list) else []\n    except subprocess.CalledProcessError as e:\n        print(f\"Error fetching inline comments: {e.stderr}\", file=sys.stderr)\n        return []\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing inline comments JSON: {e}\", file=sys.stderr)\n        return []\n        return []\n\n\ndef fetch_review_threads_graphql(owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:\n    \"\"\"Fetch review threads with resolution status using GitHub GraphQL API.\"\"\"\n    query = '''\n    query($owner: String!, $repo: String!, $prNumber: Int!) {\n      repository(owner: $owner, name: $repo) {\n        pullRequest(number: $prNumber) {\n          reviewThreads(first: 100) {\n            nodes {\n              id\n              isResolved\n              resolvedBy {\n                login\n              }\n              comments(first: 1) {\n                nodes {\n                  body\n                  createdAt\n                  author {\n                    login\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    '''\n    \n    variables = {\n        \"owner\": owner,\n        \"repo\": repo, \n        \"prNumber\": pr_number\n    }\n    \n    try:\n        # Use gh api to make GraphQL request\n        import json\n        result = subprocess.run([\n            'gh', 'api', 'graphql', \n            '-F', f'owner={owner}',\n            '-F', f'repo={repo}', \n            '-F', f'prNumber={pr_number}',\n            '-f', f'query={query}'\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        \n        # Extract review threads\n        threads = data.get('data', {}).get('repository', {}).get('pullRequest', {}).get('reviewThreads', {}).get('nodes', [])\n        \n        # Filter to only CodeRabbit threads and return as list of dicts\n        coderabbit_threads = []\n        for thread in threads:\n            if thread and thread.get('comments', {}).get('nodes', []):\n                comment = thread['comments']['nodes'][0]\n                author_login = comment.get('author', {}).get('login', '')\n                # Check for both coderabbitai and coderabbitai[bot] formats\n                if author_login in ['coderabbitai', 'coderabbitai[bot]']:\n                    coderabbit_threads.append({\n                        'id': thread['id'],\n                        'isResolved': thread.get('isResolved', False),\n                        'resolvedBy': thread.get('resolvedBy', {}).get('login') if thread.get('resolvedBy') else None,\n                        'body': comment.get('body', ''),\n                        'createdAt': comment.get('createdAt', ''),\n                        'author': author_login\n                    })\n        \n        return coderabbit_threads\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Warning: Could not fetch review threads via GraphQL: {e.stderr}\", file=sys.stderr)\n        return []\n    except json.JSONDecodeError as e:\n        print(f\"Warning: Could not parse GraphQL response: {e}\", file=sys.stderr)\n        return []\n    except Exception as e:\n        print(f\"Warning: Error processing GraphQL data: {e}\", file=sys.stderr)\n        return []\n\n\ndef extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter reviews to only CodeRabbit ones.\"\"\"\n    return [\n        review for review in reviews\n        if review.get('author', {}).get('login') == 'coderabbitai'\n    ]\n\n\ndef filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter out comments that have been marked as resolved.\"\"\"\n    return [\n        comment for comment in comments\n        if not is_comment_resolved(comment.get('body', ''))\n    ]\n\n\ndef filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter out review threads that have been marked as resolved via GitHub's native resolution.\"\"\"\n    return [\n        thread for thread in threads\n        if not thread.get('isResolved', False)\n    ]\n\n\ndef filter_resolved_review_content(review_body: str) -> str:\n    \"\"\"Filter out resolved suggestions from review body content.\"\"\"\n    # Look for resolved comment patterns and remove them\n    # This handles CodeRabbit's text-based resolution markers\n    \n    # Pattern to match resolved comments within the review body\n    # Look for sections that contain resolution markers\n    resolved_patterns = [\n        # Match individual comment sections that are resolved\n        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n        # Match line-specific resolved comments\n        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n    ]\n    \n    filtered_body = review_body\n    for pattern in resolved_patterns:\n        filtered_body = re.sub(pattern, '', filtered_body, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Also remove any standalone resolution lines\n    filtered_body = re.sub(r'^.*âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*$(\\n)?', '', filtered_body, flags=re.MULTILINE | re.IGNORECASE)\n    \n    return filtered_body.strip()\n\n\ndef is_comment_resolved(comment_body: str) -> bool:\n    \"\"\"Check if a comment has been marked as resolved/addressed.\"\"\"\n    # Look for resolution indicators in the comment body\n    resolution_patterns = [\n        r'âœ…\\s*Addressed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'âœ…\\s*Resolved\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'âœ…\\s*Fixed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'âœ…\\s*Completed\\s+in\\s+commit[s]?\\s+[a-f0-9]+(?:\\s+to\\s+[a-f0-9]+)?',\n        r'\\[x\\]',  # Checkbox checked\n        r'\\[X\\]',  # Checkbox checked\n    ]\n    \n    for pattern in resolution_patterns:\n        if re.search(pattern, comment_body, re.IGNORECASE):\n            return True\n    \n    return False\n\n\ndef extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter inline comments to only CodeRabbit ones.\"\"\"\n    return [\n        comment for comment in comments\n        if comment.get('user', {}).get('login') == 'coderabbitai[bot]'\n    ]\n\n\ndef parse_review_type(body: str) -> str:\n    \"\"\"Determine the type of CodeRabbit review.\"\"\"\n    if 'Actionable comments posted:' in body:\n        return 'ACTIONABLE_REVIEW'\n    elif 'walkthrough_start' in body and 'walkthrough_end' in body:\n        return 'WALKTHROUGH'\n    elif '<!-- This is an auto-generated comment: summarize by coderabbit.ai -->' in body:\n        return 'SUMMARY'\n    else:\n        return 'OTHER'\n\n\ndef clean_html_artifacts(text: str) -> str:\n    \"\"\"Remove HTML comments and artifacts from text.\"\"\"\n    # Remove HTML comments\n    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n    \n    # Remove CodeRabbit warning blocks\n    text = re.sub(r'>\\s*â€¼ï¸\\s*\\*\\*IMPORTANT\\*\\*.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n    \n    # Remove other quote block artifacts  \n    text = re.sub(r'>\\s*Carefully review.*?(?=\\n\\n|\\n$)', '', text, flags=re.DOTALL)\n    \n    # Remove empty lines that result from artifact removal\n    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n    \n    return text.strip()\n\n\ndef clean_diff_artifacts(diff: str) -> str:\n    \"\"\"Clean up diff artifacts and markers.\"\"\"\n    if not diff:\n        return diff\n        \n    # Remove @@ hunk markers - handle both @@ ... @@ and lone @@ lines\n    diff = re.sub(r'^@@.*?@@.*?\\n', '', diff, flags=re.MULTILINE)  # Full hunk headers\n    diff = re.sub(r'^\\s*@@\\s*\\n', '', diff, flags=re.MULTILINE)    # Lone @@ lines\n    \n    # Remove empty lines at start/end\n    diff = diff.strip()\n    \n    return diff\n\n\ndef extract_prompt_for_ai_agents(body: str) -> List[str]:\n    \"\"\"Extract 'Prompt for AI Agents' sections from review body.\"\"\"\n    prompts = []\n    \n    # Pattern to find ğŸ¤– Prompt for AI Agents sections\n    # These appear to be in <summary> tags followed by clipboard content\n    pattern = r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*<div[^>]*data-snippet-clipboard-copy-content=\"([^\"]*)\"'\n    matches = re.findall(pattern, body, re.DOTALL)\n    \n    for match in matches:\n        # Decode HTML entities\n        clean_text = (match\n                     .replace('&lt;', '<')\n                     .replace('&gt;', '>')\n                     .replace('&amp;', '&')\n                     .replace('&quot;', '\"')\n                     .replace('\\\\n', '\\n')\n                     .strip())\n        if clean_text:\n            prompts.append(clean_text)\n    \n    return prompts\n\n\n# Removed unused functions - now using parse_review_sections instead\n\n\ndef parse_review_sections(body: str) -> Dict[str, Any]:\n    \"\"\"Parse different sections from CodeRabbit review body using HTML parsing.\"\"\"\n    sections = {}\n    \n    # Extract actionable comments count from text\n    actionable_match = re.search(r'Actionable comments posted:\\s*(\\d+)', body)\n    if actionable_match:\n        sections['actionable_count'] = int(actionable_match.group(1))\n    \n    # Parse HTML structure - but be careful not to mangle code content\n    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n    soup = BeautifulSoup(body, 'html.parser')\n    \n    # Find sections by looking for <details> elements with specific summary text\n    details_elements = soup.find_all('details')\n    \n    for details in details_elements:\n        summary = details.find('summary')\n        if not summary:\n            continue\n            \n        summary_text = summary.get_text(strip=True)\n        \n        # Check for outside diff range comments (avoid emoji dependency)\n        if 'Outside diff range comments' in summary_text:\n            # Extract count from summary text\n            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n            if count_match:\n                count = int(count_match.group(1))\n                blockquote = details.find('blockquote')\n                if blockquote:\n                    # Get all content inside the blockquote\n                    content = str(blockquote)\n                    sections['outside_diff_comments'] = {'count': count, 'content': content}\n        \n        # Check for nitpick comments or duplicate comments  \n        elif 'Nitpick comments' in summary_text or 'Duplicate comments' in summary_text:\n            count_match = re.search(r'\\((\\d+)\\)', summary_text)\n            if count_match:\n                count = int(count_match.group(1))\n                # Instead of using BeautifulSoup's blockquote content (which mangles code),\n                # extract the raw content from the original body text\n                \n                # Find the position of this details section in the original body\n                details_start = body.find('Nitpick comments')\n                if details_start != -1:\n                    # Find where this section ends (next major section or end)\n                    section_markers = ['ğŸ“œ Review details', 'ğŸ”‡ Additional comments', '</details>\\n\\n</details>']\n                    section_end = len(body)\n                    \n                    for marker in section_markers:\n                        marker_pos = body.find(marker, details_start)\n                        if marker_pos != -1:\n                            section_end = min(section_end, marker_pos)\n                    \n                    # Extract the raw text content\n                    raw_content = body[details_start:section_end]\n                    sections['nitpick_comments'] = {'count': count, 'content': raw_content}\n        \n        # Check for review details (usually at the end)\n        elif 'Review details' in summary_text:\n            # This section contains metadata, we might want it for context\n            blockquote = details.find('blockquote')\n            if blockquote:\n                content = str(blockquote)\n                sections['review_details'] = {'content': content}\n    \n    # Extract AI prompts (look for specific patterns)\n    sections['ai_prompts'] = extract_prompt_for_ai_agents(body)\n    \n    return sections\n\n\ndef parse_file_level_comments(content: str, debug: bool = False) -> List[Dict[str, Any]]:\n    \"\"\"Parse individual file-level comments from section content, avoiding HTML corruption.\"\"\"\n    comments = []\n    \n    # Use regex to find file sections instead of BeautifulSoup to avoid HTML corruption\n    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n    file_pattern = r'<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>'\n    file_matches = list(re.finditer(file_pattern, content, re.DOTALL))\n    \n    if debug:\n        print(f\"DEBUG: parse_file_level_comments found {len(file_matches)} file sections\", file=sys.stderr)\n    \n    for file_match in file_matches:\n        file_path = file_match.group(1).strip()\n        comment_count = int(file_match.group(2))\n        file_content = file_match.group(3)  # Raw blockquote content, not parsed by BeautifulSoup\n        \n        # Now parse line comments within this file content\n        # Find line comment patterns: `16-24`: **Title**\n        line_pattern = r'`([^`]+)`:\\s*\\*\\*(.*?)\\*\\*\\s*\\n\\n(.*?)(?=\\n\\n`[^`]+`:\\s*\\*\\*|\\n\\n---|\\n\\n</blockquote>|$)'\n        line_matches = list(re.finditer(line_pattern, file_content, re.DOTALL))\n        \n        if debug:\n            print(f\"DEBUG: File {file_path} has {len(line_matches)} line comments\", file=sys.stderr)\n        \n        for line_match in line_matches:\n            line_range = line_match.group(1).strip()\n            title = line_match.group(2).strip() \n            content_block = line_match.group(3).strip()\n            \n            # Check if this looks like a line reference\n            if re.match(r'^[\\d\\-,\\s]+$', line_range):\n                # Extract description (everything before first code block)\n                description = content_block\n                code_diff = \"\"\n                \n                code_start = content_block.find('```')\n                if code_start != -1:\n                    description = content_block[:code_start].strip()\n                    \n                    # Extract code diff from raw content (no BeautifulSoup mangling)\n                    diff_match = re.search(r'```diff\\n(.*?)\\n```', content_block, re.DOTALL)\n                    if diff_match:\n                        code_diff = diff_match.group(1).strip()\n                        # HTML decode only entities, don't let BeautifulSoup interpret as HTML\n                        code_diff = html.unescape(code_diff)\n                        code_diff = clean_html_artifacts(code_diff)  # Remove any HTML comments in diffs\n                        code_diff = clean_diff_artifacts(code_diff)  # Clean up @@ markers and artifacts\n                \n                # Clean up description\n                description = clean_html_artifacts(description)  # Remove HTML comments first\n                description_lines = []\n                for line in description.split('\\n'):\n                    line = line.strip()\n                    if line and not line.startswith('Also applies to:') and line != '---':\n                        description_lines.append(line)\n                description = ' '.join(description_lines)\n                \n                comments.append({\n                    'file': file_path,\n                    'lines': line_range,\n                    'title': title,\n                    'description': description,\n                    'code_diff': code_diff\n                })\n    \n    return comments\n\n\ndef group_comments_by_file(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, include_resolved: bool = True) -> Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Group all comments by file for better organization.\"\"\"\n    file_groups = {}\n    \n    # Process review body comments\n    for review in coderabbit_reviews:\n        review_body = review['body']\n        \n        # Filter out resolved content from review body if requested\n        if not include_resolved:\n            original_body = review_body\n            review_body = filter_resolved_review_content(review_body)\n            if debug and len(original_body) != len(review_body):\n                print(f\"DEBUG: Filtered resolved content from review body (length: {len(original_body)} -> {len(review_body)})\", file=sys.stderr)\n        \n        sections = parse_review_sections(review_body)\n        \n        # Add outside diff comments\n        if 'outside_diff_comments' in sections:\n            outside_comments = parse_file_level_comments(sections['outside_diff_comments']['content'], debug)\n            for comment in outside_comments:\n                file_path = comment['file']\n                if file_path not in file_groups:\n                    file_groups[file_path] = []\n                comment['source'] = 'outside_diff'\n                file_groups[file_path].append(comment)\n        \n        # Add nitpick comments\n        if 'nitpick_comments' in sections:\n            nitpick_comments = parse_file_level_comments(sections['nitpick_comments']['content'], debug)\n            for comment in nitpick_comments:\n                file_path = comment['file']\n                if file_path not in file_groups:\n                    file_groups[file_path] = []\n                comment['source'] = 'nitpick'\n                file_groups[file_path].append(comment)\n    \n    # Process inline comments\n    if inline_comments:\n        for comment in inline_comments:\n            file_path = comment.get('path', 'unknown')\n            line_num = comment.get('original_line', 'unknown')\n            \n            if file_path not in file_groups:\n                file_groups[file_path] = []\n                \n            # Convert inline comment to standard format\n            body = comment.get('body', '')\n            comment_type = 'ğŸ› ï¸ Refactor Suggestion' if 'ğŸ› ï¸ Refactor suggestion' in body else 'Comment'\n            \n            # Extract title from body\n            title_match = re.search(r'\\*\\*(.*?)\\*\\*', body)\n            title = title_match.group(1).strip() if title_match else \"Suggestion\"\n            \n            # Extract description and diff\n            if '<details>' in body and 'Prompt for AI Agents' in body:\n                main_content = body.split('<details>')[0].strip()\n            else:\n                main_content = body\n                \n            main_content = main_content.replace('_ğŸ› ï¸ Refactor suggestion_', '').strip()\n            main_content = clean_html_artifacts(main_content)\n            \n            # Extract AI prompt\n            ai_prompt = \"\"\n            ai_prompt_match = re.search(r'<summary>ğŸ¤– Prompt for AI Agents</summary>\\s*\\n\\n```\\n(.*?)\\n```', body, re.DOTALL)\n            if ai_prompt_match:\n                ai_prompt = ai_prompt_match.group(1).strip()\n            \n            file_groups[file_path].append({\n                'lines': str(line_num),\n                'title': title,\n                'description': main_content,\n                'code_diff': '',  # Inline comments don't have separate diffs\n                'ai_prompt': ai_prompt,\n                'source': 'inline',\n                'comment_type': comment_type\n            })\n    \n    return file_groups\n\n\ndef format_for_llm(coderabbit_reviews: List[Dict[str, Any]], inline_comments: List[Dict[str, Any]] = None, debug: bool = False, preamble: Optional[str] = None, include_resolved: bool = True) -> str:\n    \"\"\"Format CodeRabbit review feedback organized by file for LLM consumption.\"\"\"\n    output = []\n    \n    # Add custom preamble if provided\n    if preamble:\n        output.append(preamble)\n        output.append(\"\")\n        output.append(\"=\" * 60)\n        output.append(\"\")\n    \n    output.append(\"# CodeRabbit Review Feedback\")\n    output.append(\"=\" * 40)\n    output.append(\"\")\n    \n    # Group all comments by file\n    file_groups = group_comments_by_file(coderabbit_reviews, inline_comments, debug, include_resolved)\n    \n    if not file_groups:\n        output.append(\"No actionable comments found.\")\n        return \"\\n\".join(output)\n    \n    # Add summary\n    total_comments = sum(len(comments) for comments in file_groups.values())\n    output.append(f\"**Total files with feedback:** {len(file_groups)}\")\n    output.append(f\"**Total comments:** {total_comments}\")\n    output.append(\"\")\n    \n    # Process each file\n    for file_path in sorted(file_groups.keys()):\n        comments = file_groups[file_path]\n        \n        output.append(f\"## {file_path}\")\n        output.append(f\"**{len(comments)} suggestion(s)**\")\n        output.append(\"\")\n        \n        # Sort comments by priority: AI prompts first, then by line number\n        def get_sort_key(comment):\n            # Priority 1: Comments with AI prompts (highest priority)\n            has_ai_prompt = bool(comment.get('ai_prompt'))\n            \n            # Priority 2: Line number (for logical code flow)\n            line_num = 999\n            if comment['lines'].replace('-', '').isdigit():\n                line_num = int(comment['lines'].split('-')[0])\n                \n            # Return tuple: (ai_prompt_priority, line_number)\n            # Lower numbers = higher priority, so AI prompts get 0, others get 1\n            return (0 if has_ai_prompt else 1, line_num)\n        \n        comments_sorted = sorted(comments, key=get_sort_key)\n        \n        for i, comment in enumerate(comments_sorted, 1):\n            # Add AI prompt indicator to title for high-priority items\n            title = comment['title']\n            if comment.get('ai_prompt'):\n                title = f\"ğŸ¤– {title}\"\n                \n            output.append(f\"### {i}. Lines {comment['lines']}: {title}\")\n            output.append(\"\")\n            \n            # Show AI prompt, or fallback to description + diff\n            has_prompt = bool(comment.get('ai_prompt'))\n            has_diff = bool(comment.get('code_diff'))\n            \n            if debug:\n                output.append(f\"<!-- DEBUG: has_prompt={has_prompt}, has_diff={has_diff}, source={comment.get('source')} -->\")\n            \n            if has_prompt:\n                if debug:\n                    output.append(\"<!-- DEBUG: Showing prompt only, skipping description and diff -->\")\n                # AI prompt contains better explanation than description + diff\n                output.append(\"**Proposed prompt:**\")\n                output.append(\"```\")\n                output.append(comment['ai_prompt'])\n                output.append(\"```\")\n                output.append(\"\")\n            else:\n                if debug:\n                    output.append(\"<!-- DEBUG: No prompt, showing description and diff -->\")\n                \n                # Show description only if no AI prompt available\n                if comment['description'] and len(comment['description']) > 10:\n                    desc = comment['description']\n                    desc = re.sub(r'^_.*?_\\s*', '', desc)  # Remove _âš ï¸ Potential issue_ etc\n                    desc = re.sub(r'\\*\\*.*?\\*\\*\\s*', '', desc, count=1)  # Remove first **title** if duplicated\n                    desc = re.sub(r'Apply this diff:\\s*', '', desc)\n                    desc = desc.strip()\n                    \n                    if desc and desc != title:\n                        output.append(desc)\n                        output.append(\"\")\n                \n                # Show diff only if no AI prompt available\n                if has_diff:\n                    output.append(\"```diff\")\n                    output.append(comment['code_diff'])\n                    output.append(\"```\")\n                    output.append(\"\")\n            \n            if i < len(comments_sorted):\n                output.append(\"-\" * 30)\n                output.append(\"\")\n        \n        output.append(\"=\" * 60)\n        output.append(\"\")\n    \n    return \"\\n\".join(output)\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"\"\"\nCodeRabbit Review Extractor - Convert GitHub PR reviews to LLM-friendly text\n\nThis tool extracts CodeRabbit automated code review feedback from GitHub pull \nrequests and formats it as clean, organized text suitable for AI coding agents.\n\nPerfect for feeding CodeRabbit suggestions to Claude, ChatGPT, or other LLMs\nto automatically apply code improvements, refactoring suggestions, and fixes.\n        \"\"\".strip(),\n        epilog=\"\"\"\nEXAMPLES:\n  %(prog)s https://github.com/owner/repo/pull/123\n  %(prog)s owner/repo/123  \n  %(prog)s obra/lace/278 --all-reviews\n\nWHAT IT EXTRACTS:\n  â€¢ Refactor suggestions with specific code diffs\n  â€¢ AI implementation instructions (ğŸ¤– prompts) with detailed steps  \n  â€¢ Nitpick comments and code quality improvements\n  â€¢ File-organized feedback sorted by priority\n  â€¢ Clean format without HTML artifacts or noise\n\nHOW IT WORKS:\n  1. Uses 'gh' CLI to fetch PR reviews (works with private repos)\n  2. Extracts both main review summaries and inline code comments\n  3. Organizes feedback by file with line-number targeting\n  4. Prioritizes actionable items with AI implementation guides\n  5. Outputs clean text ready for LLM consumption\n\nDEFAULT BEHAVIOR:\n  Processes only the LATEST CodeRabbit review to avoid overwhelming output.\n  Use --all-reviews for PRs with multiple review iterations if needed.\n\nREQUIREMENTS:\n  â€¢ GitHub CLI (gh) installed and authenticated\n  â€¢ Python 3.6+ with beautifulsoup4 package\n\nCONFIGURATION:\n  Create ~/.coderabbit-extractor with custom preamble text to prepend to output.\n  Useful for framing feedback (e.g., critical evaluation prompts for AI agents).\n        \"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    parser.add_argument('pr_input', \n                       help='GitHub PR URL or owner/repo/number format (e.g., \"owner/repo/123\")')\n    parser.add_argument('--include-resolved', \n                       action='store_true',\n                       help='Include review suggestions that have been marked as resolved (default: filter them out)')\n    parser.add_argument('--all-reviews', \n                       action='store_true',\n                       help='Extract all CodeRabbit reviews (default: latest only)')\n    parser.add_argument('--since-commit',\n                       metavar='SHA', \n                       help='Extract reviews submitted after this commit SHA')\n    parser.add_argument('--debug',\n                       action='store_true',\n                       help='Show debug annotations in output to trace processing')\n    parser.add_argument('--version',\n                       action='version',\n                       version=f'%(prog)s {__version__}')\n    \n    # Custom handling for no arguments to show helpful info\n    if len(sys.argv) == 1:\n        print(\"ğŸ¤– CodeRabbit Review Extractor\")\n        print(\"=\" * 40)\n        print()\n        print(\"WHAT THIS DOES:\")\n        print(\"  Converts CodeRabbit GitHub PR reviews into clean text for AI coding agents\")\n        print(\"  (Claude, ChatGPT, etc.) to automatically apply code suggestions.\")\n        print()\n        print(\"QUICK START:\")\n        print(\"  python3 extract-coderabbit-feedback.py https://github.com/owner/repo/pull/123\")\n        print(\"  python3 extract-coderabbit-feedback.py owner/repo/123\")\n        print()\n        print(\"For full help and options, run:\")\n        print(\"  python3 extract-coderabbit-feedback.py --help\")\n        sys.exit(1)\n    \n    args = parser.parse_args()\n    \n    try:\n        pr_url = parse_pr_input(args.pr_input)\n        \n        # Load custom preamble from dotfile\n        preamble = load_preamble_config()\n        \n        # Fetch both reviews and inline comments\n        reviews = fetch_pr_reviews(pr_url)\n        inline_comments = fetch_pr_inline_comments(pr_url)\n        \n        # Extract PR info for GraphQL query\n        try:\n            owner, repo, pr_number = extract_pr_info_from_url(pr_url)\n            \n            # Fetch review threads with resolution status via GraphQL\n            review_threads = fetch_review_threads_graphql(owner, repo, pr_number)\n            \n            if review_threads and not args.include_resolved:\n                # Filter out resolved threads (GitHub native conversation resolution)\n                original_thread_count = len(review_threads)\n                review_threads = filter_resolved_threads(review_threads)\n                filtered_thread_count = original_thread_count - len(review_threads)\n                if filtered_thread_count > 0:\n                    print(f\"Filtered resolved review threads: {filtered_thread_count} resolved threads hidden\", file=sys.stderr)\n            elif review_threads:\n                print(f\"Found {len(review_threads)} review threads (including resolved)\", file=sys.stderr)\n                \n        except Exception as e:\n            print(f\"Warning: Could not fetch review thread resolution data: {e}\", file=sys.stderr)\n            review_threads = []\n        \n        coderabbit_reviews = extract_coderabbit_reviews(reviews)\n        coderabbit_inline_comments = extract_coderabbit_inline_comments(inline_comments)\n        \n        # Convert review threads to inline comment format and merge\n        if review_threads:\n            for thread in review_threads:\n                # Convert thread to inline comment format\n                inline_comment = {\n                    'id': thread['id'],\n                    'body': thread['body'],\n                    'user': {'login': 'coderabbitai[bot]'},\n                    'created_at': thread.get('createdAt', ''),  # Now includes timestamp from GraphQL\n                    'path': 'unknown',  # GraphQL doesn't provide file path in this query\n                    'original_line': 0,  # GraphQL doesn't provide line number in this query\n                    'html_url': f\"https://github.com/{owner}/{repo}/pull/{pr_number}#discussion-{thread['id']}\"\n                }\n                coderabbit_inline_comments.append(inline_comment)\n        \n        # Filter out resolved comments if not including them\n        if not args.include_resolved:\n            original_count = len(coderabbit_inline_comments)\n            coderabbit_inline_comments = filter_resolved_comments(coderabbit_inline_comments)\n            filtered_count = original_count - len(coderabbit_inline_comments)\n            if filtered_count > 0:\n                print(f\"Filtered resolved comments: {filtered_count} resolved comments hidden\", file=sys.stderr)\n        # Note: Review-level filtering is handled in group_comments_by_file()\n        \n        # Filter reviews based on options\n        if not args.all_reviews and not args.since_commit:\n            # Default: latest review with actual content\n            if coderabbit_reviews:\n                # Find the latest review with substantial content (not just continuation)\n                latest_review = None\n                for review in reversed(coderabbit_reviews):\n                    body = review.get('body', '')\n                    # Skip continuation reviews that have minimal content\n                    if (len(body) > 100 and \n                        'Actionable comments posted:' in body and\n                        not body.strip().startswith('**Review continued from previous batch')):\n                        latest_review = review\n                        break\n                \n                # Fallback to chronologically latest if no substantial review found\n                if not latest_review:\n                    latest_review = coderabbit_reviews[-1]\n                    \n                coderabbit_reviews = [latest_review]\n                \n                # Filter inline comments to only those posted after the most recent commit\n                latest_commit_time = fetch_latest_commit_time(pr_url)\n                if latest_commit_time and coderabbit_inline_comments:\n                    from datetime import datetime, timedelta\n                    \n                    try:\n                        # Parse ISO format timestamps (GitHub API standard)\n                        commit_dt = datetime.fromisoformat(latest_commit_time.replace('Z', '+00:00'))\n                        \n                        filtered_inline = []\n                        for comment in coderabbit_inline_comments:\n                            comment_time = comment.get('created_at', '')\n                            if comment_time:\n                                comment_dt = datetime.fromisoformat(comment_time.replace('Z', '+00:00'))\n                                # Include comments posted after the latest commit\n                                if comment_dt > commit_dt:\n                                    filtered_inline.append(comment)\n                        \n                        original_count = len(coderabbit_inline_comments)\n                        coderabbit_inline_comments = filtered_inline\n                        if original_count != len(filtered_inline):\n                            print(f\"Filtered inline comments: {original_count} â†’ {len(filtered_inline)} (after latest commit)\", file=sys.stderr)\n                            \n                    except Exception as e:\n                        print(f\"Warning: Could not filter inline comments by commit time: {e}\", file=sys.stderr)\n        elif args.since_commit:\n            # Filter reviews submitted after the specified commit\n            # This would require additional logic to compare timestamps with commit dates\n            print(\"--since-commit option not yet implemented\", file=sys.stderr)\n            sys.exit(1)\n        \n        if not coderabbit_reviews and not coderabbit_inline_comments:\n            print(\"No CodeRabbit reviews or comments found in this PR.\", file=sys.stderr)\n            sys.exit(1)\n        \n        # Show processing info\n        if len(coderabbit_reviews) > 1:\n            print(f\"Processing {len(coderabbit_reviews)} CodeRabbit reviews...\", file=sys.stderr)\n        elif len(coderabbit_reviews) == 1:\n            print(f\"Processing latest CodeRabbit review...\", file=sys.stderr)\n            \n        if coderabbit_inline_comments:\n            print(f\"Found {len(coderabbit_inline_comments)} inline comments\", file=sys.stderr)\n        \n        formatted_output = format_for_llm(coderabbit_reviews, coderabbit_inline_comments, args.debug, preamble, args.include_resolved)\n        print(formatted_output)\n        \n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()","numLines":922,"startLine":1,"totalLines":922}}}
{"parentUuid":"0cdf09eb-bae1-44c8-b81f-1ceaa6c1ce04","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_018azbijtbA2xK7rt3bKocKY","type":"message","role":"assistant","content":[{"type":"text","text":"Excellent. Now let me search for any other relevant utility functions that might help me understand the filtering approach better:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":13842,"cache_read_input_tokens":8589,"cache_creation":{"ephemeral_5m_input_tokens":13842,"ephemeral_1h_input_tokens":0},"output_tokens":137,"service_tier":"standard"}},"requestId":"req_011CV5oZcQR4qyJ2jqaN9dEA","type":"assistant","uuid":"31d71279-e420-4328-814a-12355ed08460","timestamp":"2025-11-13T11:23:49.863Z"}
{"parentUuid":"31d71279-e420-4328-814a-12355ed08460","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_018azbijtbA2xK7rt3bKocKY","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PKGXngVcXjz2iS9bATAppt","name":"Grep","input":{"pattern":"def clean_|def filter_|def extract_","path":"/Users/dain/workspace/coderabbit-review-helper","output_mode":"content"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":13842,"cache_read_input_tokens":8589,"cache_creation":{"ephemeral_5m_input_tokens":13842,"ephemeral_1h_input_tokens":0},"output_tokens":137,"service_tier":"standard"}},"requestId":"req_011CV5oZcQR4qyJ2jqaN9dEA","type":"assistant","uuid":"79d2ca7d-aa35-4a8c-91d0-e9e9147bfdbe","timestamp":"2025-11-13T11:23:50.444Z"}
{"parentUuid":"79d2ca7d-aa35-4a8c-91d0-e9e9147bfdbe","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PKGXngVcXjz2iS9bATAppt","type":"tool_result","content":"extract-coderabbit-feedback.py:54:def extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\nextract-coderabbit-feedback.py:226:def extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:234:def filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:242:def filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:250:def filter_resolved_review_content(review_body: str) -> str:\nextract-coderabbit-feedback.py:293:def extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:313:def clean_html_artifacts(text: str) -> str:\nextract-coderabbit-feedback.py:330:def clean_diff_artifacts(diff: str) -> str:\nextract-coderabbit-feedback.py:345:def extract_prompt_for_ai_agents(body: str) -> List[str]:"}]},"uuid":"5e806902-69cd-48ef-b255-2eb0b75050df","timestamp":"2025-11-13T11:23:50.879Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"extract-coderabbit-feedback.py:54:def extract_pr_info_from_url(pr_url: str) -> Tuple[str, str, int]:\nextract-coderabbit-feedback.py:226:def extract_coderabbit_reviews(reviews: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:234:def filter_resolved_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:242:def filter_resolved_threads(threads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:250:def filter_resolved_review_content(review_body: str) -> str:\nextract-coderabbit-feedback.py:293:def extract_coderabbit_inline_comments(comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\nextract-coderabbit-feedback.py:313:def clean_html_artifacts(text: str) -> str:\nextract-coderabbit-feedback.py:330:def clean_diff_artifacts(diff: str) -> str:\nextract-coderabbit-feedback.py:345:def extract_prompt_for_ai_agents(body: str) -> List[str]:","numLines":9}}
{"parentUuid":"5e806902-69cd-48ef-b255-2eb0b75050df","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01UubariQyWs41xV6kzTBBiH","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me look at how HTML is processed in the inline comment handling and description cleaning:"}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":799,"cache_read_input_tokens":22431,"cache_creation":{"ephemeral_5m_input_tokens":799,"ephemeral_1h_input_tokens":0},"output_tokens":163,"service_tier":"standard"}},"requestId":"req_011CV5oZr7ftH75vMh56RMhB","type":"assistant","uuid":"e49bd73c-1716-4ae0-8854-a245a81792ca","timestamp":"2025-11-13T11:23:52.500Z"}
{"parentUuid":"e49bd73c-1716-4ae0-8854-a245a81792ca","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01UubariQyWs41xV6kzTBBiH","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HCwauwjyk5vJaMjYD3j5CT","name":"Grep","input":{"pattern":"<details>|</details>|verbose|script|analysis","path":"/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py","output_mode":"content","head_limit":50}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":799,"cache_read_input_tokens":22431,"cache_creation":{"ephemeral_5m_input_tokens":799,"ephemeral_1h_input_tokens":0},"output_tokens":163,"service_tier":"standard"}},"requestId":"req_011CV5oZr7ftH75vMh56RMhB","type":"assistant","uuid":"fe20e7e0-7510-4180-927b-38e141f4878d","timestamp":"2025-11-13T11:23:53.114Z"}
{"parentUuid":"fe20e7e0-7510-4180-927b-38e141f4878d","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HCwauwjyk5vJaMjYD3j5CT","type":"tool_result","content":"259:        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n261:        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n382:    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n385:    # Find sections by looking for <details> elements with specific summary text\n419:                    section_markers = ['ğŸ“œ Review details', 'ğŸ”‡ Additional comments', '</details>\\n\\n</details>']\n450:    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n451:    file_pattern = r'<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>'\n477:                # Extract description (everything before first code block)\n478:                description = content_block\n483:                    description = content_block[:code_start].strip()\n494:                # Clean up description\n495:                description = clean_html_artifacts(description)  # Remove HTML comments first\n496:                description_lines = []\n497:                for line in description.split('\\n'):\n500:                        description_lines.append(line)\n501:                description = ' '.join(description_lines)\n507:                    'description': description,\n568:            # Extract description and diff\n569:            if '<details>' in body and 'Prompt for AI Agents' in body:\n570:                main_content = body.split('<details>')[0].strip()\n586:                'description': main_content,\n657:            # Show AI prompt, or fallback to description + diff\n666:                    output.append(\"<!-- DEBUG: Showing prompt only, skipping description and diff -->\")\n667:                # AI prompt contains better explanation than description + diff\n675:                    output.append(\"<!-- DEBUG: No prompt, showing description and diff -->\")\n677:                # Show description only if no AI prompt available\n678:                if comment['description'] and len(comment['description']) > 10:\n679:                    desc = comment['description']\n709:        description=\"\"\"\n750:        formatter_class=argparse.RawDescriptionHelpFormatter\n\n[Showing results with pagination = limit: 50, offset: 0]"}]},"uuid":"8648842d-1623-419d-91bb-1148dd5853b5","timestamp":"2025-11-13T11:23:53.538Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"259:        r'<details>.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?</details>',\n261:        r'`[^`]+`:\\s*\\*\\*[^*]+\\*\\*.*?âœ…\\s*(?:Addressed|Resolved|Fixed|Completed).*?(?=<details>|$)',\n382:    # Only use BeautifulSoup for extracting the <details> sections, then work with raw content\n385:    # Find sections by looking for <details> elements with specific summary text\n419:                    section_markers = ['ğŸ“œ Review details', 'ğŸ”‡ Additional comments', '</details>\\n\\n</details>']\n450:    # Pattern: <summary>filename (count)</summary><blockquote>content</blockquote></details>\n451:    file_pattern = r'<summary>([^<]+?)\\s*\\((\\d+)\\)</summary><blockquote>(.*?)</blockquote></details>'\n477:                # Extract description (everything before first code block)\n478:                description = content_block\n483:                    description = content_block[:code_start].strip()\n494:                # Clean up description\n495:                description = clean_html_artifacts(description)  # Remove HTML comments first\n496:                description_lines = []\n497:                for line in description.split('\\n'):\n500:                        description_lines.append(line)\n501:                description = ' '.join(description_lines)\n507:                    'description': description,\n568:            # Extract description and diff\n569:            if '<details>' in body and 'Prompt for AI Agents' in body:\n570:                main_content = body.split('<details>')[0].strip()\n586:                'description': main_content,\n657:            # Show AI prompt, or fallback to description + diff\n666:                    output.append(\"<!-- DEBUG: Showing prompt only, skipping description and diff -->\")\n667:                # AI prompt contains better explanation than description + diff\n675:                    output.append(\"<!-- DEBUG: No prompt, showing description and diff -->\")\n677:                # Show description only if no AI prompt available\n678:                if comment['description'] and len(comment['description']) > 10:\n679:                    desc = comment['description']\n709:        description=\"\"\"\n750:        formatter_class=argparse.RawDescriptionHelpFormatter","numLines":30,"appliedLimit":50}}
{"parentUuid":"8648842d-1623-419d-91bb-1148dd5853b5","isSidechain":true,"userType":"external","cwd":"/Users/dain/workspace/coderabbit-review-helper","sessionId":"741790a4-4fe2-4644-9a51-fb4482074060","version":"2.0.37","gitBranch":"main","agentId":"b52b1c09","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01MfdaFKk8sUzSbQzXZ2Q6q2","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect. Now let me create a comprehensive summary document:\n\n## Summary: HTML Cleaning and <details> Tag Filtering Analysis\n\nBased on my examination of `/Users/dain/workspace/coderabbit-review-helper/extract-coderabbit-feedback.py`, here's what I found:\n\n### 1. Current HTML Cleaning Approach\n\n**`clean_html_artifacts()` function (lines 313-327):**\nCurrently handles:\n- HTML comments removal: `<!--.*?-->`\n- CodeRabbit warning blocks: `>\\s*â€¼ï¸\\s*\\*\\*IMPORTANT\\*\\*.*?`\n- Quote block artifacts: `>\\s*Carefully review.*?`\n- Excess whitespace cleanup\n\nThis is a **text-level cleaning function** that works on extracted/processed text, not raw HTML.\n\n**Key insight:** The current approach removes comment markers and warning text but **does NOT handle `<details>` tags themselves**.\n\n### 2. Where HTML Processing Currently Happens\n\n**Multiple locations process comment bodies:**\n\n1. **`parse_review_sections()` (lines 372-442)**\n   - Uses BeautifulSoup to find `<details>` elements\n   - Extracts sections by looking for specific summary text\n   - Handles: outside diff comments, nitpick comments, review details\n   - **Does NOT filter out `<details>` tags** - it uses them for structural navigation\n\n2. **`parse_file_level_comments()` (lines 445-511)**\n   - Uses regex to find file sections within blockquotes\n   - Calls `clean_html_artifacts()` on extracted descriptions (line 495)\n   - Calls `clean_html_artifacts()` on code diffs (line 491)\n   - **Operates after `<details>` are already parsed** - receives blockquote content\n\n3. **`group_comments_by_file()` (lines 514-593)**\n   - Processes review body comments and inline comments\n   - Calls `filter_resolved_review_content()` on review bodies (line 525)\n   - Calls `clean_html_artifacts()` on inline comment content (line 575)\n   - **Coordinates high-level filtering** before parsing into comments\n\n4. **Inline comment processing (lines 569-591)**\n   - Splits on `<details>` to extract main content: `body.split('<details>')[0]` (line 570)\n   - Calls `clean_html_artifacts()` on the extracted portion (line 575)\n   - **Already filters out `<details>` sections** from inline comments\n\n### 3. Best Locations to Add <details> Tag Filtering\n\n**Three strategic locations exist:**\n\n**Option A: In `clean_html_artifacts()` (lines 313-327) - RECOMMENDED**\n- **When:** After HTML comments but before artifact removal\n- **Why:** Centralised, handles all `<details>` removal consistently\n- **Impact:** Would affect all calls to `clean_html_artifacts()`\n- **Lines affected:** 491, 495, 575 (existing calls would automatically benefit)\n- **Code location:** Add new pattern before line 316\n\n**Option B: In `parse_file_level_comments()` (lines 445-511)**\n- **When:** After extracting blockquote content, before parsing comments\n- **Why:** File-level specific, targets where verbose analysis chains appear\n- **Impact:** Only affects file-level comment parsing\n- **Specific point:** Add after line 460 (after extracting `file_content`)\n\n**Option C: In `group_comments_by_file()` (lines 514-593)**\n- **When:** Early in comment processing, before delegation to parse functions\n- **Why:** High-level filtering before detailed processing\n- **Impact:** Affects both review bodies and inline comments\n- **Specific point:** Add after line 520 or line 561\n\n### 4. Current <details> Tag Usage in the Codebase\n\nThe code **intentionally uses `<details>` tags for structural navigation**:\n\n- Line 259: Pattern matches resolved sections within `<details>` tags\n- Line 386: `soup.find_all('details')` - explicitly looks for details elements\n- Line 389-391: Extracts summary from each details element\n- Line 569: `if '<details>' in body and 'Prompt for AI Agents' in body:` - checks for specific details sections\n- Line 451: File pattern expects `</details>` as section boundary\n\n### 5. Filtering Strategy Recommendation\n\n**Use ONE filter in a NEW, dedicated function (not `clean_html_artifacts()`)**\n\n**Why separate?**\n- `clean_html_artifacts()` removes markers/artifacts from **already-extracted text**\n- The new function should remove **structure elements and their contents** from **raw HTML**\n- Different concerns = different functions = better maintainability\n\n**Proposed function placement: Between `clean_html_artifacts()` and `clean_diff_artifacts()` (after line 327)**\n\n**Proposed function signature:**\n```python\ndef remove_verbose_details_sections(text: str) -> str:\n    \"\"\"Remove verbose <details> sections containing analysis chains and script outputs.\n    \n    Removes complete <details>...</details> blocks that contain:\n    - Analysis/reasoning chains\n    - Script execution outputs  \n    - Internal processing logs\n    \n    Keeps <details> blocks for AI prompts and essential content.\n    \"\"\"\n```\n\n**Optimal call locations (in order of precedence):**\n\n1. **In `parse_file_level_comments()` - line 461**\n   - Call BEFORE regex parsing: `file_content = remove_verbose_details_sections(file_content)`\n   - Handles file-level analysis chains\n\n2. **In inline comment processing - line 570**\n   - Call on the main_content: `main_content = remove_verbose_details_sections(main_content)`\n   - Handles inline comment verbose sections\n\n3. **In `filter_resolved_review_content()` - line 264**\n   - Call before pattern matching: `filtered_body = remove_verbose_details_sections(filtered_body)`\n   - Handles resolved content filtering\n\n### Summary Table\n\n| Function | Purpose | Current HTML Handling | Best Filter Location |\n|----------|---------|----------------------|----------------------|\n| `clean_html_artifacts()` | Remove markers/artifacts from extracted text | Regex-based cleanup | Metadata/artifacts only |\n| `parse_review_sections()` | Navigate structure using `<details>` | BeautifulSoup parsing | N/A (structural use) |\n| `parse_file_level_comments()` | Extract individual comments from sections | Receives parsed content | **âœ“ After content extraction** |\n| `group_comments_by_file()` | Coordinate comment processing | Calls child functions | **âœ“ Early in flow** |\n| Inline comment processing | Split on `<details>` boundary | Already filters partially | **âœ“ Complete removal** |\n\n**Recommendation: Implement ONE new function and call it in 2-3 strategic locations (file-level and inline comment processing) for maximum coverage.**"}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":974,"cache_read_input_tokens":23230,"cache_creation":{"ephemeral_5m_input_tokens":974,"ephemeral_1h_input_tokens":0},"output_tokens":1662,"service_tier":"standard"}},"requestId":"req_011CV5oa3V2xr6WS7mHQJZCG","type":"assistant","uuid":"48dc3de5-c46e-48c7-b490-f9357b68f6d0","timestamp":"2025-11-13T11:24:10.420Z"}
